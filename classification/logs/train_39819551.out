==========================================
Job started at: Wed Nov 19 15:07:47 MST 2025
Job ID: 39819551
Node: sg239
==========================================
Python version:
Python 3.12.12
PyTorch version:
2.9.1+cu126
CUDA available:
True
GPU info:
Wed Nov 19 15:08:06 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 580.95.05              Driver Version: 580.95.05      CUDA Version: 13.0     |
+-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  NVIDIA A100-SXM4-80GB          On  |   00000000:81:00.0 Off |                    0 |
| N/A   35C    P0             79W /  500W |       0MiB /  81920MiB |      0%      Default |
|                                         |                        |             Disabled |
+-----------------------------------------+------------------------+----------------------+

+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
==========================================
Starting training...
==========================================
/etc/python/sitecustomize.py:117: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  mod = _original_import(name, globals, locals, fromlist, level)
/scratch/smehta90/InternImage_MIMIC/classification/ops_dcnv3/functions/dcnv3_func.py:22: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/scratch/smehta90/InternImage_MIMIC/classification/ops_dcnv3/functions/dcnv3_func.py:58: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/home/smehta90/.conda/envs/INTERNIMAGE_ENV/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
=> merge config from configs/internimage_b_mimic_cxr_224.yaml
RANK and WORLD_SIZE in environ: 0/1
/home/smehta90/.conda/envs/INTERNIMAGE_ENV/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[rank0]:[W1119 15:08:24.652171523 ProcessGroupNCCL.cpp:5072] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()
O1 O1
[2025-11-19 15:08:25 internimage_b_mimic_cxr_224](main.py 729): INFO Full config saved to output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/config.json
[2025-11-19 15:08:25 internimage_b_mimic_cxr_224](main.py 732): INFO AMP_OPT_LEVEL: O1
AMP_TYPE: float16
AUG:
  AUTO_AUGMENT: none
  COLOR_JITTER: 0.0
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MEAN:
  - 0.485
  - 0.456
  - 0.406
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 0.0
  MIXUP_SWITCH_PROB: 0.0
  RANDOM_RESIZED_CROP: false
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.1
  STD:
  - 0.229
  - 0.224
  - 0.225
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: mimic_cxr
  DATA_PATH: /scratch/smehta90/mimic_splits
  IMG_ON_MEMORY: false
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 12
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_22K_TO_1K: false
EVAL_FREQ: 1
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.5
  DROP_PATH_TYPE: linear
  DROP_RATE: 0.0
  INTERN_IMAGE:
    CENTER_FEATURE_SCALE: false
    CHANNELS: 112
    CORE_OP: DCNv3
    DEPTHS:
    - 4
    - 4
    - 21
    - 4
    DW_KERNEL_SIZE: null
    GROUPS:
    - 7
    - 14
    - 28
    - 56
    LAYER_SCALE: 1.0e-05
    LEVEL2_POST_NORM: false
    LEVEL2_POST_NORM_BLOCK_IDS: null
    MLP_RATIO: 4.0
    OFFSET_SCALE: 1.0
    POST_NORM: true
    REMOVE_CENTER: false
    RES_POST_NORM: false
    USE_CLIP_PROJECTOR: false
  LABEL_SMOOTHING: 0.0
  NAME: internimage_b_mimic_cxr_224
  NUM_CLASSES: 14
  PRETRAINED: pretrained/internimage_b_1k_224.pth
  RESUME: ''
  TYPE: intern_image
OUTPUT: output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224
PRINT_FREQ: 50
SAVE_CKPT_NUM: 1
SAVE_FREQ: 1
SEED: 42
TAG: run1
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EMA:
    DECAY: 0.9999
    ENABLE: true
  EPOCHS: 20
  LR_LAYER_DECAY: true
  LR_LAYER_DECAY_RATIO: 0.875
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    DCN_LR_MUL: null
    EPS: 1.0e-08
    FREEZE_BACKBONE: null
    MOMENTUM: 0.9
    NAME: adamw
    USE_ZERO: false
  RAND_INIT_FT_HEAD: true
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 2
  WARMUP_LR: 5.0e-07
  WEIGHT_DECAY: 0.05

Loading MIMIC-CXR train split from: /scratch/smehta90/mimic_splits/mimic-cxr-2.0.0-chexpert-train.csv
Filtered from 368945 to 368945 samples for split='train'
Using column 'path' for image paths
Loaded 368945 samples for train split
Label shape: (368945, 14)
Positive label distribution: [ 63485.  62554.  14236.  35279.   9752.   7404.  10490.  74302. 141239.
  74745.   3319.  25489.  14001.  81890.]
Verifying image accessibility...
  5/5 test images accessible
local rank 0 / global rank 0successfully build train dataset
Loading MIMIC-CXR val split from: /scratch/smehta90/mimic_splits/mimic-cxr-2.0.0-chexpert-val.csv
Filtered from 2991 to 2991 samples for split='validate'
Using column 'path' for image paths
Loaded 2991 samples for val split
Label shape: (2991, 14)
Positive label distribution: [ 528.  534.  113.  326.   90.   34.  109.  560. 1129.  670.   22.  194.
  112.  726.]
Verifying image accessibility...
  5/5 test images accessible
local rank 0 / global rank 0successfully build val dataset
Loading MIMIC-CXR test split from: /scratch/smehta90/mimic_splits/mimic-cxr-2.0.0-chexpert-test.csv
Filtered from 5159 to 5159 samples for split='test'
Using column 'path' for image paths
Loaded 5159 samples for test split
Label shape: (5159, 14)
Positive label distribution: [1034. 1258.  326.  959.  200.  167.  202. 1561.  984. 1542.  119.  539.
  144. 1457.]
Verifying image accessibility...
  5/5 test images accessible
local rank 0 / global rank 0successfully build test dataset
[2025-11-19 15:08:27 internimage_b_mimic_cxr_224](main.py 175): INFO Creating model:intern_image/internimage_b_mimic_cxr_224
using core type: DCNv3
using activation layer: GELU
using main norm layer: LN
using dpr: linear, 0.5
level2_post_norm: False
level2_post_norm_block_ids: None
res_post_norm: False
remove_center: False
[2025-11-19 15:08:29 internimage_b_mimic_cxr_224](main.py 178): INFO InternImage(
  (patch_embed): StemLayer(
    (conv1): Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm1): Sequential(
      (0): to_channels_last()
      (1): LayerNorm((56,), eps=1e-06, elementwise_affine=True)
      (2): to_channels_first()
    )
    (act): GELU(approximate='none')
    (conv2): Conv2d(56, 112, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm2): Sequential(
      (0): to_channels_last()
      (1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
    )
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (levels): ModuleList(
    (0): InternImageBlock(
      (blocks): ModuleList(
        (0): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(112, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=112)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=112, out_features=126, bias=True)
            (mask): Linear(in_features=112, out_features=63, bias=True)
            (input_proj): Linear(in_features=112, out_features=112, bias=True)
            (output_proj): Linear(in_features=112, out_features=112, bias=True)
          )
          (drop_path): Identity()
          (norm2): Sequential(
            (0): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=112, out_features=448, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=448, out_features=112, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(112, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=112)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=112, out_features=126, bias=True)
            (mask): Linear(in_features=112, out_features=63, bias=True)
            (input_proj): Linear(in_features=112, out_features=112, bias=True)
            (output_proj): Linear(in_features=112, out_features=112, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.016)
          (norm2): Sequential(
            (0): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=112, out_features=448, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=448, out_features=112, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(112, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=112)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=112, out_features=126, bias=True)
            (mask): Linear(in_features=112, out_features=63, bias=True)
            (input_proj): Linear(in_features=112, out_features=112, bias=True)
            (output_proj): Linear(in_features=112, out_features=112, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.031)
          (norm2): Sequential(
            (0): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=112, out_features=448, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=448, out_features=112, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(112, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=112)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=112, out_features=126, bias=True)
            (mask): Linear(in_features=112, out_features=63, bias=True)
            (input_proj): Linear(in_features=112, out_features=112, bias=True)
            (output_proj): Linear(in_features=112, out_features=112, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): Sequential(
            (0): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=112, out_features=448, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=448, out_features=112, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): DownsampleLayer(
        (conv): Conv2d(112, 224, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (norm): Sequential(
          (0): to_channels_last()
          (1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (1): InternImageBlock(
      (blocks): ModuleList(
        (0): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=224)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=224, out_features=252, bias=True)
            (mask): Linear(in_features=224, out_features=126, bias=True)
            (input_proj): Linear(in_features=224, out_features=224, bias=True)
            (output_proj): Linear(in_features=224, out_features=224, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.062)
          (norm2): Sequential(
            (0): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=224, out_features=896, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=896, out_features=224, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=224)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=224, out_features=252, bias=True)
            (mask): Linear(in_features=224, out_features=126, bias=True)
            (input_proj): Linear(in_features=224, out_features=224, bias=True)
            (output_proj): Linear(in_features=224, out_features=224, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): Sequential(
            (0): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=224, out_features=896, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=896, out_features=224, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=224)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=224, out_features=252, bias=True)
            (mask): Linear(in_features=224, out_features=126, bias=True)
            (input_proj): Linear(in_features=224, out_features=224, bias=True)
            (output_proj): Linear(in_features=224, out_features=224, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.094)
          (norm2): Sequential(
            (0): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=224, out_features=896, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=896, out_features=224, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=224)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=224, out_features=252, bias=True)
            (mask): Linear(in_features=224, out_features=126, bias=True)
            (input_proj): Linear(in_features=224, out_features=224, bias=True)
            (output_proj): Linear(in_features=224, out_features=224, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): Sequential(
            (0): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=224, out_features=896, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=896, out_features=224, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): DownsampleLayer(
        (conv): Conv2d(224, 448, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (norm): Sequential(
          (0): to_channels_last()
          (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (2): InternImageBlock(
      (blocks): ModuleList(
        (0): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.125)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.141)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.156)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.172)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.188)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.203)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.219)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.234)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.250)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.266)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.281)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.297)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.312)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.328)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.344)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.359)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.375)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.391)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (18): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.406)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (19): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.422)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (20): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.438)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): DownsampleLayer(
        (conv): Conv2d(448, 896, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (norm): Sequential(
          (0): to_channels_last()
          (1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (3): InternImageBlock(
      (blocks): ModuleList(
        (0): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=896)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=896, out_features=1008, bias=True)
            (mask): Linear(in_features=896, out_features=504, bias=True)
            (input_proj): Linear(in_features=896, out_features=896, bias=True)
            (output_proj): Linear(in_features=896, out_features=896, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.453)
          (norm2): Sequential(
            (0): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=896, out_features=3584, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3584, out_features=896, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=896)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=896, out_features=1008, bias=True)
            (mask): Linear(in_features=896, out_features=504, bias=True)
            (input_proj): Linear(in_features=896, out_features=896, bias=True)
            (output_proj): Linear(in_features=896, out_features=896, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.469)
          (norm2): Sequential(
            (0): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=896, out_features=3584, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3584, out_features=896, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=896)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=896, out_features=1008, bias=True)
            (mask): Linear(in_features=896, out_features=504, bias=True)
            (input_proj): Linear(in_features=896, out_features=896, bias=True)
            (output_proj): Linear(in_features=896, out_features=896, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.484)
          (norm2): Sequential(
            (0): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=896, out_features=3584, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3584, out_features=896, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=896)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=896, out_features=1008, bias=True)
            (mask): Linear(in_features=896, out_features=504, bias=True)
            (input_proj): Linear(in_features=896, out_features=896, bias=True)
            (output_proj): Linear(in_features=896, out_features=896, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.500)
          (norm2): Sequential(
            (0): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=896, out_features=3584, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3584, out_features=896, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (conv_head): Sequential(
    (0): Conv2d(896, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): Sequential(
      (0): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): GELU(approximate='none')
  )
  (head): Linear(in_features=1344, out_features=14, bias=True)
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
)
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
no decay params: {no_decay_name}
lr_ratio_params:
patch_embed.conv1.weight (0.0001, 0.013939837037683136, 0.05, True)
patch_embed.conv1.bias (0.0001, 0.013939837037683136, 0.0, True)
patch_embed.norm1.1.weight (0.0001, 0.013939837037683136, 0.0, True)
patch_embed.norm1.1.bias (0.0001, 0.013939837037683136, 0.0, True)
patch_embed.conv2.weight (0.0001, 0.013939837037683136, 0.05, True)
patch_embed.conv2.bias (0.0001, 0.013939837037683136, 0.0, True)
patch_embed.norm2.1.weight (0.0001, 0.013939837037683136, 0.0, True)
patch_embed.norm2.1.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.gamma1 (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.gamma2 (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.norm1.0.weight (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.norm1.0.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.dcn.dw_conv.0.weight (0.0001, 0.013939837037683136, 0.05, True)
levels.0.blocks.0.dcn.dw_conv.0.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.dcn.dw_conv.1.1.weight (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.dcn.dw_conv.1.1.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.dcn.offset.weight (0.0001, 0.013939837037683136, 0.05, True)
levels.0.blocks.0.dcn.offset.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.dcn.mask.weight (0.0001, 0.013939837037683136, 0.05, True)
levels.0.blocks.0.dcn.mask.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.dcn.input_proj.weight (0.0001, 0.013939837037683136, 0.05, True)
levels.0.blocks.0.dcn.input_proj.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.dcn.output_proj.weight (0.0001, 0.013939837037683136, 0.05, True)
levels.0.blocks.0.dcn.output_proj.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.norm2.0.weight (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.norm2.0.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.mlp.fc1.weight (0.0001, 0.013939837037683136, 0.05, True)
levels.0.blocks.0.mlp.fc1.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.mlp.fc2.weight (0.0001, 0.013939837037683136, 0.05, True)
levels.0.blocks.0.mlp.fc2.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.1.gamma1 (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.gamma2 (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.norm1.0.weight (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.norm1.0.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.dcn.dw_conv.0.weight (0.0001, 0.015931242328780727, 0.05, True)
levels.0.blocks.1.dcn.dw_conv.0.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.dcn.dw_conv.1.1.weight (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.dcn.dw_conv.1.1.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.dcn.offset.weight (0.0001, 0.015931242328780727, 0.05, True)
levels.0.blocks.1.dcn.offset.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.dcn.mask.weight (0.0001, 0.015931242328780727, 0.05, True)
levels.0.blocks.1.dcn.mask.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.dcn.input_proj.weight (0.0001, 0.015931242328780727, 0.05, True)
levels.0.blocks.1.dcn.input_proj.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.dcn.output_proj.weight (0.0001, 0.015931242328780727, 0.05, True)
levels.0.blocks.1.dcn.output_proj.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.norm2.0.weight (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.norm2.0.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.mlp.fc1.weight (0.0001, 0.015931242328780727, 0.05, True)
levels.0.blocks.1.mlp.fc1.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.mlp.fc2.weight (0.0001, 0.015931242328780727, 0.05, True)
levels.0.blocks.1.mlp.fc2.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.2.gamma1 (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.gamma2 (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.norm1.0.weight (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.norm1.0.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.dcn.dw_conv.0.weight (0.0001, 0.018207134090035115, 0.05, True)
levels.0.blocks.2.dcn.dw_conv.0.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.dcn.dw_conv.1.1.weight (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.dcn.dw_conv.1.1.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.dcn.offset.weight (0.0001, 0.018207134090035115, 0.05, True)
levels.0.blocks.2.dcn.offset.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.dcn.mask.weight (0.0001, 0.018207134090035115, 0.05, True)
levels.0.blocks.2.dcn.mask.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.dcn.input_proj.weight (0.0001, 0.018207134090035115, 0.05, True)
levels.0.blocks.2.dcn.input_proj.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.dcn.output_proj.weight (0.0001, 0.018207134090035115, 0.05, True)
levels.0.blocks.2.dcn.output_proj.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.norm2.0.weight (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.norm2.0.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.mlp.fc1.weight (0.0001, 0.018207134090035115, 0.05, True)
levels.0.blocks.2.mlp.fc1.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.mlp.fc2.weight (0.0001, 0.018207134090035115, 0.05, True)
levels.0.blocks.2.mlp.fc2.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.3.gamma1 (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.gamma2 (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.norm1.0.weight (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.norm1.0.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.dcn.dw_conv.0.weight (0.0001, 0.02080815324575442, 0.05, True)
levels.0.blocks.3.dcn.dw_conv.0.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.dcn.dw_conv.1.1.weight (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.dcn.dw_conv.1.1.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.dcn.offset.weight (0.0001, 0.02080815324575442, 0.05, True)
levels.0.blocks.3.dcn.offset.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.dcn.mask.weight (0.0001, 0.02080815324575442, 0.05, True)
levels.0.blocks.3.dcn.mask.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.dcn.input_proj.weight (0.0001, 0.02080815324575442, 0.05, True)
levels.0.blocks.3.dcn.input_proj.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.dcn.output_proj.weight (0.0001, 0.02080815324575442, 0.05, True)
levels.0.blocks.3.dcn.output_proj.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.norm2.0.weight (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.norm2.0.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.mlp.fc1.weight (0.0001, 0.02080815324575442, 0.05, True)
levels.0.blocks.3.mlp.fc1.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.mlp.fc2.weight (0.0001, 0.02080815324575442, 0.05, True)
levels.0.blocks.3.mlp.fc2.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.downsample.conv.weight (0.0001, 0.02378074656657648, 0.05, True)
levels.0.downsample.norm.1.weight (0.0001, 0.02378074656657648, 0.0, True)
levels.0.downsample.norm.1.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.gamma1 (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.gamma2 (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.norm1.0.weight (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.norm1.0.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.dcn.dw_conv.0.weight (0.0001, 0.02378074656657648, 0.05, True)
levels.1.blocks.0.dcn.dw_conv.0.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.dcn.dw_conv.1.1.weight (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.dcn.dw_conv.1.1.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.dcn.offset.weight (0.0001, 0.02378074656657648, 0.05, True)
levels.1.blocks.0.dcn.offset.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.dcn.mask.weight (0.0001, 0.02378074656657648, 0.05, True)
levels.1.blocks.0.dcn.mask.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.dcn.input_proj.weight (0.0001, 0.02378074656657648, 0.05, True)
levels.1.blocks.0.dcn.input_proj.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.dcn.output_proj.weight (0.0001, 0.02378074656657648, 0.05, True)
levels.1.blocks.0.dcn.output_proj.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.norm2.0.weight (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.norm2.0.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.mlp.fc1.weight (0.0001, 0.02378074656657648, 0.05, True)
levels.1.blocks.0.mlp.fc1.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.mlp.fc2.weight (0.0001, 0.02378074656657648, 0.05, True)
levels.1.blocks.0.mlp.fc2.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.1.gamma1 (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.gamma2 (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.norm1.0.weight (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.norm1.0.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.dcn.dw_conv.0.weight (0.0001, 0.027177996076087403, 0.05, True)
levels.1.blocks.1.dcn.dw_conv.0.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.dcn.dw_conv.1.1.weight (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.dcn.dw_conv.1.1.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.dcn.offset.weight (0.0001, 0.027177996076087403, 0.05, True)
levels.1.blocks.1.dcn.offset.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.dcn.mask.weight (0.0001, 0.027177996076087403, 0.05, True)
levels.1.blocks.1.dcn.mask.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.dcn.input_proj.weight (0.0001, 0.027177996076087403, 0.05, True)
levels.1.blocks.1.dcn.input_proj.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.dcn.output_proj.weight (0.0001, 0.027177996076087403, 0.05, True)
levels.1.blocks.1.dcn.output_proj.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.norm2.0.weight (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.norm2.0.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.mlp.fc1.weight (0.0001, 0.027177996076087403, 0.05, True)
levels.1.blocks.1.mlp.fc1.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.mlp.fc2.weight (0.0001, 0.027177996076087403, 0.05, True)
levels.1.blocks.1.mlp.fc2.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.2.gamma1 (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.gamma2 (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.norm1.0.weight (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.norm1.0.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.dcn.dw_conv.0.weight (0.0001, 0.03106056694409989, 0.05, True)
levels.1.blocks.2.dcn.dw_conv.0.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.dcn.dw_conv.1.1.weight (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.dcn.dw_conv.1.1.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.dcn.offset.weight (0.0001, 0.03106056694409989, 0.05, True)
levels.1.blocks.2.dcn.offset.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.dcn.mask.weight (0.0001, 0.03106056694409989, 0.05, True)
levels.1.blocks.2.dcn.mask.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.dcn.input_proj.weight (0.0001, 0.03106056694409989, 0.05, True)
levels.1.blocks.2.dcn.input_proj.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.dcn.output_proj.weight (0.0001, 0.03106056694409989, 0.05, True)
levels.1.blocks.2.dcn.output_proj.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.norm2.0.weight (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.norm2.0.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.mlp.fc1.weight (0.0001, 0.03106056694409989, 0.05, True)
levels.1.blocks.2.mlp.fc1.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.mlp.fc2.weight (0.0001, 0.03106056694409989, 0.05, True)
levels.1.blocks.2.mlp.fc2.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.3.gamma1 (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.gamma2 (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.norm1.0.weight (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.norm1.0.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.dcn.dw_conv.0.weight (0.0001, 0.03549779079325702, 0.05, True)
levels.1.blocks.3.dcn.dw_conv.0.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.dcn.dw_conv.1.1.weight (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.dcn.dw_conv.1.1.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.dcn.offset.weight (0.0001, 0.03549779079325702, 0.05, True)
levels.1.blocks.3.dcn.offset.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.dcn.mask.weight (0.0001, 0.03549779079325702, 0.05, True)
levels.1.blocks.3.dcn.mask.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.dcn.input_proj.weight (0.0001, 0.03549779079325702, 0.05, True)
levels.1.blocks.3.dcn.input_proj.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.dcn.output_proj.weight (0.0001, 0.03549779079325702, 0.05, True)
levels.1.blocks.3.dcn.output_proj.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.norm2.0.weight (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.norm2.0.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.mlp.fc1.weight (0.0001, 0.03549779079325702, 0.05, True)
levels.1.blocks.3.mlp.fc1.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.mlp.fc2.weight (0.0001, 0.03549779079325702, 0.05, True)
levels.1.blocks.3.mlp.fc2.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.downsample.conv.weight (0.0001, 0.040568903763722304, 0.05, True)
levels.1.downsample.norm.1.weight (0.0001, 0.040568903763722304, 0.0, True)
levels.1.downsample.norm.1.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.gamma1 (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.gamma2 (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.norm1.0.weight (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.norm1.0.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.dcn.dw_conv.0.weight (0.0001, 0.040568903763722304, 0.05, True)
levels.2.blocks.0.dcn.dw_conv.0.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.dcn.dw_conv.1.1.weight (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.dcn.dw_conv.1.1.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.dcn.offset.weight (0.0001, 0.040568903763722304, 0.05, True)
levels.2.blocks.0.dcn.offset.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.dcn.mask.weight (0.0001, 0.040568903763722304, 0.05, True)
levels.2.blocks.0.dcn.mask.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.dcn.input_proj.weight (0.0001, 0.040568903763722304, 0.05, True)
levels.2.blocks.0.dcn.input_proj.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.dcn.output_proj.weight (0.0001, 0.040568903763722304, 0.05, True)
levels.2.blocks.0.dcn.output_proj.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.norm2.0.weight (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.norm2.0.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.mlp.fc1.weight (0.0001, 0.040568903763722304, 0.05, True)
levels.2.blocks.0.mlp.fc1.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.mlp.fc2.weight (0.0001, 0.040568903763722304, 0.05, True)
levels.2.blocks.0.mlp.fc2.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.1.gamma1 (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.gamma2 (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.norm1.0.weight (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.norm1.0.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.dcn.dw_conv.0.weight (0.0001, 0.04636446144425406, 0.05, True)
levels.2.blocks.1.dcn.dw_conv.0.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.dcn.dw_conv.1.1.weight (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.dcn.dw_conv.1.1.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.dcn.offset.weight (0.0001, 0.04636446144425406, 0.05, True)
levels.2.blocks.1.dcn.offset.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.dcn.mask.weight (0.0001, 0.04636446144425406, 0.05, True)
levels.2.blocks.1.dcn.mask.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.dcn.input_proj.weight (0.0001, 0.04636446144425406, 0.05, True)
levels.2.blocks.1.dcn.input_proj.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.dcn.output_proj.weight (0.0001, 0.04636446144425406, 0.05, True)
levels.2.blocks.1.dcn.output_proj.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.norm2.0.weight (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.norm2.0.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.mlp.fc1.weight (0.0001, 0.04636446144425406, 0.05, True)
levels.2.blocks.1.mlp.fc1.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.mlp.fc2.weight (0.0001, 0.04636446144425406, 0.05, True)
levels.2.blocks.1.mlp.fc2.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.2.gamma1 (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.gamma2 (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.norm1.0.weight (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.norm1.0.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.dcn.dw_conv.0.weight (0.0001, 0.05298795593629036, 0.05, True)
levels.2.blocks.2.dcn.dw_conv.0.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.dcn.dw_conv.1.1.weight (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.dcn.dw_conv.1.1.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.dcn.offset.weight (0.0001, 0.05298795593629036, 0.05, True)
levels.2.blocks.2.dcn.offset.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.dcn.mask.weight (0.0001, 0.05298795593629036, 0.05, True)
levels.2.blocks.2.dcn.mask.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.dcn.input_proj.weight (0.0001, 0.05298795593629036, 0.05, True)
levels.2.blocks.2.dcn.input_proj.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.dcn.output_proj.weight (0.0001, 0.05298795593629036, 0.05, True)
levels.2.blocks.2.dcn.output_proj.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.norm2.0.weight (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.norm2.0.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.mlp.fc1.weight (0.0001, 0.05298795593629036, 0.05, True)
levels.2.blocks.2.mlp.fc1.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.mlp.fc2.weight (0.0001, 0.05298795593629036, 0.05, True)
levels.2.blocks.2.mlp.fc2.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.3.gamma1 (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.gamma2 (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.norm1.0.weight (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.norm1.0.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.dcn.dw_conv.0.weight (0.0001, 0.06055766392718898, 0.05, True)
levels.2.blocks.3.dcn.dw_conv.0.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.dcn.dw_conv.1.1.weight (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.dcn.dw_conv.1.1.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.dcn.offset.weight (0.0001, 0.06055766392718898, 0.05, True)
levels.2.blocks.3.dcn.offset.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.dcn.mask.weight (0.0001, 0.06055766392718898, 0.05, True)
levels.2.blocks.3.dcn.mask.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.dcn.input_proj.weight (0.0001, 0.06055766392718898, 0.05, True)
levels.2.blocks.3.dcn.input_proj.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.dcn.output_proj.weight (0.0001, 0.06055766392718898, 0.05, True)
levels.2.blocks.3.dcn.output_proj.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.norm2.0.weight (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.norm2.0.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.mlp.fc1.weight (0.0001, 0.06055766392718898, 0.05, True)
levels.2.blocks.3.mlp.fc1.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.mlp.fc2.weight (0.0001, 0.06055766392718898, 0.05, True)
levels.2.blocks.3.mlp.fc2.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.4.gamma1 (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.gamma2 (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.norm1.0.weight (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.norm1.0.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.dcn.dw_conv.0.weight (0.0001, 0.06920875877393026, 0.05, True)
levels.2.blocks.4.dcn.dw_conv.0.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.dcn.dw_conv.1.1.weight (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.dcn.dw_conv.1.1.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.dcn.offset.weight (0.0001, 0.06920875877393026, 0.05, True)
levels.2.blocks.4.dcn.offset.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.dcn.mask.weight (0.0001, 0.06920875877393026, 0.05, True)
levels.2.blocks.4.dcn.mask.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.dcn.input_proj.weight (0.0001, 0.06920875877393026, 0.05, True)
levels.2.blocks.4.dcn.input_proj.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.dcn.output_proj.weight (0.0001, 0.06920875877393026, 0.05, True)
levels.2.blocks.4.dcn.output_proj.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.norm2.0.weight (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.norm2.0.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.mlp.fc1.weight (0.0001, 0.06920875877393026, 0.05, True)
levels.2.blocks.4.mlp.fc1.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.mlp.fc2.weight (0.0001, 0.06920875877393026, 0.05, True)
levels.2.blocks.4.mlp.fc2.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.5.gamma1 (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.gamma2 (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.norm1.0.weight (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.norm1.0.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.dcn.dw_conv.0.weight (0.0001, 0.07909572431306316, 0.05, True)
levels.2.blocks.5.dcn.dw_conv.0.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.dcn.dw_conv.1.1.weight (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.dcn.dw_conv.1.1.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.dcn.offset.weight (0.0001, 0.07909572431306316, 0.05, True)
levels.2.blocks.5.dcn.offset.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.dcn.mask.weight (0.0001, 0.07909572431306316, 0.05, True)
levels.2.blocks.5.dcn.mask.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.dcn.input_proj.weight (0.0001, 0.07909572431306316, 0.05, True)
levels.2.blocks.5.dcn.input_proj.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.dcn.output_proj.weight (0.0001, 0.07909572431306316, 0.05, True)
levels.2.blocks.5.dcn.output_proj.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.norm2.0.weight (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.norm2.0.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.mlp.fc1.weight (0.0001, 0.07909572431306316, 0.05, True)
levels.2.blocks.5.mlp.fc1.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.mlp.fc2.weight (0.0001, 0.07909572431306316, 0.05, True)
levels.2.blocks.5.mlp.fc2.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.6.gamma1 (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.gamma2 (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.norm1.0.weight (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.norm1.0.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.dcn.dw_conv.0.weight (0.0001, 0.09039511350064361, 0.05, True)
levels.2.blocks.6.dcn.dw_conv.0.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.dcn.dw_conv.1.1.weight (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.dcn.dw_conv.1.1.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.dcn.offset.weight (0.0001, 0.09039511350064361, 0.05, True)
levels.2.blocks.6.dcn.offset.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.dcn.mask.weight (0.0001, 0.09039511350064361, 0.05, True)
levels.2.blocks.6.dcn.mask.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.dcn.input_proj.weight (0.0001, 0.09039511350064361, 0.05, True)
levels.2.blocks.6.dcn.input_proj.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.dcn.output_proj.weight (0.0001, 0.09039511350064361, 0.05, True)
levels.2.blocks.6.dcn.output_proj.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.norm2.0.weight (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.norm2.0.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.mlp.fc1.weight (0.0001, 0.09039511350064361, 0.05, True)
levels.2.blocks.6.mlp.fc1.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.mlp.fc2.weight (0.0001, 0.09039511350064361, 0.05, True)
levels.2.blocks.6.mlp.fc2.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.7.gamma1 (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.gamma2 (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.norm1.0.weight (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.norm1.0.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.dcn.dw_conv.0.weight (0.0001, 0.1033087011435927, 0.05, True)
levels.2.blocks.7.dcn.dw_conv.0.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.dcn.dw_conv.1.1.weight (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.dcn.dw_conv.1.1.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.dcn.offset.weight (0.0001, 0.1033087011435927, 0.05, True)
levels.2.blocks.7.dcn.offset.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.dcn.mask.weight (0.0001, 0.1033087011435927, 0.05, True)
levels.2.blocks.7.dcn.mask.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.dcn.input_proj.weight (0.0001, 0.1033087011435927, 0.05, True)
levels.2.blocks.7.dcn.input_proj.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.dcn.output_proj.weight (0.0001, 0.1033087011435927, 0.05, True)
levels.2.blocks.7.dcn.output_proj.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.norm2.0.weight (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.norm2.0.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.mlp.fc1.weight (0.0001, 0.1033087011435927, 0.05, True)
levels.2.blocks.7.mlp.fc1.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.mlp.fc2.weight (0.0001, 0.1033087011435927, 0.05, True)
levels.2.blocks.7.mlp.fc2.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.8.gamma1 (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.gamma2 (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.norm1.0.weight (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.norm1.0.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.dcn.dw_conv.0.weight (0.0001, 0.1180670870212488, 0.05, True)
levels.2.blocks.8.dcn.dw_conv.0.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.dcn.dw_conv.1.1.weight (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.dcn.dw_conv.1.1.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.dcn.offset.weight (0.0001, 0.1180670870212488, 0.05, True)
levels.2.blocks.8.dcn.offset.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.dcn.mask.weight (0.0001, 0.1180670870212488, 0.05, True)
levels.2.blocks.8.dcn.mask.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.dcn.input_proj.weight (0.0001, 0.1180670870212488, 0.05, True)
levels.2.blocks.8.dcn.input_proj.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.dcn.output_proj.weight (0.0001, 0.1180670870212488, 0.05, True)
levels.2.blocks.8.dcn.output_proj.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.norm2.0.weight (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.norm2.0.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.mlp.fc1.weight (0.0001, 0.1180670870212488, 0.05, True)
levels.2.blocks.8.mlp.fc1.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.mlp.fc2.weight (0.0001, 0.1180670870212488, 0.05, True)
levels.2.blocks.8.mlp.fc2.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.9.gamma1 (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.gamma2 (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.norm1.0.weight (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.norm1.0.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.dcn.dw_conv.0.weight (0.0001, 0.13493381373857005, 0.05, True)
levels.2.blocks.9.dcn.dw_conv.0.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.dcn.dw_conv.1.1.weight (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.dcn.dw_conv.1.1.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.dcn.offset.weight (0.0001, 0.13493381373857005, 0.05, True)
levels.2.blocks.9.dcn.offset.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.dcn.mask.weight (0.0001, 0.13493381373857005, 0.05, True)
levels.2.blocks.9.dcn.mask.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.dcn.input_proj.weight (0.0001, 0.13493381373857005, 0.05, True)
levels.2.blocks.9.dcn.input_proj.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.dcn.output_proj.weight (0.0001, 0.13493381373857005, 0.05, True)
levels.2.blocks.9.dcn.output_proj.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.norm2.0.weight (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.norm2.0.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.mlp.fc1.weight (0.0001, 0.13493381373857005, 0.05, True)
levels.2.blocks.9.mlp.fc1.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.mlp.fc2.weight (0.0001, 0.13493381373857005, 0.05, True)
levels.2.blocks.9.mlp.fc2.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.10.gamma1 (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.gamma2 (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.norm1.0.weight (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.norm1.0.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.dcn.dw_conv.0.weight (0.0001, 0.15421007284408006, 0.05, True)
levels.2.blocks.10.dcn.dw_conv.0.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.dcn.dw_conv.1.1.weight (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.dcn.dw_conv.1.1.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.dcn.offset.weight (0.0001, 0.15421007284408006, 0.05, True)
levels.2.blocks.10.dcn.offset.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.dcn.mask.weight (0.0001, 0.15421007284408006, 0.05, True)
levels.2.blocks.10.dcn.mask.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.dcn.input_proj.weight (0.0001, 0.15421007284408006, 0.05, True)
levels.2.blocks.10.dcn.input_proj.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.dcn.output_proj.weight (0.0001, 0.15421007284408006, 0.05, True)
levels.2.blocks.10.dcn.output_proj.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.norm2.0.weight (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.norm2.0.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.mlp.fc1.weight (0.0001, 0.15421007284408006, 0.05, True)
levels.2.blocks.10.mlp.fc1.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.mlp.fc2.weight (0.0001, 0.15421007284408006, 0.05, True)
levels.2.blocks.10.mlp.fc2.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.11.gamma1 (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.gamma2 (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.norm1.0.weight (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.norm1.0.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.dcn.dw_conv.0.weight (0.0001, 0.1762400832503772, 0.05, True)
levels.2.blocks.11.dcn.dw_conv.0.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.dcn.dw_conv.1.1.weight (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.dcn.dw_conv.1.1.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.dcn.offset.weight (0.0001, 0.1762400832503772, 0.05, True)
levels.2.blocks.11.dcn.offset.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.dcn.mask.weight (0.0001, 0.1762400832503772, 0.05, True)
levels.2.blocks.11.dcn.mask.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.dcn.input_proj.weight (0.0001, 0.1762400832503772, 0.05, True)
levels.2.blocks.11.dcn.input_proj.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.dcn.output_proj.weight (0.0001, 0.1762400832503772, 0.05, True)
levels.2.blocks.11.dcn.output_proj.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.norm2.0.weight (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.norm2.0.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.mlp.fc1.weight (0.0001, 0.1762400832503772, 0.05, True)
levels.2.blocks.11.mlp.fc1.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.mlp.fc2.weight (0.0001, 0.1762400832503772, 0.05, True)
levels.2.blocks.11.mlp.fc2.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.12.gamma1 (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.gamma2 (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.norm1.0.weight (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.norm1.0.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.dcn.dw_conv.0.weight (0.0001, 0.2014172380004311, 0.05, True)
levels.2.blocks.12.dcn.dw_conv.0.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.dcn.dw_conv.1.1.weight (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.dcn.dw_conv.1.1.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.dcn.offset.weight (0.0001, 0.2014172380004311, 0.05, True)
levels.2.blocks.12.dcn.offset.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.dcn.mask.weight (0.0001, 0.2014172380004311, 0.05, True)
levels.2.blocks.12.dcn.mask.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.dcn.input_proj.weight (0.0001, 0.2014172380004311, 0.05, True)
levels.2.blocks.12.dcn.input_proj.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.dcn.output_proj.weight (0.0001, 0.2014172380004311, 0.05, True)
levels.2.blocks.12.dcn.output_proj.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.norm2.0.weight (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.norm2.0.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.mlp.fc1.weight (0.0001, 0.2014172380004311, 0.05, True)
levels.2.blocks.12.mlp.fc1.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.mlp.fc2.weight (0.0001, 0.2014172380004311, 0.05, True)
levels.2.blocks.12.mlp.fc2.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.13.gamma1 (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.gamma2 (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.norm1.0.weight (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.norm1.0.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.dcn.dw_conv.0.weight (0.0001, 0.23019112914334983, 0.05, True)
levels.2.blocks.13.dcn.dw_conv.0.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.dcn.dw_conv.1.1.weight (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.dcn.dw_conv.1.1.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.dcn.offset.weight (0.0001, 0.23019112914334983, 0.05, True)
levels.2.blocks.13.dcn.offset.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.dcn.mask.weight (0.0001, 0.23019112914334983, 0.05, True)
levels.2.blocks.13.dcn.mask.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.dcn.input_proj.weight (0.0001, 0.23019112914334983, 0.05, True)
levels.2.blocks.13.dcn.input_proj.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.dcn.output_proj.weight (0.0001, 0.23019112914334983, 0.05, True)
levels.2.blocks.13.dcn.output_proj.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.norm2.0.weight (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.norm2.0.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.mlp.fc1.weight (0.0001, 0.23019112914334983, 0.05, True)
levels.2.blocks.13.mlp.fc1.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.mlp.fc2.weight (0.0001, 0.23019112914334983, 0.05, True)
levels.2.blocks.13.mlp.fc2.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.14.gamma1 (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.gamma2 (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.norm1.0.weight (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.norm1.0.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.dcn.dw_conv.0.weight (0.0001, 0.2630755761638284, 0.05, True)
levels.2.blocks.14.dcn.dw_conv.0.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.dcn.dw_conv.1.1.weight (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.dcn.dw_conv.1.1.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.dcn.offset.weight (0.0001, 0.2630755761638284, 0.05, True)
levels.2.blocks.14.dcn.offset.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.dcn.mask.weight (0.0001, 0.2630755761638284, 0.05, True)
levels.2.blocks.14.dcn.mask.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.dcn.input_proj.weight (0.0001, 0.2630755761638284, 0.05, True)
levels.2.blocks.14.dcn.input_proj.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.dcn.output_proj.weight (0.0001, 0.2630755761638284, 0.05, True)
levels.2.blocks.14.dcn.output_proj.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.norm2.0.weight (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.norm2.0.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.mlp.fc1.weight (0.0001, 0.2630755761638284, 0.05, True)
levels.2.blocks.14.mlp.fc1.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.mlp.fc2.weight (0.0001, 0.2630755761638284, 0.05, True)
levels.2.blocks.14.mlp.fc2.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.15.gamma1 (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.gamma2 (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.norm1.0.weight (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.norm1.0.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.dcn.dw_conv.0.weight (0.0001, 0.30065780133008957, 0.05, True)
levels.2.blocks.15.dcn.dw_conv.0.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.dcn.dw_conv.1.1.weight (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.dcn.dw_conv.1.1.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.dcn.offset.weight (0.0001, 0.30065780133008957, 0.05, True)
levels.2.blocks.15.dcn.offset.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.dcn.mask.weight (0.0001, 0.30065780133008957, 0.05, True)
levels.2.blocks.15.dcn.mask.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.dcn.input_proj.weight (0.0001, 0.30065780133008957, 0.05, True)
levels.2.blocks.15.dcn.input_proj.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.dcn.output_proj.weight (0.0001, 0.30065780133008957, 0.05, True)
levels.2.blocks.15.dcn.output_proj.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.norm2.0.weight (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.norm2.0.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.mlp.fc1.weight (0.0001, 0.30065780133008957, 0.05, True)
levels.2.blocks.15.mlp.fc1.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.mlp.fc2.weight (0.0001, 0.30065780133008957, 0.05, True)
levels.2.blocks.15.mlp.fc2.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.16.gamma1 (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.gamma2 (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.norm1.0.weight (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.norm1.0.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.dcn.dw_conv.0.weight (0.0001, 0.34360891580581665, 0.05, True)
levels.2.blocks.16.dcn.dw_conv.0.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.dcn.dw_conv.1.1.weight (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.dcn.dw_conv.1.1.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.dcn.offset.weight (0.0001, 0.34360891580581665, 0.05, True)
levels.2.blocks.16.dcn.offset.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.dcn.mask.weight (0.0001, 0.34360891580581665, 0.05, True)
levels.2.blocks.16.dcn.mask.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.dcn.input_proj.weight (0.0001, 0.34360891580581665, 0.05, True)
levels.2.blocks.16.dcn.input_proj.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.dcn.output_proj.weight (0.0001, 0.34360891580581665, 0.05, True)
levels.2.blocks.16.dcn.output_proj.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.norm2.0.weight (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.norm2.0.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.mlp.fc1.weight (0.0001, 0.34360891580581665, 0.05, True)
levels.2.blocks.16.mlp.fc1.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.mlp.fc2.weight (0.0001, 0.34360891580581665, 0.05, True)
levels.2.blocks.16.mlp.fc2.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.17.gamma1 (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.gamma2 (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.norm1.0.weight (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.norm1.0.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.dcn.dw_conv.0.weight (0.0001, 0.39269590377807617, 0.05, True)
levels.2.blocks.17.dcn.dw_conv.0.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.dcn.dw_conv.1.1.weight (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.dcn.dw_conv.1.1.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.dcn.offset.weight (0.0001, 0.39269590377807617, 0.05, True)
levels.2.blocks.17.dcn.offset.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.dcn.mask.weight (0.0001, 0.39269590377807617, 0.05, True)
levels.2.blocks.17.dcn.mask.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.dcn.input_proj.weight (0.0001, 0.39269590377807617, 0.05, True)
levels.2.blocks.17.dcn.input_proj.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.dcn.output_proj.weight (0.0001, 0.39269590377807617, 0.05, True)
levels.2.blocks.17.dcn.output_proj.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.norm2.0.weight (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.norm2.0.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.mlp.fc1.weight (0.0001, 0.39269590377807617, 0.05, True)
levels.2.blocks.17.mlp.fc1.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.mlp.fc2.weight (0.0001, 0.39269590377807617, 0.05, True)
levels.2.blocks.17.mlp.fc2.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.18.gamma1 (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.gamma2 (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.norm1.0.weight (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.norm1.0.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.dcn.dw_conv.0.weight (0.0001, 0.4487953186035156, 0.05, True)
levels.2.blocks.18.dcn.dw_conv.0.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.dcn.dw_conv.1.1.weight (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.dcn.dw_conv.1.1.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.dcn.offset.weight (0.0001, 0.4487953186035156, 0.05, True)
levels.2.blocks.18.dcn.offset.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.dcn.mask.weight (0.0001, 0.4487953186035156, 0.05, True)
levels.2.blocks.18.dcn.mask.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.dcn.input_proj.weight (0.0001, 0.4487953186035156, 0.05, True)
levels.2.blocks.18.dcn.input_proj.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.dcn.output_proj.weight (0.0001, 0.4487953186035156, 0.05, True)
levels.2.blocks.18.dcn.output_proj.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.norm2.0.weight (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.norm2.0.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.mlp.fc1.weight (0.0001, 0.4487953186035156, 0.05, True)
levels.2.blocks.18.mlp.fc1.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.mlp.fc2.weight (0.0001, 0.4487953186035156, 0.05, True)
levels.2.blocks.18.mlp.fc2.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.19.gamma1 (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.gamma2 (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.norm1.0.weight (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.norm1.0.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.dcn.dw_conv.0.weight (0.0001, 0.512908935546875, 0.05, True)
levels.2.blocks.19.dcn.dw_conv.0.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.dcn.dw_conv.1.1.weight (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.dcn.dw_conv.1.1.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.dcn.offset.weight (0.0001, 0.512908935546875, 0.05, True)
levels.2.blocks.19.dcn.offset.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.dcn.mask.weight (0.0001, 0.512908935546875, 0.05, True)
levels.2.blocks.19.dcn.mask.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.dcn.input_proj.weight (0.0001, 0.512908935546875, 0.05, True)
levels.2.blocks.19.dcn.input_proj.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.dcn.output_proj.weight (0.0001, 0.512908935546875, 0.05, True)
levels.2.blocks.19.dcn.output_proj.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.norm2.0.weight (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.norm2.0.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.mlp.fc1.weight (0.0001, 0.512908935546875, 0.05, True)
levels.2.blocks.19.mlp.fc1.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.mlp.fc2.weight (0.0001, 0.512908935546875, 0.05, True)
levels.2.blocks.19.mlp.fc2.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.20.gamma1 (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.gamma2 (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.norm1.0.weight (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.norm1.0.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.dcn.dw_conv.0.weight (0.0001, 0.586181640625, 0.05, True)
levels.2.blocks.20.dcn.dw_conv.0.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.dcn.dw_conv.1.1.weight (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.dcn.dw_conv.1.1.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.dcn.offset.weight (0.0001, 0.586181640625, 0.05, True)
levels.2.blocks.20.dcn.offset.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.dcn.mask.weight (0.0001, 0.586181640625, 0.05, True)
levels.2.blocks.20.dcn.mask.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.dcn.input_proj.weight (0.0001, 0.586181640625, 0.05, True)
levels.2.blocks.20.dcn.input_proj.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.dcn.output_proj.weight (0.0001, 0.586181640625, 0.05, True)
levels.2.blocks.20.dcn.output_proj.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.norm2.0.weight (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.norm2.0.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.mlp.fc1.weight (0.0001, 0.586181640625, 0.05, True)
levels.2.blocks.20.mlp.fc1.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.mlp.fc2.weight (0.0001, 0.586181640625, 0.05, True)
levels.2.blocks.20.mlp.fc2.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.downsample.conv.weight (0.0001, 0.669921875, 0.05, True)
levels.2.downsample.norm.1.weight (0.0001, 0.669921875, 0.0, True)
levels.2.downsample.norm.1.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.gamma1 (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.gamma2 (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.norm1.0.weight (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.norm1.0.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.dcn.dw_conv.0.weight (0.0001, 0.669921875, 0.05, True)
levels.3.blocks.0.dcn.dw_conv.0.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.dcn.dw_conv.1.1.weight (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.dcn.dw_conv.1.1.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.dcn.offset.weight (0.0001, 0.669921875, 0.05, True)
levels.3.blocks.0.dcn.offset.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.dcn.mask.weight (0.0001, 0.669921875, 0.05, True)
levels.3.blocks.0.dcn.mask.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.dcn.input_proj.weight (0.0001, 0.669921875, 0.05, True)
levels.3.blocks.0.dcn.input_proj.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.dcn.output_proj.weight (0.0001, 0.669921875, 0.05, True)
levels.3.blocks.0.dcn.output_proj.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.norm2.0.weight (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.norm2.0.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.mlp.fc1.weight (0.0001, 0.669921875, 0.05, True)
levels.3.blocks.0.mlp.fc1.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.mlp.fc2.weight (0.0001, 0.669921875, 0.05, True)
levels.3.blocks.0.mlp.fc2.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.1.gamma1 (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.gamma2 (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.norm1.0.weight (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.norm1.0.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.dcn.dw_conv.0.weight (0.0001, 0.765625, 0.05, True)
levels.3.blocks.1.dcn.dw_conv.0.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.dcn.dw_conv.1.1.weight (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.dcn.dw_conv.1.1.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.dcn.offset.weight (0.0001, 0.765625, 0.05, True)
levels.3.blocks.1.dcn.offset.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.dcn.mask.weight (0.0001, 0.765625, 0.05, True)
levels.3.blocks.1.dcn.mask.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.dcn.input_proj.weight (0.0001, 0.765625, 0.05, True)
levels.3.blocks.1.dcn.input_proj.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.dcn.output_proj.weight (0.0001, 0.765625, 0.05, True)
levels.3.blocks.1.dcn.output_proj.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.norm2.0.weight (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.norm2.0.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.mlp.fc1.weight (0.0001, 0.765625, 0.05, True)
levels.3.blocks.1.mlp.fc1.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.mlp.fc2.weight (0.0001, 0.765625, 0.05, True)
levels.3.blocks.1.mlp.fc2.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.2.gamma1 (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.gamma2 (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.norm1.0.weight (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.norm1.0.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.dcn.dw_conv.0.weight (0.0001, 0.875, 0.05, True)
levels.3.blocks.2.dcn.dw_conv.0.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.dcn.dw_conv.1.1.weight (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.dcn.dw_conv.1.1.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.dcn.offset.weight (0.0001, 0.875, 0.05, True)
levels.3.blocks.2.dcn.offset.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.dcn.mask.weight (0.0001, 0.875, 0.05, True)
levels.3.blocks.2.dcn.mask.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.dcn.input_proj.weight (0.0001, 0.875, 0.05, True)
levels.3.blocks.2.dcn.input_proj.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.dcn.output_proj.weight (0.0001, 0.875, 0.05, True)
levels.3.blocks.2.dcn.output_proj.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.norm2.0.weight (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.norm2.0.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.mlp.fc1.weight (0.0001, 0.875, 0.05, True)
levels.3.blocks.2.mlp.fc1.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.mlp.fc2.weight (0.0001, 0.875, 0.05, True)
levels.3.blocks.2.mlp.fc2.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.3.gamma1 (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.gamma2 (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.norm1.0.weight (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.norm1.0.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.dcn.dw_conv.0.weight (0.0001, 1.0, 0.05, True)
levels.3.blocks.3.dcn.dw_conv.0.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.dcn.dw_conv.1.1.weight (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.dcn.dw_conv.1.1.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.dcn.offset.weight (0.0001, 1.0, 0.05, True)
levels.3.blocks.3.dcn.offset.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.dcn.mask.weight (0.0001, 1.0, 0.05, True)
levels.3.blocks.3.dcn.mask.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.dcn.input_proj.weight (0.0001, 1.0, 0.05, True)
levels.3.blocks.3.dcn.input_proj.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.dcn.output_proj.weight (0.0001, 1.0, 0.05, True)
levels.3.blocks.3.dcn.output_proj.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.norm2.0.weight (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.norm2.0.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.mlp.fc1.weight (0.0001, 1.0, 0.05, True)
levels.3.blocks.3.mlp.fc1.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.mlp.fc2.weight (0.0001, 1.0, 0.05, True)
levels.3.blocks.3.mlp.fc2.bias (0.0001, 1.0, 0.0, True)
conv_head.0.weight (0.0001, None, 0.05, True)
conv_head.1.0.weight (0.0001, None, 0.0, True)
conv_head.1.0.bias (0.0001, None, 0.0, True)
head.weight (0.0001, None, 0.05, True)
head.bias (0.0001, None, 0.0, True)
/scratch/smehta90/InternImage_MIMIC/classification/utils.py:430: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
[2025-11-19 15:08:29 internimage_b_mimic_cxr_224](main.py 214): INFO Using native Torch AMP. Training in mixed precision.
[2025-11-19 15:08:29 internimage_b_mimic_cxr_224](main.py 226): INFO using fp16_compress_hook!
[2025-11-19 15:08:29 internimage_b_mimic_cxr_224](main.py 234): INFO number of params: 96135662
[2025-11-19 15:08:29 internimage_b_mimic_cxr_224](main.py 247): INFO Using BCEWithLogitsLoss for multi-label classification
All checkpoints founded in output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224: ['ckpt_epoch_6.pth', 'ckpt_epoch_best.pth', 'ckpt_epoch_ema_best.pth']
The latest checkpoint founded: output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_6.pth
[2025-11-19 15:08:29 internimage_b_mimic_cxr_224](main.py 270): INFO auto resuming from output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_6.pth
[2025-11-19 15:08:29 internimage_b_mimic_cxr_224](utils.py 60): INFO ==============> Resuming form output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_6.pth....................
resuming model
[2025-11-19 15:08:33 internimage_b_mimic_cxr_224](utils.py 92): INFO <All keys matched successfully>
resuming optimizer
resuming lr_scheduler
[2025-11-19 15:08:33 internimage_b_mimic_cxr_224](utils.py 110): INFO => loaded successfully output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_6.pth (epoch 6)
[2025-11-19 15:09:18 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 44.996 (44.996)	Loss 0.2706 (0.2706)	Mem 4564MB
[2025-11-19 15:10:00 internimage_b_mimic_cxr_224](utils_multilabel.py 141): INFO  * Mean AUC-ROC 0.7978 Loss 0.2338
[2025-11-19 15:10:00 internimage_b_mimic_cxr_224](utils_multilabel.py 142): INFO  * Per-class AUC-ROC: ['0.8328', '0.8229', '0.8154', '0.9030', '0.7326', '0.5805', '0.7014', '0.7365', '0.8590', '0.9215', '0.7390', '0.7458', '0.8601', '0.9183']
[2025-11-19 15:10:00 internimage_b_mimic_cxr_224](main.py 283): INFO AUC-ROC of the network on the 2991 val images: 0.7978
Using EMA with decay = 0.99990000
[2025-11-19 15:10:01 internimage_b_mimic_cxr_224](utils.py 24): INFO ==============> Resuming form output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_6.pth....................
[2025-11-19 15:10:02 internimage_b_mimic_cxr_224](utils.py 44): INFO <All keys matched successfully>
[2025-11-19 15:10:02 internimage_b_mimic_cxr_224](utils.py 45): INFO Loaded state_dict_ema
[2025-11-19 15:10:11 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 8.631 (8.631)	Loss 0.2888 (0.2888)	Mem 4564MB
[2025-11-19 15:10:23 internimage_b_mimic_cxr_224](utils_multilabel.py 141): INFO  * Mean AUC-ROC 0.7852 Loss 0.2556
[2025-11-19 15:10:23 internimage_b_mimic_cxr_224](utils_multilabel.py 142): INFO  * Per-class AUC-ROC: ['0.8217', '0.8061', '0.7981', '0.8968', '0.7141', '0.6002', '0.6895', '0.7293', '0.8503', '0.9089', '0.7230', '0.7309', '0.8128', '0.9110']
[2025-11-19 15:10:23 internimage_b_mimic_cxr_224](main.py 315): INFO AUC-ROC of the ema network on the 2991 val images: 0.7852
[2025-11-19 15:10:23 internimage_b_mimic_cxr_224](main.py 331): INFO Start training
/scratch/smehta90/InternImage_MIMIC/classification/main.py:483: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
/scratch/smehta90/InternImage_MIMIC/classification/main.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
/home/smehta90/.conda/envs/INTERNIMAGE_ENV/lib/python3.12/site-packages/torch/autograd/graph.py:841: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1344, 896, 1, 1], strides() = [896, 1, 896, 896]
bucket_view.sizes() = [1344, 896, 1, 1], strides() = [896, 1, 1, 1] (Triggered internally at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:334.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-11-19 15:11:25 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][0/2882]	eta 2 days, 1:06:54 lr 0.000001	time 61.3514 (61.3514)	model_time 18.2880 (18.2880)	loss 0.2514 (0.2514)	grad_norm 0.5459 (0.5459/0.0000)	mem 34249MB
[2025-11-19 15:13:31 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][50/2882]	eta 2:53:50 lr 0.000001	time 0.6278 (3.6832)	model_time 0.6276 (0.9778)	loss 0.2531 (0.2381)	grad_norm 0.7327 (0.9594/0.2118)	mem 34631MB
[2025-11-19 15:15:43 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][100/2882]	eta 2:26:47 lr 0.000001	time 0.6326 (3.1658)	model_time 0.6323 (0.8054)	loss 0.2605 (0.2369)	grad_norm 1.1799 (0.9185/0.2321)	mem 34631MB
[2025-11-19 15:17:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][150/2882]	eta 2:14:38 lr 0.000001	time 0.6290 (2.9570)	model_time 0.6288 (0.7473)	loss 0.2294 (0.2367)	grad_norm 1.4800 (0.9170/0.2300)	mem 34631MB
[2025-11-19 15:19:55 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][200/2882]	eta 2:07:11 lr 0.000001	time 0.6343 (2.8453)	model_time 0.6340 (0.7191)	loss 0.2399 (0.2365)	grad_norm 1.0266 (0.9289/0.2257)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg'
[2025-11-19 15:22:01 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][250/2882]	eta 2:01:50 lr 0.000001	time 0.6308 (2.7777)	model_time 0.6305 (0.7015)	loss 0.2203 (0.2371)	grad_norm 0.8607 (0.9200/0.2282)	mem 34631MB
[2025-11-19 15:24:30 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][300/2882]	eta 2:01:05 lr 0.000001	time 17.8593 (2.8139)	model_time 0.6292 (0.6898)	loss 0.2186 (0.2361)	grad_norm 0.9109 (0.9125/0.2304)	mem 34631MB
[2025-11-19 15:26:57 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][350/2882]	eta 1:59:28 lr 0.000001	time 1.8395 (2.8311)	model_time 0.6309 (0.6818)	loss 0.2232 (0.2357)	grad_norm 0.7603 (0.9005/0.2358)	mem 34631MB
[2025-11-19 15:29:15 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][400/2882]	eta 1:56:41 lr 0.000001	time 0.6246 (2.8208)	model_time 0.6244 (0.6754)	loss 0.2135 (0.2356)	grad_norm 1.3137 (0.9108/0.2450)	mem 34631MB
[2025-11-19 15:31:24 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][450/2882]	eta 1:53:17 lr 0.000001	time 0.7090 (2.7949)	model_time 0.6317 (0.6706)	loss 0.2482 (0.2352)	grad_norm 1.3269 (0.9066/0.2455)	mem 34631MB
[2025-11-19 15:33:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][500/2882]	eta 1:51:31 lr 0.000001	time 0.6301 (2.8091)	model_time 0.6298 (0.6666)	loss 0.2404 (0.2354)	grad_norm 1.2238 (0.8872/0.2430)	mem 34631MB
[2025-11-19 15:36:11 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][550/2882]	eta 1:49:11 lr 0.000001	time 0.6295 (2.8092)	model_time 0.6292 (0.6636)	loss 0.2078 (0.2354)	grad_norm 1.1824 (0.8910/0.2488)	mem 34631MB
[2025-11-19 15:38:43 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][600/2882]	eta 1:47:34 lr 0.000001	time 9.3551 (2.8283)	model_time 0.6297 (0.6607)	loss 0.2352 (0.2355)	grad_norm 1.0504 (0.9012/0.2485)	mem 34631MB
[2025-11-19 15:41:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][650/2882]	eta 1:45:09 lr 0.000001	time 0.6321 (2.8269)	model_time 0.6318 (0.6584)	loss 0.2230 (0.2355)	grad_norm 0.8335 (0.9039/0.2473)	mem 34631MB
[2025-11-19 15:43:21 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][700/2882]	eta 1:42:34 lr 0.000001	time 0.6279 (2.8205)	model_time 0.6277 (0.6564)	loss 0.2460 (0.2358)	grad_norm 1.0286 (0.9046/0.2416)	mem 34631MB
[2025-11-19 15:45:38 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][750/2882]	eta 1:40:03 lr 0.000001	time 9.5856 (2.8159)	model_time 0.6339 (0.6547)	loss 0.2345 (0.2361)	grad_norm 0.8968 (0.9078/0.2480)	mem 34631MB
[2025-11-19 15:47:45 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][800/2882]	eta 1:37:06 lr 0.000001	time 0.6366 (2.7986)	model_time 0.6359 (0.6532)	loss 0.2461 (0.2363)	grad_norm 0.7534 (0.9183/0.2569)	mem 34631MB
[2025-11-19 15:49:58 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][850/2882]	eta 1:34:31 lr 0.000001	time 0.6365 (2.7909)	model_time 0.6362 (0.6521)	loss 0.2435 (0.2363)	grad_norm 0.8920 (0.9127/0.2470)	mem 34631MB
[2025-11-19 15:52:08 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][900/2882]	eta 1:31:48 lr 0.000001	time 0.6303 (2.7795)	model_time 0.6300 (0.6510)	loss 0.2818 (0.2363)	grad_norm 1.7221 (0.9042/0.2453)	mem 34631MB
[2025-11-19 15:54:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][950/2882]	eta 1:30:19 lr 0.000001	time 6.9053 (2.8053)	model_time 0.6317 (0.6500)	loss 0.2321 (0.2363)	grad_norm 0.7987 (0.9087/0.2463)	mem 34631MB
[2025-11-19 15:57:06 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1000/2882]	eta 1:27:48 lr 0.000001	time 0.6317 (2.7994)	model_time 0.6314 (0.6492)	loss 0.2462 (0.2362)	grad_norm 1.0639 (0.9010/0.2390)	mem 34631MB
[2025-11-19 15:59:24 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1050/2882]	eta 1:25:26 lr 0.000001	time 13.9631 (2.7982)	model_time 0.6317 (0.6485)	loss 0.2526 (0.2363)	grad_norm 1.0285 (0.8970/0.2237)	mem 34631MB
[2025-11-19 16:01:45 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1100/2882]	eta 1:23:08 lr 0.000001	time 0.6299 (2.7991)	model_time 0.6297 (0.6477)	loss 0.2345 (0.2362)	grad_norm 1.2987 (0.8958/0.2137)	mem 34631MB
[2025-11-19 16:04:00 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1150/2882]	eta 1:20:40 lr 0.000001	time 0.6311 (2.7949)	model_time 0.6309 (0.6472)	loss 0.2472 (0.2362)	grad_norm 0.6855 (0.8916/0.2189)	mem 34631MB
[2025-11-19 16:06:30 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1200/2882]	eta 1:18:34 lr 0.000001	time 0.6369 (2.8029)	model_time 0.6366 (0.6466)	loss 0.2344 (0.2363)	grad_norm 0.4583 (0.8883/0.2158)	mem 34631MB
[2025-11-19 16:09:06 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1250/2882]	eta 1:16:35 lr 0.000001	time 1.8816 (2.8159)	model_time 0.6343 (0.6461)	loss 0.2147 (0.2361)	grad_norm 0.4830 (0.8724/0.2149)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg'
[2025-11-19 16:11:30 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1300/2882]	eta 1:14:18 lr 0.000001	time 0.6309 (2.8183)	model_time 0.6307 (0.6457)	loss 0.2502 (0.2361)	grad_norm 1.1329 (0.8689/0.2156)	mem 34631MB
[2025-11-19 16:14:21 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1350/2882]	eta 1:12:32 lr 0.000001	time 27.4322 (2.8408)	model_time 0.6292 (0.6453)	loss 0.2356 (0.2361)	grad_norm 0.7954 (0.8690/0.2248)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg'
[2025-11-19 16:16:38 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1400/2882]	eta 1:10:04 lr 0.000001	time 0.6323 (2.8370)	model_time 0.6321 (0.6448)	loss 0.2361 (0.2362)	grad_norm 0.9847 (0.8692/0.2208)	mem 34631MB
[2025-11-19 16:19:03 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1450/2882]	eta 1:07:45 lr 0.000001	time 0.6286 (2.8389)	model_time 0.6284 (0.6445)	loss 0.2291 (0.2362)	grad_norm 0.4845 (0.8692/0.2209)	mem 34631MB
[2025-11-19 16:21:24 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1500/2882]	eta 1:05:22 lr 0.000001	time 0.6290 (2.8385)	model_time 0.6285 (0.6440)	loss 0.2555 (0.2361)	grad_norm 1.0143 (0.8801/0.2254)	mem 34631MB
[2025-11-19 16:23:52 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1550/2882]	eta 1:03:06 lr 0.000001	time 0.6362 (2.8426)	model_time 0.6359 (0.6438)	loss 0.2261 (0.2361)	grad_norm 1.2530 (0.8873/0.2184)	mem 34631MB
[2025-11-19 16:26:31 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1600/2882]	eta 1:00:57 lr 0.000001	time 0.6268 (2.8532)	model_time 0.6266 (0.6433)	loss 0.2493 (0.2361)	grad_norm 1.0573 (0.8823/0.2112)	mem 34631MB
[2025-11-19 16:29:41 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1650/2882]	eta 0:59:10 lr 0.000001	time 34.6042 (2.8819)	model_time 0.6352 (0.6430)	loss 0.2332 (0.2362)	grad_norm 0.8065 (0.8723/0.2055)	mem 34631MB
[2025-11-19 16:32:17 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1700/2882]	eta 0:56:54 lr 0.000001	time 0.6310 (2.8885)	model_time 0.6307 (0.6427)	loss 0.2201 (0.2361)	grad_norm 0.8273 (0.8683/0.2098)	mem 34631MB
[2025-11-19 16:34:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1750/2882]	eta 0:54:36 lr 0.000001	time 0.6323 (2.8942)	model_time 0.6321 (0.6424)	loss 0.2155 (0.2361)	grad_norm 1.1780 (0.8815/0.2100)	mem 34631MB
[2025-11-19 16:37:24 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1800/2882]	eta 0:52:16 lr 0.000001	time 0.6297 (2.8989)	model_time 0.6295 (0.6421)	loss 0.2523 (0.2361)	grad_norm 0.9464 (0.8727/0.2090)	mem 34631MB
[2025-11-19 16:39:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1850/2882]	eta 0:49:52 lr 0.000001	time 0.6302 (2.8995)	model_time 0.6300 (0.6419)	loss 0.2369 (0.2360)	grad_norm 1.1067 (0.8639/0.2161)	mem 34631MB
[2025-11-19 16:42:34 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1900/2882]	eta 0:47:36 lr 0.000001	time 0.6317 (2.9093)	model_time 0.6313 (0.6416)	loss 0.2352 (0.2359)	grad_norm 0.8288 (0.8850/0.2324)	mem 34631MB
[2025-11-19 16:45:58 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][1950/2882]	eta 0:45:39 lr 0.000001	time 32.7423 (2.9393)	model_time 0.6320 (0.6414)	loss 0.2360 (0.2360)	grad_norm 0.6122 (0.9061/0.2465)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg'
[2025-11-19 16:48:46 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2000/2882]	eta 0:43:21 lr 0.000001	time 0.6264 (2.9500)	model_time 0.6261 (0.6411)	loss 0.2391 (0.2360)	grad_norm 0.8459 (0.9068/0.2493)	mem 34631MB
[2025-11-19 16:51:57 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2050/2882]	eta 0:41:11 lr 0.000001	time 0.6287 (2.9709)	model_time 0.6284 (0.6409)	loss 0.2733 (0.2359)	grad_norm 1.2537 (0.8974/0.2495)	mem 34631MB
[2025-11-19 16:55:00 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2100/2882]	eta 0:38:56 lr 0.000001	time 0.6355 (2.9875)	model_time 0.6350 (0.6407)	loss 0.2584 (0.2359)	grad_norm 0.6014 (0.8873/0.2459)	mem 34631MB
[2025-11-19 16:58:02 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2150/2882]	eta 0:36:38 lr 0.000001	time 0.6323 (3.0028)	model_time 0.6321 (0.6405)	loss 0.2328 (0.2360)	grad_norm 1.1188 (0.8984/0.2379)	mem 34631MB
[2025-11-19 17:00:56 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2200/2882]	eta 0:34:15 lr 0.000001	time 0.6335 (3.0135)	model_time 0.6333 (0.6403)	loss 0.2316 (0.2360)	grad_norm 0.9768 (0.8868/0.2322)	mem 34631MB
[2025-11-19 17:04:14 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2250/2882]	eta 0:31:57 lr 0.000001	time 33.4490 (3.0343)	model_time 0.6303 (0.6402)	loss 0.2052 (0.2359)	grad_norm 0.8097 (0.8762/0.2223)	mem 34631MB
[2025-11-19 17:07:12 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2300/2882]	eta 0:29:32 lr 0.000001	time 0.6358 (3.0458)	model_time 0.6354 (0.6401)	loss 0.2338 (0.2360)	grad_norm 0.5017 (0.8780/0.2350)	mem 34631MB
[2025-11-19 17:09:55 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2350/2882]	eta 0:27:02 lr 0.000001	time 0.6246 (3.0507)	model_time 0.6243 (0.6400)	loss 0.2475 (0.2360)	grad_norm 0.6018 (0.8863/0.2361)	mem 34631MB
[2025-11-19 17:12:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2400/2882]	eta 0:24:34 lr 0.000001	time 0.6287 (3.0598)	model_time 0.6285 (0.6400)	loss 0.2370 (0.2360)	grad_norm 1.3874 (0.8967/0.2406)	mem 34631MB
[2025-11-19 17:15:54 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2450/2882]	eta 0:22:07 lr 0.000001	time 0.6303 (3.0726)	model_time 0.6298 (0.6399)	loss 0.2433 (0.2360)	grad_norm 1.0200 (0.8932/0.2475)	mem 34631MB
[2025-11-19 17:18:52 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2500/2882]	eta 0:19:37 lr 0.000001	time 0.6294 (3.0821)	model_time 0.6292 (0.6397)	loss 0.2275 (0.2360)	grad_norm 0.5301 (0.9009/0.2510)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg'
[2025-11-19 17:22:23 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2550/2882]	eta 0:17:10 lr 0.000001	time 37.0692 (3.1044)	model_time 0.6330 (0.6396)	loss 0.2463 (0.2361)	grad_norm 1.0227 (0.9084/0.2465)	mem 34631MB
[2025-11-19 17:25:15 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2600/2882]	eta 0:14:37 lr 0.000001	time 0.6258 (3.1108)	model_time 0.6256 (0.6394)	loss 0.2371 (0.2360)	grad_norm 0.7027 (0.9130/0.2346)	mem 34631MB
[2025-11-19 17:28:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2650/2882]	eta 0:12:03 lr 0.000001	time 0.6294 (3.1164)	model_time 0.6292 (0.6393)	loss 0.2399 (0.2360)	grad_norm 0.9554 (0.9058/0.2284)	mem 34631MB
[2025-11-19 17:31:06 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2700/2882]	eta 0:09:28 lr 0.000001	time 0.6330 (3.1257)	model_time 0.6328 (0.6391)	loss 0.2173 (0.2361)	grad_norm 0.8226 (0.9161/0.2233)	mem 34631MB
[2025-11-19 17:34:00 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2750/2882]	eta 0:06:53 lr 0.000001	time 0.6330 (3.1320)	model_time 0.6328 (0.6389)	loss 0.2442 (0.2361)	grad_norm 0.6490 (0.9031/0.2216)	mem 34631MB
[2025-11-19 17:36:26 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2800/2882]	eta 0:04:16 lr 0.000001	time 0.6329 (3.1284)	model_time 0.6327 (0.6388)	loss 0.2725 (0.2361)	grad_norm 1.8505 (0.8987/0.2194)	mem 34631MB
[2025-11-19 17:39:36 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [7/20][2850/2882]	eta 0:01:40 lr 0.000001	time 29.0855 (3.1401)	model_time 0.6320 (0.6387)	loss 0.2481 (0.2360)	grad_norm 0.8129 (0.8955/0.2225)	mem 34631MB
[2025-11-19 17:41:00 internimage_b_mimic_cxr_224](main.py 583): INFO EPOCH 7 training takes 2:30:36
[2025-11-19 17:41:00 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_7.pth saving......
[2025-11-19 17:41:02 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_7.pth saved !!!
[2025-11-19 17:41:37 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 35.155 (35.155)	Loss 0.2722 (0.2722)	Mem 34631MB
[2025-11-19 17:42:22 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:7] * Mean AUC-ROC 0.8030 Loss 0.2332
[2025-11-19 17:42:22 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:7] * Per-class AUC-ROC: ['0.8359', '0.8253', '0.8131', '0.9014', '0.7388', '0.5818', '0.7270', '0.7395', '0.8597', '0.9215', '0.7533', '0.7553', '0.8690', '0.9203']
[2025-11-19 17:42:22 internimage_b_mimic_cxr_224](main.py 365): INFO AUC-ROC of the network on the 2991 val images: 0.8030
[2025-11-19 17:42:22 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_best.pth saving......
[2025-11-19 17:42:24 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_best.pth saved !!!
[2025-11-19 17:42:24 internimage_b_mimic_cxr_224](main.py 380): INFO Max AUC-ROC: 0.8030
[2025-11-19 17:42:34 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 9.496 (9.496)	Loss 0.2826 (0.2826)	Mem 34631MB
[2025-11-19 17:42:50 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:7] * Mean AUC-ROC 0.7909 Loss 0.2476
[2025-11-19 17:42:50 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:7] * Per-class AUC-ROC: ['0.8265', '0.8136', '0.8044', '0.8991', '0.7216', '0.6022', '0.6949', '0.7323', '0.8534', '0.9138', '0.7286', '0.7369', '0.8314', '0.9137']
[2025-11-19 17:42:50 internimage_b_mimic_cxr_224](main.py 385): INFO AUC-ROC of the ema network on the 2991 val images: 0.7909
[2025-11-19 17:42:50 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saving......
[2025-11-19 17:42:52 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saved !!!
[2025-11-19 17:42:52 internimage_b_mimic_cxr_224](main.py 400): INFO Max ema AUC-ROC: 0.7909
/scratch/smehta90/InternImage_MIMIC/classification/main.py:483: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
/scratch/smehta90/InternImage_MIMIC/classification/main.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
[2025-11-19 17:43:34 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][0/2882]	eta 1 day, 9:59:45 lr 0.000001	time 42.4653 (42.4653)	model_time 0.6347 (0.6347)	loss 0.2426 (0.2426)	grad_norm 1.2685 (1.2685/0.0000)	mem 34631MB
[2025-11-19 17:46:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][50/2882]	eta 2:59:29 lr 0.000001	time 0.6304 (3.8029)	model_time 0.6301 (0.6324)	loss 0.2140 (0.2326)	grad_norm 0.9988 (0.8711/0.2176)	mem 34631MB
[2025-11-19 17:48:43 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][100/2882]	eta 2:41:27 lr 0.000001	time 0.6326 (3.4823)	model_time 0.6323 (0.6341)	loss 0.2298 (0.2337)	grad_norm 0.8501 (0.8839/0.2333)	mem 34631MB
[2025-11-19 17:51:06 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][150/2882]	eta 2:29:11 lr 0.000001	time 0.6362 (3.2766)	model_time 0.6356 (0.6335)	loss 0.2309 (0.2323)	grad_norm 0.8458 (0.8769/0.2352)	mem 34631MB
[2025-11-19 17:53:34 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][200/2882]	eta 2:22:54 lr 0.000001	time 0.6360 (3.1970)	model_time 0.6357 (0.6334)	loss 0.2362 (0.2334)	grad_norm 1.1175 (0.8949/0.2357)	mem 34631MB
[2025-11-19 17:56:07 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][250/2882]	eta 2:19:01 lr 0.000001	time 0.6362 (3.1693)	model_time 0.6360 (0.6333)	loss 0.2320 (0.2344)	grad_norm 0.6658 (0.8834/0.2295)	mem 34631MB
[2025-11-19 17:59:16 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][300/2882]	eta 2:20:40 lr 0.000001	time 30.8327 (3.2691)	model_time 0.6369 (0.6333)	loss 0.2468 (0.2356)	grad_norm 1.1478 (0.8833/0.2246)	mem 34631MB
[2025-11-19 18:01:53 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][350/2882]	eta 2:17:11 lr 0.000001	time 0.6306 (3.2510)	model_time 0.6303 (0.6333)	loss 0.2219 (0.2350)	grad_norm 0.5708 (0.8851/0.2308)	mem 34631MB
[2025-11-19 18:04:42 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][400/2882]	eta 2:15:09 lr 0.000001	time 0.6349 (3.2674)	model_time 0.6346 (0.6335)	loss 0.2404 (0.2355)	grad_norm 1.1749 (0.8826/0.2324)	mem 34631MB
[2025-11-19 18:07:20 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][450/2882]	eta 2:11:59 lr 0.000001	time 0.6286 (3.2563)	model_time 0.6283 (0.6335)	loss 0.2180 (0.2352)	grad_norm 0.7841 (0.8900/0.2286)	mem 34631MB
[2025-11-19 18:10:10 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][500/2882]	eta 2:09:50 lr 0.000001	time 10.8198 (3.2707)	model_time 0.6317 (0.6337)	loss 0.2475 (0.2356)	grad_norm 0.7154 (0.8877/0.2342)	mem 34631MB
[2025-11-19 18:12:52 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][550/2882]	eta 2:06:58 lr 0.000001	time 0.6356 (3.2668)	model_time 0.6352 (0.6338)	loss 0.2182 (0.2356)	grad_norm 0.7033 (0.8974/0.2463)	mem 34631MB
[2025-11-19 18:15:38 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][600/2882]	eta 2:04:28 lr 0.000001	time 5.0846 (3.2726)	model_time 0.6334 (0.6337)	loss 0.2323 (0.2356)	grad_norm 0.8386 (0.9039/0.2590)	mem 34631MB
[2025-11-19 18:18:29 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][650/2882]	eta 2:02:08 lr 0.000001	time 0.6547 (3.2832)	model_time 0.6545 (0.6340)	loss 0.2386 (0.2355)	grad_norm 0.4963 (0.9119/0.2544)	mem 34631MB
[2025-11-19 18:21:14 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][700/2882]	eta 1:59:27 lr 0.000001	time 0.6353 (3.2849)	model_time 0.6350 (0.6344)	loss 0.2426 (0.2355)	grad_norm 0.9094 (0.9113/0.2499)	mem 34631MB
[2025-11-19 18:24:06 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][750/2882]	eta 1:57:06 lr 0.000001	time 0.6338 (3.2955)	model_time 0.6336 (0.6345)	loss 0.2273 (0.2358)	grad_norm 1.0090 (0.9055/0.2538)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg'
[2025-11-19 18:27:16 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][800/2882]	eta 1:55:26 lr 0.000001	time 14.9864 (3.3266)	model_time 0.6331 (0.6345)	loss 0.2451 (0.2357)	grad_norm 1.0437 (0.8940/0.2406)	mem 34631MB
[2025-11-19 18:30:08 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][850/2882]	eta 1:52:52 lr 0.000001	time 0.6276 (3.3328)	model_time 0.6274 (0.6346)	loss 0.2466 (0.2355)	grad_norm 0.9091 (0.8975/0.2334)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg'
[2025-11-19 18:32:56 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][900/2882]	eta 1:50:09 lr 0.000001	time 10.2402 (3.3349)	model_time 0.6335 (0.6345)	loss 0.2300 (0.2355)	grad_norm 1.2689 (0.8898/0.2219)	mem 34631MB
[2025-11-19 18:35:17 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][950/2882]	eta 1:46:30 lr 0.000001	time 0.6318 (3.3077)	model_time 0.6316 (0.6344)	loss 0.2369 (0.2355)	grad_norm 1.4820 (0.8825/0.2209)	mem 34631MB
[2025-11-19 18:37:44 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1000/2882]	eta 1:43:10 lr 0.000001	time 9.3148 (3.2891)	model_time 0.6324 (0.6345)	loss 0.2277 (0.2354)	grad_norm 1.0436 (0.8900/0.2218)	mem 34631MB
[2025-11-19 18:40:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1050/2882]	eta 1:39:43 lr 0.000001	time 4.0032 (3.2663)	model_time 0.6338 (0.6344)	loss 0.2328 (0.2354)	grad_norm 0.8337 (0.8941/0.2197)	mem 34631MB
[2025-11-19 18:42:46 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1100/2882]	eta 1:36:58 lr 0.000001	time 15.6111 (3.2650)	model_time 0.6301 (0.6343)	loss 0.2368 (0.2354)	grad_norm 0.7869 (0.8845/0.2298)	mem 34631MB
[2025-11-19 18:45:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1150/2882]	eta 1:33:37 lr 0.000001	time 0.6319 (3.2432)	model_time 0.6316 (0.6343)	loss 0.2076 (0.2354)	grad_norm 0.7722 (0.8796/0.2251)	mem 34631MB
[2025-11-19 18:47:26 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1200/2882]	eta 1:30:26 lr 0.000001	time 0.6358 (3.2263)	model_time 0.6355 (0.6342)	loss 0.2428 (0.2355)	grad_norm 0.8124 (0.8727/0.2210)	mem 34631MB
[2025-11-19 18:49:52 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1250/2882]	eta 1:27:25 lr 0.000001	time 0.6347 (3.2142)	model_time 0.6344 (0.6342)	loss 0.2196 (0.2353)	grad_norm 0.7210 (0.8681/0.2214)	mem 34631MB
[2025-11-19 18:52:28 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1300/2882]	eta 1:24:38 lr 0.000001	time 3.7545 (3.2102)	model_time 0.6321 (0.6341)	loss 0.2448 (0.2354)	grad_norm 0.7238 (0.8594/0.2201)	mem 34631MB
[2025-11-19 18:55:02 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1350/2882]	eta 1:21:50 lr 0.000001	time 9.7467 (3.2054)	model_time 0.6318 (0.6340)	loss 0.2214 (0.2355)	grad_norm 0.8100 (0.8549/0.2198)	mem 34631MB
[2025-11-19 18:57:25 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1400/2882]	eta 1:18:52 lr 0.000001	time 4.8251 (3.1933)	model_time 0.6308 (0.6340)	loss 0.2262 (0.2355)	grad_norm 1.7085 (0.8622/0.2245)	mem 34631MB
[2025-11-19 18:59:40 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1450/2882]	eta 1:15:48 lr 0.000001	time 0.6387 (3.1761)	model_time 0.6385 (0.6339)	loss 0.2514 (0.2354)	grad_norm 1.4509 (0.8572/0.2261)	mem 34631MB
[2025-11-19 19:02:00 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1500/2882]	eta 1:12:51 lr 0.000001	time 0.6371 (3.1634)	model_time 0.6368 (0.6340)	loss 0.2399 (0.2354)	grad_norm 0.9431 (0.8665/0.2281)	mem 34631MB
[2025-11-19 19:04:22 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1550/2882]	eta 1:10:00 lr 0.000001	time 3.9086 (3.1534)	model_time 0.6332 (0.6340)	loss 0.2402 (0.2354)	grad_norm 0.5764 (0.8713/0.2288)	mem 34631MB
[2025-11-19 19:06:53 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1600/2882]	eta 1:07:17 lr 0.000001	time 0.6315 (3.1491)	model_time 0.6313 (0.6339)	loss 0.2627 (0.2354)	grad_norm 0.4997 (0.8795/0.2323)	mem 34631MB
[2025-11-19 19:09:18 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1650/2882]	eta 1:04:30 lr 0.000001	time 2.1151 (3.1413)	model_time 0.6323 (0.6339)	loss 0.2250 (0.2354)	grad_norm 0.8827 (0.8672/0.2340)	mem 34631MB
[2025-11-19 19:11:33 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1700/2882]	eta 1:01:37 lr 0.000001	time 0.9636 (3.1284)	model_time 0.6399 (0.6340)	loss 0.2140 (0.2355)	grad_norm 0.6395 (0.8807/0.2261)	mem 34631MB
[2025-11-19 19:13:56 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1750/2882]	eta 0:58:52 lr 0.000001	time 0.6294 (3.1208)	model_time 0.6292 (0.6340)	loss 0.2501 (0.2356)	grad_norm 0.6741 (0.8849/0.2204)	mem 34631MB
[2025-11-19 19:16:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1800/2882]	eta 0:56:00 lr 0.000001	time 0.6345 (3.1055)	model_time 0.6342 (0.6340)	loss 0.2142 (0.2355)	grad_norm 0.5362 (0.8817/0.2153)	mem 34631MB
[2025-11-19 19:18:13 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1850/2882]	eta 0:53:09 lr 0.000001	time 0.6301 (3.0910)	model_time 0.6299 (0.6340)	loss 0.2227 (0.2355)	grad_norm 1.3321 (0.8960/0.2220)	mem 34631MB
[2025-11-19 19:20:45 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1900/2882]	eta 0:50:33 lr 0.000001	time 0.6308 (3.0894)	model_time 0.6305 (0.6340)	loss 0.2045 (0.2355)	grad_norm 0.6941 (0.8876/0.2253)	mem 34631MB
[2025-11-19 19:23:29 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][1950/2882]	eta 0:48:04 lr 0.000001	time 17.0741 (3.0945)	model_time 0.8179 (0.6341)	loss 0.2101 (0.2354)	grad_norm 0.8199 (0.8993/0.2147)	mem 34631MB
[2025-11-19 19:26:12 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2000/2882]	eta 0:45:33 lr 0.000001	time 3.9442 (3.0988)	model_time 0.6327 (0.6341)	loss 0.2141 (0.2354)	grad_norm 1.0292 (0.8951/0.2185)	mem 34631MB
[2025-11-19 19:28:37 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2050/2882]	eta 0:42:54 lr 0.000001	time 0.6345 (3.0939)	model_time 0.6342 (0.6341)	loss 0.1966 (0.2354)	grad_norm 0.9904 (0.8905/0.2270)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg'
[2025-11-19 19:31:13 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2100/2882]	eta 0:40:19 lr 0.000001	time 0.6295 (3.0945)	model_time 0.6293 (0.6341)	loss 0.2756 (0.2354)	grad_norm 1.0435 (0.9026/0.2350)	mem 34631MB
[2025-11-19 19:33:54 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2150/2882]	eta 0:37:47 lr 0.000001	time 0.6339 (3.0973)	model_time 0.6336 (0.6341)	loss 0.2138 (0.2353)	grad_norm 1.5906 (0.8999/0.2446)	mem 34631MB
[2025-11-19 19:36:40 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2200/2882]	eta 0:35:15 lr 0.000001	time 0.6323 (3.1024)	model_time 0.6320 (0.6340)	loss 0.2527 (0.2353)	grad_norm 0.8687 (0.9086/0.2425)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg'
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg'
[2025-11-19 19:39:41 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2250/2882]	eta 0:32:48 lr 0.000001	time 27.1653 (3.1140)	model_time 0.6306 (0.6340)	loss 0.2330 (0.2352)	grad_norm 0.7077 (0.9065/0.2454)	mem 34631MB
[2025-11-19 19:42:21 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2300/2882]	eta 0:30:13 lr 0.000001	time 0.6279 (3.1157)	model_time 0.6277 (0.6340)	loss 0.2325 (0.2352)	grad_norm 0.6666 (0.9066/0.2446)	mem 34631MB
[2025-11-19 19:45:00 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2350/2882]	eta 0:27:38 lr 0.000001	time 0.6429 (3.1170)	model_time 0.6426 (0.6341)	loss 0.2326 (0.2352)	grad_norm 0.8938 (0.9165/0.2444)	mem 34631MB
[2025-11-19 19:47:38 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2400/2882]	eta 0:25:02 lr 0.000001	time 0.6413 (3.1180)	model_time 0.6411 (0.6341)	loss 0.2499 (0.2352)	grad_norm 0.7664 (0.9185/0.2560)	mem 34631MB
[2025-11-19 19:50:29 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2450/2882]	eta 0:22:29 lr 0.000001	time 0.6369 (3.1244)	model_time 0.6366 (0.6341)	loss 0.2185 (0.2351)	grad_norm 0.6878 (0.9127/0.2414)	mem 34631MB
[2025-11-19 19:53:12 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2500/2882]	eta 0:19:54 lr 0.000001	time 0.6401 (3.1269)	model_time 0.6398 (0.6342)	loss 0.2385 (0.2351)	grad_norm 1.4626 (0.9059/0.2329)	mem 34631MB
[2025-11-19 19:56:10 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2550/2882]	eta 0:17:20 lr 0.000001	time 26.5767 (3.1354)	model_time 0.6422 (0.6342)	loss 0.2121 (0.2350)	grad_norm 0.7425 (0.9109/0.2333)	mem 34631MB
[2025-11-19 19:58:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2600/2882]	eta 0:14:44 lr 0.000001	time 0.6325 (3.1366)	model_time 0.6322 (0.6342)	loss 0.2509 (0.2350)	grad_norm 0.8805 (0.9071/0.2336)	mem 34631MB
[2025-11-19 20:01:21 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2650/2882]	eta 0:12:07 lr 0.000001	time 0.6356 (3.1343)	model_time 0.6353 (0.6342)	loss 0.2128 (0.2350)	grad_norm 0.9569 (0.9037/0.2310)	mem 34631MB
[2025-11-19 20:04:03 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2700/2882]	eta 0:09:30 lr 0.000001	time 0.6389 (3.1364)	model_time 0.6386 (0.6342)	loss 0.2349 (0.2350)	grad_norm 0.9386 (0.9082/0.2210)	mem 34631MB
[2025-11-19 20:06:37 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2750/2882]	eta 0:06:53 lr 0.000001	time 0.6302 (3.1355)	model_time 0.6300 (0.6342)	loss 0.2411 (0.2351)	grad_norm 0.5487 (0.9059/0.2161)	mem 34631MB
[2025-11-19 20:09:16 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2800/2882]	eta 0:04:17 lr 0.000001	time 0.6353 (3.1360)	model_time 0.6350 (0.6342)	loss 0.2075 (0.2351)	grad_norm 1.1257 (0.9058/0.2185)	mem 34631MB
[2025-11-19 20:12:18 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [8/20][2850/2882]	eta 0:01:40 lr 0.000001	time 19.8078 (3.1450)	model_time 0.6317 (0.6342)	loss 0.2184 (0.2351)	grad_norm 0.9762 (0.9100/0.2192)	mem 34631MB
[2025-11-19 20:13:47 internimage_b_mimic_cxr_224](main.py 583): INFO EPOCH 8 training takes 2:30:55
[2025-11-19 20:13:47 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_8.pth saving......
[2025-11-19 20:13:50 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_8.pth saved !!!
[2025-11-19 20:14:29 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 39.514 (39.514)	Loss 0.2680 (0.2680)	Mem 34631MB
[2025-11-19 20:15:15 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:8] * Mean AUC-ROC 0.8058 Loss 0.2312
[2025-11-19 20:15:15 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:8] * Per-class AUC-ROC: ['0.8366', '0.8265', '0.8128', '0.9031', '0.7386', '0.5782', '0.7410', '0.7422', '0.8624', '0.9229', '0.7603', '0.7553', '0.8791', '0.9220']
[2025-11-19 20:15:15 internimage_b_mimic_cxr_224](main.py 365): INFO AUC-ROC of the network on the 2991 val images: 0.8058
[2025-11-19 20:15:15 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_best.pth saving......
[2025-11-19 20:15:18 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_best.pth saved !!!
[2025-11-19 20:15:18 internimage_b_mimic_cxr_224](main.py 380): INFO Max AUC-ROC: 0.8058
[2025-11-19 20:15:29 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 10.843 (10.843)	Loss 0.2786 (0.2786)	Mem 34631MB
[2025-11-19 20:15:45 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:8] * Mean AUC-ROC 0.7953 Loss 0.2424
[2025-11-19 20:15:45 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:8] * Per-class AUC-ROC: ['0.8298', '0.8189', '0.8092', '0.9008', '0.7274', '0.6019', '0.7004', '0.7350', '0.8559', '0.9169', '0.7340', '0.7428', '0.8459', '0.9157']
[2025-11-19 20:15:45 internimage_b_mimic_cxr_224](main.py 385): INFO AUC-ROC of the ema network on the 2991 val images: 0.7953
[2025-11-19 20:15:45 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saving......
[2025-11-19 20:15:47 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saved !!!
[2025-11-19 20:15:47 internimage_b_mimic_cxr_224](main.py 400): INFO Max ema AUC-ROC: 0.7953
/scratch/smehta90/InternImage_MIMIC/classification/main.py:483: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
/scratch/smehta90/InternImage_MIMIC/classification/main.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
[2025-11-19 20:16:28 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][0/2882]	eta 1 day, 8:55:03 lr 0.000001	time 41.1184 (41.1184)	model_time 0.6376 (0.6376)	loss 0.2228 (0.2228)	grad_norm 0.6860 (0.6860/0.0000)	mem 34631MB
[2025-11-19 20:19:15 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][50/2882]	eta 3:13:00 lr 0.000001	time 0.6311 (4.0892)	model_time 0.6309 (0.6307)	loss 0.2316 (0.2332)	grad_norm 0.9216 (0.9052/0.2880)	mem 34631MB
[2025-11-19 20:21:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][100/2882]	eta 2:47:15 lr 0.000001	time 0.6275 (3.6074)	model_time 0.6273 (0.6311)	loss 0.2080 (0.2340)	grad_norm 0.6896 (0.8620/0.2736)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg'
[2025-11-19 20:24:20 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][150/2882]	eta 2:34:50 lr 0.000001	time 0.6313 (3.4006)	model_time 0.6309 (0.6328)	loss 0.2016 (0.2360)	grad_norm 0.9373 (0.8755/0.2583)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg'
[2025-11-19 20:26:52 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][200/2882]	eta 2:27:50 lr 0.000001	time 0.6300 (3.3073)	model_time 0.6298 (0.6329)	loss 0.2318 (0.2353)	grad_norm 0.6347 (0.8749/0.2524)	mem 34631MB
[2025-11-19 20:29:16 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][250/2882]	eta 2:21:20 lr 0.000001	time 0.6353 (3.2223)	model_time 0.6350 (0.6333)	loss 0.2249 (0.2355)	grad_norm 0.5650 (0.8847/0.2480)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg'
[2025-11-19 20:32:21 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][300/2882]	eta 2:22:06 lr 0.000001	time 32.0597 (3.3024)	model_time 0.6356 (0.6334)	loss 0.2370 (0.2354)	grad_norm 1.2513 (0.8836/0.2439)	mem 34631MB
[2025-11-19 20:34:48 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][350/2882]	eta 2:17:13 lr 0.000001	time 0.6316 (3.2519)	model_time 0.6314 (0.6333)	loss 0.2315 (0.2347)	grad_norm 1.0189 (0.8817/0.2340)	mem 34631MB
[2025-11-19 20:37:12 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][400/2882]	eta 2:12:36 lr 0.000001	time 0.6349 (3.2059)	model_time 0.6347 (0.6331)	loss 0.2255 (0.2344)	grad_norm 0.6750 (0.8990/0.2251)	mem 34631MB
[2025-11-19 20:39:28 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][450/2882]	eta 2:07:46 lr 0.000001	time 0.6285 (3.1521)	model_time 0.6283 (0.6328)	loss 0.2482 (0.2344)	grad_norm 0.8720 (0.8930/0.2257)	mem 34631MB
[2025-11-19 20:41:49 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][500/2882]	eta 2:03:45 lr 0.000001	time 0.6308 (3.1172)	model_time 0.6306 (0.6326)	loss 0.2458 (0.2346)	grad_norm 0.9952 (0.9019/0.2251)	mem 34631MB
[2025-11-19 20:44:00 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][550/2882]	eta 1:59:27 lr 0.000001	time 0.6342 (3.0737)	model_time 0.6339 (0.6329)	loss 0.2684 (0.2344)	grad_norm 0.8550 (0.9012/0.2358)	mem 34631MB
[2025-11-19 20:46:39 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][600/2882]	eta 1:57:13 lr 0.000001	time 25.3175 (3.0824)	model_time 0.6348 (0.6328)	loss 0.2020 (0.2344)	grad_norm 0.9066 (0.9102/0.2436)	mem 34631MB
[2025-11-19 20:48:52 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][650/2882]	eta 1:53:26 lr 0.000001	time 0.6354 (3.0495)	model_time 0.6351 (0.6328)	loss 0.2227 (0.2345)	grad_norm 0.6829 (0.9113/0.2385)	mem 34631MB
[2025-11-19 20:51:06 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][700/2882]	eta 1:49:54 lr 0.000001	time 0.6340 (3.0223)	model_time 0.6338 (0.6331)	loss 0.2216 (0.2343)	grad_norm 0.8900 (0.9110/0.2424)	mem 34631MB
[2025-11-19 20:53:29 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][750/2882]	eta 1:47:02 lr 0.000001	time 0.6337 (3.0125)	model_time 0.6334 (0.6331)	loss 0.2339 (0.2341)	grad_norm 1.0322 (0.9229/0.2491)	mem 34631MB
[2025-11-19 20:55:45 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][800/2882]	eta 1:43:52 lr 0.000001	time 0.6327 (2.9934)	model_time 0.6324 (0.6330)	loss 0.2626 (0.2342)	grad_norm 1.2934 (0.9249/0.2503)	mem 34631MB
[2025-11-19 20:58:00 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][850/2882]	eta 1:40:49 lr 0.000001	time 0.6301 (2.9770)	model_time 0.6296 (0.6332)	loss 0.2366 (0.2343)	grad_norm 0.8829 (0.9178/0.2492)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg'
[2025-11-19 21:00:36 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][900/2882]	eta 1:38:36 lr 0.000001	time 25.0820 (2.9850)	model_time 0.5491 (0.6331)	loss 0.2294 (0.2343)	grad_norm inf (0.9073/0.2442)	mem 34631MB
[2025-11-19 21:02:49 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][950/2882]	eta 1:35:33 lr 0.000001	time 0.6314 (2.9678)	model_time 0.6312 (0.6331)	loss 0.2373 (0.2344)	grad_norm 1.3237 (0.9055/0.2546)	mem 34631MB
[2025-11-19 21:05:29 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1000/2882]	eta 1:33:26 lr 0.000001	time 0.6287 (2.9792)	model_time 0.6286 (0.6332)	loss 0.2184 (0.2344)	grad_norm 1.1787 (0.9013/0.2583)	mem 34631MB
[2025-11-19 21:07:59 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1050/2882]	eta 1:31:00 lr 0.000001	time 8.2915 (2.9805)	model_time 0.6370 (0.6332)	loss 0.2101 (0.2347)	grad_norm 0.6133 (0.8967/0.2521)	mem 34631MB
[2025-11-19 21:10:18 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1100/2882]	eta 1:28:13 lr 0.000001	time 0.6363 (2.9706)	model_time 0.6359 (0.6332)	loss 0.2263 (0.2347)	grad_norm 1.1539 (0.8968/0.2479)	mem 34631MB
[2025-11-19 21:12:28 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1150/2882]	eta 1:25:18 lr 0.000001	time 0.6359 (2.9553)	model_time 0.6356 (0.6334)	loss 0.2277 (0.2345)	grad_norm 0.6275 (0.8872/0.2346)	mem 34631MB
[2025-11-19 21:14:59 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1200/2882]	eta 1:22:55 lr 0.000001	time 16.9952 (2.9580)	model_time 0.6365 (0.6334)	loss 0.2264 (0.2343)	grad_norm 1.0123 (0.8897/0.2342)	mem 34631MB
[2025-11-19 21:17:08 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1250/2882]	eta 1:20:02 lr 0.000001	time 0.6304 (2.9425)	model_time 0.6302 (0.6336)	loss 0.2393 (0.2344)	grad_norm 0.7754 (0.8949/0.2377)	mem 34631MB
[2025-11-19 21:19:24 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1300/2882]	eta 1:17:21 lr 0.000001	time 0.6288 (2.9340)	model_time 0.6286 (0.6336)	loss 0.2128 (0.2344)	grad_norm 0.9771 (0.8913/0.2255)	mem 34631MB
[2025-11-19 21:22:07 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1350/2882]	eta 1:15:13 lr 0.000001	time 18.7390 (2.9461)	model_time 0.6458 (0.6337)	loss 0.2491 (0.2345)	grad_norm 0.9391 (0.8908/0.2321)	mem 34631MB
[2025-11-19 21:24:37 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1400/2882]	eta 1:12:48 lr 0.000001	time 0.6314 (2.9479)	model_time 0.6312 (0.6338)	loss 0.2050 (0.2345)	grad_norm 0.8203 (0.8872/0.2344)	mem 34631MB
[2025-11-19 21:27:18 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1450/2882]	eta 1:10:34 lr 0.000001	time 0.6326 (2.9571)	model_time 0.6323 (0.6337)	loss 0.2241 (0.2345)	grad_norm 0.7039 (0.8925/0.2425)	mem 34631MB
[2025-11-19 21:29:43 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1500/2882]	eta 1:08:04 lr 0.000001	time 4.4949 (2.9555)	model_time 0.6333 (0.6338)	loss 0.2369 (0.2345)	grad_norm 0.5214 (0.8925/0.2456)	mem 34631MB
[2025-11-19 21:32:01 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1550/2882]	eta 1:05:27 lr 0.000001	time 0.6326 (2.9489)	model_time 0.6323 (0.6339)	loss 0.2449 (0.2345)	grad_norm 1.2371 (0.8890/0.2347)	mem 34631MB
[2025-11-19 21:34:17 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1600/2882]	eta 1:02:51 lr 0.000001	time 0.6356 (2.9419)	model_time 0.6353 (0.6339)	loss 0.2452 (0.2345)	grad_norm 1.1397 (0.8948/0.2474)	mem 34631MB
[2025-11-19 21:36:57 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1650/2882]	eta 1:00:33 lr 0.000001	time 12.1238 (2.9496)	model_time 0.6319 (0.6340)	loss 0.2292 (0.2344)	grad_norm 0.6553 (0.8967/0.2487)	mem 34631MB
[2025-11-19 21:39:22 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1700/2882]	eta 0:58:05 lr 0.000001	time 0.6651 (2.9485)	model_time 0.6649 (0.6340)	loss 0.2371 (0.2345)	grad_norm 0.6939 (0.9016/0.2569)	mem 34631MB
[2025-11-19 21:41:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1750/2882]	eta 0:55:37 lr 0.000001	time 0.6303 (2.9487)	model_time 0.6300 (0.6340)	loss 0.2086 (0.2344)	grad_norm 0.9509 (0.9055/0.2495)	mem 34631MB
[2025-11-19 21:44:23 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1800/2882]	eta 0:53:13 lr 0.000001	time 5.9218 (2.9517)	model_time 0.6334 (0.6339)	loss 0.2095 (0.2343)	grad_norm 0.6187 (0.9184/0.2507)	mem 34631MB
[2025-11-19 21:46:49 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1850/2882]	eta 0:50:45 lr 0.000001	time 0.6359 (2.9507)	model_time 0.6355 (0.6340)	loss 0.2505 (0.2343)	grad_norm 0.9334 (0.9215/0.2504)	mem 34631MB
[2025-11-19 21:49:13 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1900/2882]	eta 0:48:15 lr 0.000001	time 0.6314 (2.9489)	model_time 0.6312 (0.6340)	loss 0.2310 (0.2343)	grad_norm 0.9631 (0.9259/0.2480)	mem 34631MB
[2025-11-19 21:52:10 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][1950/2882]	eta 0:46:02 lr 0.000001	time 8.7374 (2.9640)	model_time 0.6331 (0.6339)	loss 0.2326 (0.2342)	grad_norm 0.6882 (0.9245/0.2437)	mem 34631MB
[2025-11-19 21:54:19 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2000/2882]	eta 0:43:26 lr 0.000001	time 0.6275 (2.9548)	model_time 0.6273 (0.6339)	loss 0.2500 (0.2343)	grad_norm 1.0418 (0.9310/0.2398)	mem 34631MB
[2025-11-19 21:56:41 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2050/2882]	eta 0:40:56 lr 0.000001	time 0.6333 (2.9520)	model_time 0.6329 (0.6338)	loss 0.2355 (0.2343)	grad_norm 0.8971 (0.9481/0.2497)	mem 34631MB
[2025-11-19 21:59:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2100/2882]	eta 0:38:27 lr 0.000001	time 0.6341 (2.9501)	model_time 0.6338 (0.6339)	loss 0.2242 (0.2342)	grad_norm 0.6025 (0.9350/0.2535)	mem 34631MB
[2025-11-19 22:01:27 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2150/2882]	eta 0:35:57 lr 0.000001	time 0.6360 (2.9475)	model_time 0.6357 (0.6338)	loss 0.2179 (0.2342)	grad_norm 0.9461 (0.9370/0.2621)	mem 34631MB
[2025-11-19 22:03:42 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2200/2882]	eta 0:33:26 lr 0.000001	time 0.7525 (2.9418)	model_time 0.7522 (0.6339)	loss 0.2185 (0.2341)	grad_norm 1.0516 (0.9266/0.2608)	mem 34631MB
[2025-11-19 22:06:25 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2250/2882]	eta 0:31:03 lr 0.000001	time 0.6313 (2.9489)	model_time 0.6311 (0.6339)	loss 0.2509 (0.2342)	grad_norm 1.0798 (0.9366/0.2679)	mem 34631MB
[2025-11-19 22:08:44 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2300/2882]	eta 0:28:34 lr 0.000001	time 0.6320 (2.9455)	model_time 0.6318 (0.6338)	loss 0.2600 (0.2342)	grad_norm 0.9418 (0.9307/0.2680)	mem 34631MB
[2025-11-19 22:11:09 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2350/2882]	eta 0:26:06 lr 0.000001	time 0.6316 (2.9445)	model_time 0.6313 (0.6338)	loss 0.2223 (0.2342)	grad_norm 0.9716 (0.9137/0.2663)	mem 34631MB
[2025-11-19 22:13:35 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2400/2882]	eta 0:23:38 lr 0.000001	time 0.6307 (2.9438)	model_time 0.6304 (0.6338)	loss 0.2177 (0.2342)	grad_norm 1.0093 (0.9147/0.2598)	mem 34631MB
[2025-11-19 22:16:06 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2450/2882]	eta 0:21:12 lr 0.000001	time 0.6313 (2.9452)	model_time 0.6311 (0.6337)	loss 0.2528 (0.2343)	grad_norm 0.6458 (0.9027/0.2499)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg'
[2025-11-19 22:18:31 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2500/2882]	eta 0:18:44 lr 0.000001	time 0.6363 (2.9444)	model_time 0.6360 (0.6337)	loss 0.2511 (0.2342)	grad_norm 0.7200 (0.9091/0.2498)	mem 34631MB
[2025-11-19 22:21:24 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2550/2882]	eta 0:16:20 lr 0.000001	time 24.1760 (2.9544)	model_time 0.6303 (0.6336)	loss 0.2214 (0.2343)	grad_norm 0.3601 (0.8972/0.2454)	mem 34631MB
[2025-11-19 22:23:44 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2600/2882]	eta 0:13:52 lr 0.000001	time 0.6313 (2.9514)	model_time 0.6310 (0.6336)	loss 0.2113 (0.2343)	grad_norm 0.4848 (0.8857/0.2421)	mem 34631MB
[2025-11-19 22:26:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2650/2882]	eta 0:11:24 lr 0.000001	time 0.6297 (2.9491)	model_time 0.6294 (0.6336)	loss 0.2629 (0.2342)	grad_norm 0.8910 (0.8920/0.2371)	mem 34631MB
[2025-11-19 22:28:23 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2700/2882]	eta 0:08:56 lr 0.000001	time 0.6295 (2.9458)	model_time 0.6293 (0.6336)	loss 0.2303 (0.2343)	grad_norm 0.7126 (0.9056/0.2427)	mem 34631MB
[2025-11-19 22:30:43 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2750/2882]	eta 0:06:28 lr 0.000001	time 0.6313 (2.9429)	model_time 0.6311 (0.6336)	loss 0.2469 (0.2342)	grad_norm 0.8282 (0.9090/0.2460)	mem 34631MB
[2025-11-19 22:33:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2800/2882]	eta 0:04:01 lr 0.000001	time 0.6271 (2.9412)	model_time 0.6269 (0.6335)	loss 0.2348 (0.2342)	grad_norm 0.8786 (0.9144/0.2448)	mem 34631MB
[2025-11-19 22:36:00 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [9/20][2850/2882]	eta 0:01:34 lr 0.000001	time 28.5912 (2.9510)	model_time 0.6295 (0.6335)	loss 0.2594 (0.2341)	grad_norm 1.1671 (0.9009/0.2372)	mem 34631MB
[2025-11-19 22:37:14 internimage_b_mimic_cxr_224](main.py 583): INFO EPOCH 9 training takes 2:21:27
[2025-11-19 22:37:14 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_9.pth saving......
[2025-11-19 22:37:16 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_9.pth saved !!!
[2025-11-19 22:37:52 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 35.511 (35.511)	Loss 0.2741 (0.2741)	Mem 34631MB
[2025-11-19 22:38:35 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:9] * Mean AUC-ROC 0.8058 Loss 0.2311
[2025-11-19 22:38:35 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:9] * Per-class AUC-ROC: ['0.8373', '0.8273', '0.8171', '0.9035', '0.7339', '0.5899', '0.7196', '0.7426', '0.8634', '0.9230', '0.7617', '0.7526', '0.8861', '0.9225']
[2025-11-19 22:38:35 internimage_b_mimic_cxr_224](main.py 365): INFO AUC-ROC of the network on the 2991 val images: 0.8058
[2025-11-19 22:38:35 internimage_b_mimic_cxr_224](main.py 380): INFO Max AUC-ROC: 0.8058
[2025-11-19 22:38:43 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 8.554 (8.554)	Loss 0.2763 (0.2763)	Mem 34631MB
[2025-11-19 22:38:56 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:9] * Mean AUC-ROC 0.7986 Loss 0.2389
[2025-11-19 22:38:56 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:9] * Per-class AUC-ROC: ['0.8324', '0.8224', '0.8119', '0.9018', '0.7307', '0.5996', '0.7061', '0.7372', '0.8577', '0.9190', '0.7408', '0.7468', '0.8560', '0.9172']
[2025-11-19 22:38:56 internimage_b_mimic_cxr_224](main.py 385): INFO AUC-ROC of the ema network on the 2991 val images: 0.7986
[2025-11-19 22:38:56 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saving......
[2025-11-19 22:38:58 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saved !!!
[2025-11-19 22:38:58 internimage_b_mimic_cxr_224](main.py 400): INFO Max ema AUC-ROC: 0.7986
/scratch/smehta90/InternImage_MIMIC/classification/main.py:483: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
/scratch/smehta90/InternImage_MIMIC/classification/main.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
[2025-11-19 22:39:35 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][0/2882]	eta 1 day, 5:33:39 lr 0.000001	time 36.9257 (36.9257)	model_time 0.6295 (0.6295)	loss 0.2416 (0.2416)	grad_norm 1.5293 (1.5293/0.0000)	mem 34631MB
[2025-11-19 22:41:47 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][50/2882]	eta 2:36:14 lr 0.000001	time 0.6232 (3.3103)	model_time 0.6230 (0.6318)	loss 0.2549 (0.2349)	grad_norm 1.4852 (0.9501/0.3145)	mem 34631MB
[2025-11-19 22:44:07 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][100/2882]	eta 2:22:06 lr 0.000001	time 1.3173 (3.0649)	model_time 0.6287 (0.6302)	loss 0.2718 (0.2322)	grad_norm 1.2083 (0.9246/0.2845)	mem 34631MB
[2025-11-19 22:46:21 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][150/2882]	eta 2:13:44 lr 0.000001	time 0.9401 (2.9371)	model_time 0.6309 (0.6315)	loss 0.2455 (0.2334)	grad_norm 0.8127 (0.9068/0.2811)	mem 34631MB
[2025-11-19 22:48:28 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][200/2882]	eta 2:06:43 lr 0.000001	time 0.6305 (2.8351)	model_time 0.6302 (0.6313)	loss 0.2341 (0.2330)	grad_norm 0.6556 (0.9046/0.2804)	mem 34631MB
[2025-11-19 22:50:39 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][250/2882]	eta 2:02:29 lr 0.000001	time 0.6332 (2.7924)	model_time 0.6329 (0.6311)	loss 0.2081 (0.2334)	grad_norm 0.9374 (0.8946/0.2693)	mem 34631MB
[2025-11-19 22:53:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][300/2882]	eta 2:01:08 lr 0.000001	time 16.8392 (2.8149)	model_time 0.6367 (0.6316)	loss 0.2333 (0.2334)	grad_norm 0.8817 (0.8836/0.2578)	mem 34631MB
[2025-11-19 22:55:16 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][350/2882]	eta 1:57:34 lr 0.000001	time 0.6290 (2.7860)	model_time 0.6288 (0.6314)	loss 0.2299 (0.2334)	grad_norm 0.7079 (0.8789/0.2463)	mem 34631MB
[2025-11-19 22:57:37 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][400/2882]	eta 1:55:29 lr 0.000001	time 0.6313 (2.7917)	model_time 0.6311 (0.6316)	loss 0.2153 (0.2335)	grad_norm 1.0558 (0.8826/0.2452)	mem 34631MB
[2025-11-19 22:59:43 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][450/2882]	eta 1:51:55 lr 0.000001	time 2.4373 (2.7611)	model_time 0.6283 (0.6314)	loss 0.2361 (0.2336)	grad_norm 0.8987 (0.8902/0.2475)	mem 34631MB
[2025-11-19 23:02:03 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][500/2882]	eta 1:49:44 lr 0.000001	time 0.6299 (2.7642)	model_time 0.6296 (0.6314)	loss 0.2250 (0.2338)	grad_norm 0.6015 (0.8899/0.2417)	mem 34631MB
[2025-11-19 23:04:15 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][550/2882]	eta 1:47:00 lr 0.000001	time 0.6317 (2.7532)	model_time 0.6316 (0.6314)	loss 0.2274 (0.2338)	grad_norm 1.0780 (0.9005/0.2518)	mem 34631MB
[2025-11-19 23:06:30 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][600/2882]	eta 1:44:33 lr 0.000001	time 6.2821 (2.7491)	model_time 0.6296 (0.6314)	loss 0.2531 (0.2335)	grad_norm 1.0406 (0.9043/0.2498)	mem 34631MB
[2025-11-19 23:08:46 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][650/2882]	eta 1:42:10 lr 0.000001	time 0.6368 (2.7467)	model_time 0.6365 (0.6314)	loss 0.2424 (0.2336)	grad_norm 0.8963 (0.9186/0.2522)	mem 34631MB
[2025-11-19 23:11:29 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][700/2882]	eta 1:41:14 lr 0.000001	time 5.9828 (2.7839)	model_time 0.6344 (0.6316)	loss 0.2144 (0.2336)	grad_norm 1.1337 (0.9164/0.2496)	mem 34631MB
[2025-11-19 23:13:47 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][750/2882]	eta 1:38:51 lr 0.000001	time 0.6311 (2.7823)	model_time 0.6309 (0.6316)	loss 0.2562 (0.2336)	grad_norm 0.5445 (0.9085/0.2453)	mem 34631MB
[2025-11-19 23:16:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][800/2882]	eta 1:36:29 lr 0.000001	time 0.6346 (2.7806)	model_time 0.6343 (0.6317)	loss 0.2443 (0.2336)	grad_norm 0.5861 (0.9168/0.2502)	mem 34631MB
[2025-11-19 23:18:26 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][850/2882]	eta 1:34:13 lr 0.000001	time 0.6302 (2.7823)	model_time 0.6300 (0.6317)	loss 0.2424 (0.2337)	grad_norm 0.7542 (0.9201/0.2493)	mem 34631MB
[2025-11-19 23:20:55 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][900/2882]	eta 1:32:17 lr 0.000001	time 10.3690 (2.7938)	model_time 0.6307 (0.6316)	loss 0.2390 (0.2335)	grad_norm 0.7602 (0.9251/0.2555)	mem 34631MB
[2025-11-19 23:23:16 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][950/2882]	eta 1:29:59 lr 0.000001	time 0.6292 (2.7948)	model_time 0.6290 (0.6316)	loss 0.2375 (0.2334)	grad_norm 1.0564 (0.9156/0.2668)	mem 34631MB
[2025-11-19 23:26:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1000/2882]	eta 1:28:33 lr 0.000001	time 9.5593 (2.8232)	model_time 0.6361 (0.6318)	loss 0.2340 (0.2335)	grad_norm 0.6695 (0.9310/0.2712)	mem 34631MB
[2025-11-19 23:28:29 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1050/2882]	eta 1:26:18 lr 0.000001	time 0.8387 (2.8265)	model_time 0.6324 (0.6318)	loss 0.2631 (0.2334)	grad_norm 1.4390 (0.9364/0.2770)	mem 34631MB
[2025-11-19 23:30:58 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1100/2882]	eta 1:24:10 lr 0.000001	time 0.6289 (2.8342)	model_time 0.6286 (0.6319)	loss 0.2372 (0.2334)	grad_norm 0.9516 (0.9310/0.2820)	mem 34631MB
[2025-11-19 23:33:28 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1150/2882]	eta 1:22:01 lr 0.000001	time 0.6351 (2.8413)	model_time 0.6348 (0.6321)	loss 0.2404 (0.2334)	grad_norm 0.7532 (0.9266/0.2795)	mem 34631MB
[2025-11-19 23:35:54 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1200/2882]	eta 1:19:43 lr 0.000001	time 0.6362 (2.8441)	model_time 0.6359 (0.6321)	loss 0.2541 (0.2335)	grad_norm 0.8767 (0.9255/0.2809)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg'
[2025-11-19 23:38:11 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1250/2882]	eta 1:17:15 lr 0.000001	time 0.6328 (2.8402)	model_time 0.6324 (0.6321)	loss 0.2635 (0.2335)	grad_norm 0.8340 (0.9266/0.2724)	mem 34631MB
[2025-11-19 23:40:41 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1300/2882]	eta 1:15:03 lr 0.000001	time 8.4465 (2.8468)	model_time 0.6332 (0.6322)	loss 0.2639 (0.2337)	grad_norm 0.6872 (0.9195/0.2761)	mem 34631MB
[2025-11-19 23:42:59 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1350/2882]	eta 1:12:35 lr 0.000001	time 6.0595 (2.8428)	model_time 0.6333 (0.6321)	loss 0.2095 (0.2336)	grad_norm 0.9818 (0.9239/0.2631)	mem 34631MB
[2025-11-19 23:45:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1400/2882]	eta 1:09:55 lr 0.000001	time 0.6358 (2.8311)	model_time 0.6356 (0.6323)	loss 0.2375 (0.2336)	grad_norm 1.4818 (0.9295/0.2517)	mem 34631MB
[2025-11-19 23:47:09 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1450/2882]	eta 1:07:17 lr 0.000001	time 0.6316 (2.8195)	model_time 0.6314 (0.6323)	loss 0.2141 (0.2334)	grad_norm 0.9709 (0.9306/0.2551)	mem 34631MB
[2025-11-19 23:49:14 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1500/2882]	eta 1:04:42 lr 0.000001	time 0.6370 (2.8091)	model_time 0.6368 (0.6324)	loss 0.2671 (0.2335)	grad_norm 0.9644 (0.9337/0.2516)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg'
[2025-11-19 23:51:20 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1550/2882]	eta 1:02:09 lr 0.000001	time 0.6321 (2.7997)	model_time 0.6319 (0.6324)	loss 0.1979 (0.2334)	grad_norm 1.2668 (0.9387/0.2498)	mem 34631MB
[2025-11-19 23:53:48 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1600/2882]	eta 0:59:55 lr 0.000001	time 0.6324 (2.8047)	model_time 0.6320 (0.6325)	loss 0.2363 (0.2332)	grad_norm 0.6188 (0.9346/0.2470)	mem 34631MB
[2025-11-19 23:56:15 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1650/2882]	eta 0:57:40 lr 0.000001	time 12.4907 (2.8088)	model_time 0.6361 (0.6326)	loss 0.2377 (0.2333)	grad_norm 1.0141 (0.9297/0.2533)	mem 34631MB
[2025-11-19 23:58:28 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1700/2882]	eta 0:55:14 lr 0.000001	time 0.6347 (2.8040)	model_time 0.6345 (0.6326)	loss 0.2401 (0.2333)	grad_norm 0.5954 (0.9185/0.2522)	mem 34631MB
[2025-11-20 00:00:31 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1750/2882]	eta 0:52:43 lr 0.000001	time 0.6286 (2.7947)	model_time 0.6284 (0.6325)	loss 0.2211 (0.2332)	grad_norm 1.0133 (0.9153/0.2428)	mem 34631MB
[2025-11-20 00:02:36 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1800/2882]	eta 0:50:14 lr 0.000001	time 0.6291 (2.7865)	model_time 0.6288 (0.6324)	loss 0.2293 (0.2332)	grad_norm 1.2444 (0.9040/0.2418)	mem 34631MB
[2025-11-20 00:04:48 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1850/2882]	eta 0:47:51 lr 0.000001	time 0.6332 (2.7823)	model_time 0.6329 (0.6324)	loss 0.2666 (0.2332)	grad_norm 1.4173 (0.8932/0.2346)	mem 34631MB
[2025-11-20 00:07:08 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1900/2882]	eta 0:45:32 lr 0.000001	time 0.6308 (2.7826)	model_time 0.6304 (0.6323)	loss 0.2387 (0.2332)	grad_norm 1.0304 (0.8876/0.2342)	mem 34631MB
[2025-11-20 00:09:18 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][1950/2882]	eta 0:43:09 lr 0.000001	time 9.8242 (2.7784)	model_time 0.6263 (0.6323)	loss 0.2312 (0.2331)	grad_norm 0.7848 (0.8887/0.2394)	mem 34631MB
[2025-11-20 00:11:20 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2000/2882]	eta 0:40:42 lr 0.000001	time 0.6290 (2.7696)	model_time 0.6288 (0.6323)	loss 0.2403 (0.2332)	grad_norm 1.3935 (0.9044/0.2541)	mem 34631MB
[2025-11-20 00:13:20 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2050/2882]	eta 0:38:17 lr 0.000001	time 0.6296 (2.7609)	model_time 0.6294 (0.6323)	loss 0.2129 (0.2332)	grad_norm 0.7465 (0.9190/0.2647)	mem 34631MB
[2025-11-20 00:15:22 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2100/2882]	eta 0:35:52 lr 0.000001	time 0.6281 (2.7531)	model_time 0.6280 (0.6322)	loss 0.2341 (0.2333)	grad_norm 1.0005 (0.9342/0.2811)	mem 34631MB
[2025-11-20 00:17:21 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2150/2882]	eta 0:33:29 lr 0.000001	time 0.6347 (2.7446)	model_time 0.6343 (0.6323)	loss 0.2015 (0.2334)	grad_norm 1.1065 (0.9378/0.2777)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg'
[2025-11-20 00:19:47 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2200/2882]	eta 0:31:14 lr 0.000001	time 0.6304 (2.7482)	model_time 0.6300 (0.6323)	loss 0.2411 (0.2333)	grad_norm 0.9440 (0.9382/0.2723)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg'
[2025-11-20 00:21:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2250/2882]	eta 0:28:53 lr 0.000001	time 0.6328 (2.7423)	model_time 0.6326 (0.6324)	loss 0.2354 (0.2332)	grad_norm 0.8172 (0.9344/0.2697)	mem 34631MB
[2025-11-20 00:24:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2300/2882]	eta 0:26:35 lr 0.000001	time 0.6355 (2.7409)	model_time 0.6352 (0.6324)	loss 0.2566 (0.2333)	grad_norm 1.1747 (0.9220/0.2629)	mem 34631MB
[2025-11-20 00:26:20 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2350/2882]	eta 0:24:17 lr 0.000001	time 0.6337 (2.7401)	model_time 0.6334 (0.6324)	loss 0.2554 (0.2333)	grad_norm 1.0079 (0.9148/0.2617)	mem 34631MB
[2025-11-20 00:28:36 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2400/2882]	eta 0:22:00 lr 0.000001	time 0.6312 (2.7399)	model_time 0.6308 (0.6325)	loss 0.2624 (0.2333)	grad_norm 0.9171 (0.9104/0.2430)	mem 34631MB
[2025-11-20 00:30:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2450/2882]	eta 0:19:43 lr 0.000001	time 0.6338 (2.7388)	model_time 0.6336 (0.6325)	loss 0.2369 (0.2333)	grad_norm 0.9178 (0.9103/0.2530)	mem 34631MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg'
[2025-11-20 00:33:26 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2500/2882]	eta 0:17:29 lr 0.000001	time 0.6279 (2.7463)	model_time 0.6277 (0.6324)	loss 0.2369 (0.2332)	grad_norm 0.9039 (0.9159/0.2551)	mem 34631MB
[2025-11-20 00:35:42 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2550/2882]	eta 0:15:11 lr 0.000001	time 0.6346 (2.7456)	model_time 0.6344 (0.6324)	loss 0.2254 (0.2333)	grad_norm 1.2332 (0.9219/0.2517)	mem 34631MB
[2025-11-20 00:37:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2600/2882]	eta 0:12:53 lr 0.000001	time 0.6332 (2.7422)	model_time 0.6330 (0.6324)	loss 0.2211 (0.2332)	grad_norm 1.0890 (0.9176/0.2451)	mem 34631MB
[2025-11-20 00:40:07 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2650/2882]	eta 0:10:36 lr 0.000001	time 0.6269 (2.7419)	model_time 0.6267 (0.6324)	loss 0.2478 (0.2332)	grad_norm 1.4823 (0.9151/0.2385)	mem 34631MB
[2025-11-20 00:42:27 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2700/2882]	eta 0:08:19 lr 0.000001	time 0.6343 (2.7429)	model_time 0.6340 (0.6324)	loss 0.2377 (0.2332)	grad_norm 1.0428 (0.9234/0.2425)	mem 34631MB
[2025-11-20 00:44:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2750/2882]	eta 0:06:02 lr 0.000001	time 0.6368 (2.7451)	model_time 0.6365 (0.6324)	loss 0.2447 (0.2332)	grad_norm 0.7464 (0.9317/0.2471)	mem 34631MB
[2025-11-20 00:47:27 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2800/2882]	eta 0:03:45 lr 0.000001	time 0.6326 (2.7524)	model_time 0.6323 (0.6324)	loss 0.2409 (0.2332)	grad_norm 0.9594 (0.9425/0.2535)	mem 34631MB
[2025-11-20 00:49:49 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [10/20][2850/2882]	eta 0:01:28 lr 0.000001	time 6.3880 (2.7539)	model_time 0.6366 (0.6325)	loss 0.2449 (0.2332)	grad_norm 0.9564 (0.9397/0.2478)	mem 34631MB
[2025-11-20 00:51:06 internimage_b_mimic_cxr_224](main.py 583): INFO EPOCH 10 training takes 2:12:08
[2025-11-20 00:51:06 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_10.pth saving......
[2025-11-20 00:51:09 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_10.pth saved !!!
[2025-11-20 00:51:43 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 33.772 (33.772)	Loss 0.2730 (0.2730)	Mem 34631MB
[2025-11-20 00:52:26 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:10] * Mean AUC-ROC 0.8074 Loss 0.2305
[2025-11-20 00:52:26 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:10] * Per-class AUC-ROC: ['0.8381', '0.8278', '0.8179', '0.9035', '0.7365', '0.5942', '0.7262', '0.7458', '0.8640', '0.9237', '0.7579', '0.7548', '0.8898', '0.9243']
[2025-11-20 00:52:26 internimage_b_mimic_cxr_224](main.py 365): INFO AUC-ROC of the network on the 2991 val images: 0.8074
[2025-11-20 00:52:26 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_best.pth saving......
[2025-11-20 00:52:29 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_best.pth saved !!!
[2025-11-20 00:52:29 internimage_b_mimic_cxr_224](main.py 380): INFO Max AUC-ROC: 0.8074
[2025-11-20 00:52:37 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 8.616 (8.616)	Loss 0.2749 (0.2749)	Mem 34631MB
[2025-11-20 00:52:50 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:10] * Mean AUC-ROC 0.8011 Loss 0.2364
[2025-11-20 00:52:50 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:10] * Per-class AUC-ROC: ['0.8343', '0.8245', '0.8138', '0.9025', '0.7327', '0.5981', '0.7115', '0.7395', '0.8594', '0.9206', '0.7448', '0.7508', '0.8633', '0.9188']
[2025-11-20 00:52:50 internimage_b_mimic_cxr_224](main.py 385): INFO AUC-ROC of the ema network on the 2991 val images: 0.8011
[2025-11-20 00:52:50 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saving......
[2025-11-20 00:52:52 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saved !!!
[2025-11-20 00:52:52 internimage_b_mimic_cxr_224](main.py 400): INFO Max ema AUC-ROC: 0.8011
/scratch/smehta90/InternImage_MIMIC/classification/main.py:483: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
/scratch/smehta90/InternImage_MIMIC/classification/main.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
[2025-11-20 00:53:27 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][0/2882]	eta 1 day, 3:42:11 lr 0.000001	time 34.6051 (34.6051)	model_time 0.6292 (0.6292)	loss 0.2284 (0.2284)	grad_norm 0.8628 (0.8628/0.0000)	mem 34631MB
[2025-11-20 00:55:23 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][50/2882]	eta 2:19:40 lr 0.000001	time 0.6336 (2.9591)	model_time 0.6333 (0.6313)	loss 0.2387 (0.2367)	grad_norm 0.8548 (0.8729/0.2781)	mem 34631MB
[2025-11-20 00:57:27 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][100/2882]	eta 2:06:05 lr 0.000001	time 0.6345 (2.7196)	model_time 0.6342 (0.6317)	loss 0.2538 (0.2338)	grad_norm 1.0504 (0.8965/0.2684)	mem 34631MB
[2025-11-20 00:59:21 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][150/2882]	eta 1:57:22 lr 0.000001	time 0.6282 (2.5777)	model_time 0.6279 (0.6320)	loss 0.2033 (0.2335)	grad_norm 1.1560 (0.9102/0.2630)	mem 34631MB
[2025-11-20 01:01:18 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][200/2882]	eta 1:52:27 lr 0.000001	time 0.6347 (2.5158)	model_time 0.6344 (0.6326)	loss 0.2371 (0.2335)	grad_norm 0.5464 (0.9057/0.2697)	mem 34631MB
[2025-11-20 01:03:16 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][250/2882]	eta 1:49:01 lr 0.000001	time 0.6324 (2.4855)	model_time 0.6321 (0.6326)	loss 0.2118 (0.2331)	grad_norm 0.6162 (0.9074/0.2766)	mem 34631MB
[2025-11-20 01:05:18 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][300/2882]	eta 1:46:41 lr 0.000001	time 10.7286 (2.4794)	model_time 0.6279 (0.6326)	loss 0.2557 (0.2330)	grad_norm 1.4185 (0.9128/0.2718)	mem 34631MB
[2025-11-20 01:07:14 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][350/2882]	eta 1:43:40 lr 0.000001	time 0.6355 (2.4569)	model_time 0.6352 (0.6330)	loss 0.2055 (0.2327)	grad_norm 0.9247 (0.9160/0.2685)	mem 34631MB
[2025-11-20 01:09:15 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][400/2882]	eta 1:41:25 lr 0.000001	time 0.6330 (2.4519)	model_time 0.6327 (0.6331)	loss 0.1922 (0.2329)	grad_norm 0.8459 (0.9298/0.2763)	mem 34631MB
[2025-11-20 01:11:14 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][450/2882]	eta 1:39:03 lr 0.000001	time 0.6340 (2.4439)	model_time 0.6336 (0.6330)	loss 0.2049 (0.2328)	grad_norm 1.2996 (0.9314/0.2760)	mem 34631MB
[2025-11-20 01:13:14 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][500/2882]	eta 1:36:47 lr 0.000001	time 0.6378 (2.4379)	model_time 0.6375 (0.6332)	loss 0.2121 (0.2329)	grad_norm 0.9340 (0.9430/0.2677)	mem 34631MB
[2025-11-20 01:15:11 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][550/2882]	eta 1:34:26 lr 0.000001	time 0.6356 (2.4299)	model_time 0.6353 (0.6332)	loss 0.2502 (0.2330)	grad_norm 0.9164 (0.9565/0.2647)	mem 34631MB
[2025-11-20 01:17:25 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][600/2882]	eta 1:33:14 lr 0.000001	time 5.9513 (2.4514)	model_time 0.6316 (0.6333)	loss 0.2544 (0.2331)	grad_norm 0.7999 (0.9593/0.2768)	mem 34631MB
[2025-11-20 01:19:26 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][650/2882]	eta 1:31:04 lr 0.000001	time 4.4814 (2.4481)	model_time 0.6330 (0.6332)	loss 0.2437 (0.2332)	grad_norm 0.8392 (0.9687/0.2775)	mem 34631MB
[2025-11-20 01:21:36 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][700/2882]	eta 1:29:24 lr 0.000001	time 0.6320 (2.4585)	model_time 0.6317 (0.6331)	loss 0.2221 (0.2333)	grad_norm 1.0877 (0.9575/0.2633)	mem 34631MB
[2025-11-20 01:23:33 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][750/2882]	eta 1:27:06 lr 0.000001	time 0.6315 (2.4513)	model_time 0.6311 (0.6330)	loss 0.2227 (0.2333)	grad_norm 0.3501 (0.9554/0.2641)	mem 34631MB
[2025-11-20 01:25:27 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][800/2882]	eta 1:24:40 lr 0.000001	time 0.6332 (2.4401)	model_time 0.6330 (0.6329)	loss 0.2231 (0.2332)	grad_norm 0.6799 (0.9480/0.2641)	mem 34631MB
[2025-11-20 01:27:29 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][850/2882]	eta 1:22:39 lr 0.000001	time 0.6261 (2.4409)	model_time 0.6259 (0.6329)	loss 0.2565 (0.2332)	grad_norm 0.5673 (0.9462/0.2530)	mem 34631MB
[2025-11-20 01:29:33 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][900/2882]	eta 1:20:42 lr 0.000001	time 8.5915 (2.4432)	model_time 0.6310 (0.6330)	loss 0.2512 (0.2333)	grad_norm 0.7682 (0.9444/0.2393)	mem 34631MB
[2025-11-20 01:31:38 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][950/2882]	eta 1:18:45 lr 0.000001	time 0.6347 (2.4461)	model_time 0.6344 (0.6328)	loss 0.2487 (0.2332)	grad_norm 0.8844 (0.9367/0.2382)	mem 34631MB
[2025-11-20 01:34:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1000/2882]	eta 1:17:28 lr 0.000001	time 0.6303 (2.4698)	model_time 0.6301 (0.6330)	loss 0.2407 (0.2332)	grad_norm 0.7745 (0.9311/0.2468)	mem 34631MB
[2025-11-20 01:36:06 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1050/2882]	eta 1:15:21 lr 0.000001	time 0.6315 (2.4679)	model_time 0.6312 (0.6329)	loss 0.2262 (0.2330)	grad_norm 1.2635 (0.9338/0.2428)	mem 34631MB
[2025-11-20 01:38:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1100/2882]	eta 1:13:09 lr 0.000001	time 0.6338 (2.4633)	model_time 0.6335 (0.6329)	loss 0.2572 (0.2329)	grad_norm 1.3284 (0.9259/0.2445)	mem 34631MB
[2025-11-20 01:40:07 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1150/2882]	eta 1:11:05 lr 0.000001	time 0.6461 (2.4629)	model_time 0.6459 (0.6329)	loss 0.2214 (0.2329)	grad_norm 0.5596 (0.9208/0.2460)	mem 34631MB
[2025-11-20 01:42:02 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1200/2882]	eta 1:08:51 lr 0.000001	time 0.6352 (2.4561)	model_time 0.6349 (0.6331)	loss 0.2405 (0.2330)	grad_norm 0.9682 (0.9103/0.2487)	mem 34631MB
[2025-11-20 01:43:56 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1250/2882]	eta 1:06:37 lr 0.000001	time 0.6355 (2.4494)	model_time 0.6352 (0.6331)	loss 0.2127 (0.2330)	grad_norm 1.5427 (0.9121/0.2434)	mem 34631MB
[2025-11-20 01:46:11 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1300/2882]	eta 1:04:50 lr 0.000001	time 0.6335 (2.4591)	model_time 0.6332 (0.6331)	loss 0.2276 (0.2329)	grad_norm 0.9952 (0.9132/0.2489)	mem 34631MB
[2025-11-20 01:48:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1350/2882]	eta 1:02:37 lr 0.000001	time 0.6310 (2.4525)	model_time 0.6307 (0.6332)	loss 0.2625 (0.2329)	grad_norm 1.4116 (0.9056/0.2507)	mem 34631MB
[2025-11-20 01:50:02 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1400/2882]	eta 1:00:28 lr 0.000001	time 0.6288 (2.4483)	model_time 0.6285 (0.6332)	loss 0.2158 (0.2330)	grad_norm 0.5709 (0.9136/0.2513)	mem 34631MB
[2025-11-20 01:51:59 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1450/2882]	eta 0:58:19 lr 0.000001	time 0.7984 (2.4441)	model_time 0.7981 (0.6333)	loss 0.2040 (0.2328)	grad_norm 1.0146 (0.9090/0.2502)	mem 34631MB
[2025-11-20 01:53:55 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1500/2882]	eta 0:56:12 lr 0.000001	time 0.6342 (2.4402)	model_time 0.6339 (0.6333)	loss 0.1974 (0.2328)	grad_norm 0.8016 (0.9150/0.2445)	mem 34641MB
[2025-11-20 01:55:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1550/2882]	eta 0:54:05 lr 0.000001	time 0.6302 (2.4367)	model_time 0.6299 (0.6333)	loss 0.2534 (0.2328)	grad_norm 1.0563 (0.9065/0.2452)	mem 34641MB
[2025-11-20 01:58:11 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1600/2882]	eta 0:52:18 lr 0.000001	time 0.6305 (2.4479)	model_time 0.6302 (0.6333)	loss 0.2142 (0.2329)	grad_norm 0.7671 (0.8948/0.2428)	mem 34641MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg'
[2025-11-20 02:00:09 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1650/2882]	eta 0:50:12 lr 0.000001	time 1.2984 (2.4452)	model_time 0.6283 (0.6332)	loss 0.2538 (0.2330)	grad_norm 0.9266 (0.8996/0.2416)	mem 34641MB
[2025-11-20 02:02:09 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1700/2882]	eta 0:48:08 lr 0.000001	time 0.6277 (2.4436)	model_time 0.6275 (0.6332)	loss 0.2213 (0.2331)	grad_norm 1.0592 (0.8959/0.2335)	mem 34641MB
[2025-11-20 02:04:08 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1750/2882]	eta 0:46:04 lr 0.000001	time 0.6315 (2.4418)	model_time 0.6312 (0.6330)	loss 0.2502 (0.2330)	grad_norm 0.8011 (0.8848/0.2259)	mem 34641MB
[2025-11-20 02:06:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1800/2882]	eta 0:43:58 lr 0.000001	time 0.6301 (2.4389)	model_time 0.6297 (0.6329)	loss 0.2570 (0.2330)	grad_norm 1.0228 (0.8816/0.2249)	mem 34641MB
[2025-11-20 02:08:12 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1850/2882]	eta 0:41:59 lr 0.000001	time 0.6296 (2.4416)	model_time 0.6293 (0.6329)	loss 0.2354 (0.2330)	grad_norm 0.6823 (0.8882/0.2227)	mem 34641MB
[2025-11-20 02:10:35 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1900/2882]	eta 0:40:08 lr 0.000001	time 0.6305 (2.4530)	model_time 0.6302 (0.6328)	loss 0.2627 (0.2329)	grad_norm 1.7427 (0.8963/0.2148)	mem 34641MB
[2025-11-20 02:12:42 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][1950/2882]	eta 0:38:08 lr 0.000001	time 0.6305 (2.4553)	model_time 0.6303 (0.6327)	loss 0.2449 (0.2328)	grad_norm 0.9413 (0.8964/0.2113)	mem 34641MB
[2025-11-20 02:14:45 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2000/2882]	eta 0:36:05 lr 0.000001	time 0.6331 (2.4550)	model_time 0.6324 (0.6327)	loss 0.2222 (0.2329)	grad_norm 0.7694 (0.9028/0.2214)	mem 34641MB
[2025-11-20 02:16:47 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2050/2882]	eta 0:34:02 lr 0.000001	time 0.6337 (2.4548)	model_time 0.6334 (0.6327)	loss 0.2228 (0.2328)	grad_norm 1.3014 (0.9102/0.2251)	mem 34641MB
[2025-11-20 02:18:45 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2100/2882]	eta 0:31:57 lr 0.000001	time 0.6330 (2.4527)	model_time 0.6327 (0.6327)	loss 0.2564 (0.2327)	grad_norm 0.6259 (0.9105/0.2205)	mem 34641MB
[2025-11-20 02:20:42 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2150/2882]	eta 0:29:53 lr 0.000001	time 0.6314 (2.4502)	model_time 0.6312 (0.6326)	loss 0.2471 (0.2327)	grad_norm 0.6091 (0.9207/0.2327)	mem 34641MB
[2025-11-20 02:23:03 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2200/2882]	eta 0:27:56 lr 0.000001	time 0.6321 (2.4585)	model_time 0.6319 (0.6327)	loss 0.2151 (0.2328)	grad_norm 0.6108 (0.9335/0.2333)	mem 34641MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg'
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg'
[2025-11-20 02:25:03 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2250/2882]	eta 0:25:53 lr 0.000001	time 0.6414 (2.4573)	model_time 0.6411 (0.6327)	loss 0.2316 (0.2327)	grad_norm 0.8675 (0.9285/0.2366)	mem 34641MB
[2025-11-20 02:27:01 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2300/2882]	eta 0:23:48 lr 0.000001	time 0.6310 (2.4552)	model_time 0.6308 (0.6327)	loss 0.2329 (0.2327)	grad_norm 0.7672 (0.9365/0.2322)	mem 34641MB
[2025-11-20 02:28:57 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2350/2882]	eta 0:21:44 lr 0.000001	time 0.6278 (2.4522)	model_time 0.6275 (0.6327)	loss 0.2276 (0.2327)	grad_norm 0.8758 (0.9406/0.2401)	mem 34641MB
[2025-11-20 02:31:08 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2400/2882]	eta 0:19:43 lr 0.000001	time 0.6309 (2.4556)	model_time 0.6307 (0.6327)	loss 0.2458 (0.2326)	grad_norm 0.6615 (0.9344/0.2449)	mem 34641MB
[2025-11-20 02:33:06 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2450/2882]	eta 0:17:40 lr 0.000001	time 0.6293 (2.4538)	model_time 0.6290 (0.6327)	loss 0.2498 (0.2326)	grad_norm 0.7495 (0.9227/0.2425)	mem 34641MB
[2025-11-20 02:35:24 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2500/2882]	eta 0:15:39 lr 0.000001	time 0.6324 (2.4598)	model_time 0.6321 (0.6327)	loss 0.2301 (0.2325)	grad_norm 1.0932 (0.9076/0.2440)	mem 34641MB
[2025-11-20 02:37:19 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2550/2882]	eta 0:13:35 lr 0.000001	time 0.6308 (2.4566)	model_time 0.6306 (0.6327)	loss 0.2550 (0.2326)	grad_norm 1.1589 (0.9049/0.2462)	mem 34641MB
[2025-11-20 02:39:27 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2600/2882]	eta 0:11:33 lr 0.000001	time 0.6332 (2.4586)	model_time 0.6329 (0.6327)	loss 0.2179 (0.2326)	grad_norm 0.7280 (0.8879/0.2437)	mem 34641MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg'
[2025-11-20 02:41:25 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2650/2882]	eta 0:09:29 lr 0.000001	time 0.6296 (2.4568)	model_time 0.6294 (0.6328)	loss 0.2248 (0.2326)	grad_norm 0.7679 (0.8833/0.2371)	mem 34641MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg'
[2025-11-20 02:43:41 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2700/2882]	eta 0:07:28 lr 0.000001	time 0.6357 (2.4617)	model_time 0.6355 (0.6328)	loss 0.2167 (0.2325)	grad_norm 1.0180 (0.9008/0.2453)	mem 34641MB
[2025-11-20 02:45:46 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2750/2882]	eta 0:05:25 lr 0.000001	time 0.6355 (2.4623)	model_time 0.6352 (0.6328)	loss 0.2501 (0.2325)	grad_norm 0.9694 (0.9095/0.2364)	mem 34641MB
[2025-11-20 02:47:59 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2800/2882]	eta 0:03:22 lr 0.000001	time 0.6282 (2.4659)	model_time 0.6280 (0.6327)	loss 0.2272 (0.2325)	grad_norm 1.0275 (0.9166/0.2307)	mem 34641MB
[2025-11-20 02:49:48 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [11/20][2850/2882]	eta 0:01:18 lr 0.000001	time 0.6270 (2.4609)	model_time 0.6268 (0.6327)	loss 0.2501 (0.2325)	grad_norm 0.9733 (0.9191/0.2306)	mem 34641MB
[2025-11-20 02:50:49 internimage_b_mimic_cxr_224](main.py 583): INFO EPOCH 11 training takes 1:57:56
[2025-11-20 02:50:49 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_11.pth saving......
[2025-11-20 02:50:51 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_11.pth saved !!!
[2025-11-20 02:51:21 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 30.349 (30.349)	Loss 0.2719 (0.2719)	Mem 34641MB
[2025-11-20 02:51:56 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:11] * Mean AUC-ROC 0.8084 Loss 0.2300
[2025-11-20 02:51:56 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:11] * Per-class AUC-ROC: ['0.8399', '0.8283', '0.8170', '0.9034', '0.7359', '0.5874', '0.7255', '0.7460', '0.8643', '0.9246', '0.7709', '0.7583', '0.8911', '0.9255']
[2025-11-20 02:51:56 internimage_b_mimic_cxr_224](main.py 365): INFO AUC-ROC of the network on the 2991 val images: 0.8084
[2025-11-20 02:51:56 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_best.pth saving......
[2025-11-20 02:51:58 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_best.pth saved !!!
[2025-11-20 02:51:58 internimage_b_mimic_cxr_224](main.py 380): INFO Max AUC-ROC: 0.8084
[2025-11-20 02:52:07 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 8.696 (8.696)	Loss 0.2741 (0.2741)	Mem 34641MB
[2025-11-20 02:52:19 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:11] * Mean AUC-ROC 0.8028 Loss 0.2347
[2025-11-20 02:52:19 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:11] * Per-class AUC-ROC: ['0.8358', '0.8261', '0.8153', '0.9029', '0.7342', '0.5947', '0.7147', '0.7412', '0.8607', '0.9218', '0.7481', '0.7535', '0.8694', '0.9204']
[2025-11-20 02:52:19 internimage_b_mimic_cxr_224](main.py 385): INFO AUC-ROC of the ema network on the 2991 val images: 0.8028
[2025-11-20 02:52:19 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saving......
[2025-11-20 02:52:22 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saved !!!
[2025-11-20 02:52:22 internimage_b_mimic_cxr_224](main.py 400): INFO Max ema AUC-ROC: 0.8028
/scratch/smehta90/InternImage_MIMIC/classification/main.py:483: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
/scratch/smehta90/InternImage_MIMIC/classification/main.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
[2025-11-20 02:52:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][0/2882]	eta 22:35:36 lr 0.000001	time 28.2221 (28.2221)	model_time 0.6347 (0.6347)	loss 0.2491 (0.2491)	grad_norm 0.9621 (0.9621/0.0000)	mem 34641MB
[2025-11-20 02:54:37 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][50/2882]	eta 2:05:19 lr 0.000001	time 0.6265 (2.6551)	model_time 0.6263 (0.6294)	loss 0.2377 (0.2335)	grad_norm 0.8770 (0.9563/0.2554)	mem 34641MB
[2025-11-20 02:56:20 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][100/2882]	eta 1:49:14 lr 0.000001	time 0.6312 (2.3559)	model_time 0.6309 (0.6304)	loss 0.2353 (0.2337)	grad_norm 0.5977 (0.9669/0.2661)	mem 34641MB
[2025-11-20 02:58:03 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][150/2882]	eta 1:42:54 lr 0.000001	time 0.6361 (2.2602)	model_time 0.6359 (0.6319)	loss 0.2350 (0.2328)	grad_norm 0.9047 (0.9412/0.2604)	mem 34641MB
[2025-11-20 02:59:45 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][200/2882]	eta 1:38:36 lr 0.000001	time 0.6360 (2.2061)	model_time 0.6357 (0.6323)	loss 0.2298 (0.2333)	grad_norm 0.6876 (0.9354/0.2612)	mem 34641MB
[2025-11-20 03:01:29 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][250/2882]	eta 1:35:39 lr 0.000001	time 0.6297 (2.1805)	model_time 0.6295 (0.6327)	loss 0.2602 (0.2332)	grad_norm 0.9757 (0.9377/0.2696)	mem 34641MB
[2025-11-20 03:03:33 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][300/2882]	eta 1:35:54 lr 0.000001	time 16.3173 (2.2287)	model_time 0.6336 (0.6326)	loss 0.2851 (0.2337)	grad_norm 0.9917 (0.9369/0.2608)	mem 34641MB
[2025-11-20 03:05:19 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][350/2882]	eta 1:33:21 lr 0.000001	time 0.6307 (2.2123)	model_time 0.6304 (0.6328)	loss 0.2361 (0.2333)	grad_norm 0.8879 (0.9256/0.2539)	mem 34641MB
[2025-11-20 03:07:14 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][400/2882]	eta 1:32:01 lr 0.000001	time 0.6343 (2.2245)	model_time 0.6340 (0.6329)	loss 0.2314 (0.2331)	grad_norm 1.0683 (0.9157/0.2561)	mem 34641MB
