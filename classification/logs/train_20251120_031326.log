/etc/python/sitecustomize.py:117: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.
  mod = _original_import(name, globals, locals, fromlist, level)
/scratch/smehta90/InternImage_MIMIC/classification/ops_dcnv3/functions/dcnv3_func.py:22: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.
  @custom_fwd
/scratch/smehta90/InternImage_MIMIC/classification/ops_dcnv3/functions/dcnv3_func.py:58: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.
  @custom_bwd
/home/smehta90/.conda/envs/INTERNIMAGE_ENV/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers
  warnings.warn(f"Importing from {__name__} is deprecated, please import via timm.layers", FutureWarning)
=> merge config from configs/internimage_b_mimic_cxr_224.yaml
RANK and WORLD_SIZE in environ: 0/1
/home/smehta90/.conda/envs/INTERNIMAGE_ENV/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4876: UserWarning: barrier(): using the device under current context. You can specify `device_id` in `init_process_group` to mute this warning.
  warnings.warn(  # warn only once
[rank0]:[W1120 03:13:40.748875889 ProcessGroupNCCL.cpp:5072] Guessing device ID based on global rank. This can cause a hang if rank to GPU mapping is heterogeneous. You can specify device_id in init_process_group()
O1 O1
[2025-11-20 03:13:40 internimage_b_mimic_cxr_224](main.py 729): INFO Full config saved to output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/config.json
[2025-11-20 03:13:40 internimage_b_mimic_cxr_224](main.py 732): INFO AMP_OPT_LEVEL: O1
AMP_TYPE: float16
AUG:
  AUTO_AUGMENT: none
  COLOR_JITTER: 0.0
  CUTMIX: 0.0
  CUTMIX_MINMAX: null
  MEAN:
  - 0.485
  - 0.456
  - 0.406
  MIXUP: 0.0
  MIXUP_MODE: batch
  MIXUP_PROB: 0.0
  MIXUP_SWITCH_PROB: 0.0
  RANDOM_RESIZED_CROP: false
  RECOUNT: 1
  REMODE: pixel
  REPROB: 0.1
  STD:
  - 0.229
  - 0.224
  - 0.225
BASE:
- ''
DATA:
  BATCH_SIZE: 128
  CACHE_MODE: part
  DATASET: mimic_cxr
  DATA_PATH: /scratch/smehta90/mimic_splits
  IMG_ON_MEMORY: false
  IMG_SIZE: 224
  INTERPOLATION: bicubic
  NUM_WORKERS: 12
  PIN_MEMORY: true
  ZIP_MODE: false
EVAL_22K_TO_1K: false
EVAL_FREQ: 1
EVAL_MODE: false
LOCAL_RANK: 0
MODEL:
  DROP_PATH_RATE: 0.5
  DROP_PATH_TYPE: linear
  DROP_RATE: 0.0
  INTERN_IMAGE:
    CENTER_FEATURE_SCALE: false
    CHANNELS: 112
    CORE_OP: DCNv3
    DEPTHS:
    - 4
    - 4
    - 21
    - 4
    DW_KERNEL_SIZE: null
    GROUPS:
    - 7
    - 14
    - 28
    - 56
    LAYER_SCALE: 1.0e-05
    LEVEL2_POST_NORM: false
    LEVEL2_POST_NORM_BLOCK_IDS: null
    MLP_RATIO: 4.0
    OFFSET_SCALE: 1.0
    POST_NORM: true
    REMOVE_CENTER: false
    RES_POST_NORM: false
    USE_CLIP_PROJECTOR: false
  LABEL_SMOOTHING: 0.0
  NAME: internimage_b_mimic_cxr_224
  NUM_CLASSES: 14
  PRETRAINED: pretrained/internimage_b_1k_224.pth
  RESUME: ''
  TYPE: intern_image
OUTPUT: output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224
PRINT_FREQ: 50
SAVE_CKPT_NUM: 1
SAVE_FREQ: 1
SEED: 42
TAG: run1
TEST:
  CROP: true
  SEQUENTIAL: false
THROUGHPUT_MODE: false
TRAIN:
  ACCUMULATION_STEPS: 1
  AUTO_RESUME: true
  BASE_LR: 0.0001
  CLIP_GRAD: 5.0
  EMA:
    DECAY: 0.9999
    ENABLE: true
  EPOCHS: 20
  LR_LAYER_DECAY: true
  LR_LAYER_DECAY_RATIO: 0.875
  LR_SCHEDULER:
    DECAY_EPOCHS: 30
    DECAY_RATE: 0.1
    NAME: cosine
  MIN_LR: 1.0e-06
  OPTIMIZER:
    BETAS:
    - 0.9
    - 0.999
    DCN_LR_MUL: null
    EPS: 1.0e-08
    FREEZE_BACKBONE: null
    MOMENTUM: 0.9
    NAME: adamw
    USE_ZERO: false
  RAND_INIT_FT_HEAD: true
  START_EPOCH: 0
  USE_CHECKPOINT: false
  WARMUP_EPOCHS: 2
  WARMUP_LR: 5.0e-07
  WEIGHT_DECAY: 0.05

Loading MIMIC-CXR train split from: /scratch/smehta90/mimic_splits/mimic-cxr-2.0.0-chexpert-train.csv
Filtered from 368945 to 368945 samples for split='train'
Using column 'path' for image paths
Loaded 368945 samples for train split
Label shape: (368945, 14)
Positive label distribution: [ 63485.  62554.  14236.  35279.   9752.   7404.  10490.  74302. 141239.
  74745.   3319.  25489.  14001.  81890.]
Verifying image accessibility...
  5/5 test images accessible
local rank 0 / global rank 0successfully build train dataset
Loading MIMIC-CXR val split from: /scratch/smehta90/mimic_splits/mimic-cxr-2.0.0-chexpert-val.csv
Filtered from 2991 to 2991 samples for split='validate'
Using column 'path' for image paths
Loaded 2991 samples for val split
Label shape: (2991, 14)
Positive label distribution: [ 528.  534.  113.  326.   90.   34.  109.  560. 1129.  670.   22.  194.
  112.  726.]
Verifying image accessibility...
  5/5 test images accessible
local rank 0 / global rank 0successfully build val dataset
Loading MIMIC-CXR test split from: /scratch/smehta90/mimic_splits/mimic-cxr-2.0.0-chexpert-test.csv
Filtered from 5159 to 5159 samples for split='test'
Using column 'path' for image paths
Loaded 5159 samples for test split
Label shape: (5159, 14)
Positive label distribution: [1034. 1258.  326.  959.  200.  167.  202. 1561.  984. 1542.  119.  539.
  144. 1457.]
Verifying image accessibility...
  5/5 test images accessible
local rank 0 / global rank 0successfully build test dataset
[2025-11-20 03:13:42 internimage_b_mimic_cxr_224](main.py 175): INFO Creating model:intern_image/internimage_b_mimic_cxr_224
using core type: DCNv3
using activation layer: GELU
using main norm layer: LN
using dpr: linear, 0.5
level2_post_norm: False
level2_post_norm_block_ids: None
res_post_norm: False
remove_center: False
[2025-11-20 03:13:45 internimage_b_mimic_cxr_224](main.py 178): INFO InternImage(
  (patch_embed): StemLayer(
    (conv1): Conv2d(3, 56, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm1): Sequential(
      (0): to_channels_last()
      (1): LayerNorm((56,), eps=1e-06, elementwise_affine=True)
      (2): to_channels_first()
    )
    (act): GELU(approximate='none')
    (conv2): Conv2d(56, 112, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm2): Sequential(
      (0): to_channels_last()
      (1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
    )
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (levels): ModuleList(
    (0): InternImageBlock(
      (blocks): ModuleList(
        (0): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(112, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=112)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=112, out_features=126, bias=True)
            (mask): Linear(in_features=112, out_features=63, bias=True)
            (input_proj): Linear(in_features=112, out_features=112, bias=True)
            (output_proj): Linear(in_features=112, out_features=112, bias=True)
          )
          (drop_path): Identity()
          (norm2): Sequential(
            (0): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=112, out_features=448, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=448, out_features=112, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(112, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=112)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=112, out_features=126, bias=True)
            (mask): Linear(in_features=112, out_features=63, bias=True)
            (input_proj): Linear(in_features=112, out_features=112, bias=True)
            (output_proj): Linear(in_features=112, out_features=112, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.016)
          (norm2): Sequential(
            (0): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=112, out_features=448, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=448, out_features=112, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(112, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=112)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=112, out_features=126, bias=True)
            (mask): Linear(in_features=112, out_features=63, bias=True)
            (input_proj): Linear(in_features=112, out_features=112, bias=True)
            (output_proj): Linear(in_features=112, out_features=112, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.031)
          (norm2): Sequential(
            (0): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=112, out_features=448, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=448, out_features=112, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(112, 112, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=112)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=112, out_features=126, bias=True)
            (mask): Linear(in_features=112, out_features=63, bias=True)
            (input_proj): Linear(in_features=112, out_features=112, bias=True)
            (output_proj): Linear(in_features=112, out_features=112, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.047)
          (norm2): Sequential(
            (0): LayerNorm((112,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=112, out_features=448, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=448, out_features=112, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): DownsampleLayer(
        (conv): Conv2d(112, 224, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (norm): Sequential(
          (0): to_channels_last()
          (1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (1): InternImageBlock(
      (blocks): ModuleList(
        (0): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=224)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=224, out_features=252, bias=True)
            (mask): Linear(in_features=224, out_features=126, bias=True)
            (input_proj): Linear(in_features=224, out_features=224, bias=True)
            (output_proj): Linear(in_features=224, out_features=224, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.062)
          (norm2): Sequential(
            (0): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=224, out_features=896, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=896, out_features=224, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=224)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=224, out_features=252, bias=True)
            (mask): Linear(in_features=224, out_features=126, bias=True)
            (input_proj): Linear(in_features=224, out_features=224, bias=True)
            (output_proj): Linear(in_features=224, out_features=224, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.078)
          (norm2): Sequential(
            (0): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=224, out_features=896, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=896, out_features=224, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=224)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=224, out_features=252, bias=True)
            (mask): Linear(in_features=224, out_features=126, bias=True)
            (input_proj): Linear(in_features=224, out_features=224, bias=True)
            (output_proj): Linear(in_features=224, out_features=224, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.094)
          (norm2): Sequential(
            (0): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=224, out_features=896, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=896, out_features=224, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(224, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=224)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=224, out_features=252, bias=True)
            (mask): Linear(in_features=224, out_features=126, bias=True)
            (input_proj): Linear(in_features=224, out_features=224, bias=True)
            (output_proj): Linear(in_features=224, out_features=224, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.109)
          (norm2): Sequential(
            (0): LayerNorm((224,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=224, out_features=896, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=896, out_features=224, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): DownsampleLayer(
        (conv): Conv2d(224, 448, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (norm): Sequential(
          (0): to_channels_last()
          (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (2): InternImageBlock(
      (blocks): ModuleList(
        (0): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.125)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.141)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.156)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.172)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.188)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.203)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (6): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.219)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (7): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.234)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (8): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.250)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (9): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.266)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (10): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.281)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (11): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.297)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (12): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.312)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (13): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.328)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (14): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.344)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (15): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.359)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (16): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.375)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (17): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.391)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (18): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.406)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (19): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.422)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (20): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(448, 448, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=448)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=448, out_features=504, bias=True)
            (mask): Linear(in_features=448, out_features=252, bias=True)
            (input_proj): Linear(in_features=448, out_features=448, bias=True)
            (output_proj): Linear(in_features=448, out_features=448, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.438)
          (norm2): Sequential(
            (0): LayerNorm((448,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=448, out_features=1792, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=1792, out_features=448, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): DownsampleLayer(
        (conv): Conv2d(448, 896, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (norm): Sequential(
          (0): to_channels_last()
          (1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
        )
      )
    )
    (3): InternImageBlock(
      (blocks): ModuleList(
        (0): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=896)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=896, out_features=1008, bias=True)
            (mask): Linear(in_features=896, out_features=504, bias=True)
            (input_proj): Linear(in_features=896, out_features=896, bias=True)
            (output_proj): Linear(in_features=896, out_features=896, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.453)
          (norm2): Sequential(
            (0): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=896, out_features=3584, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3584, out_features=896, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=896)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=896, out_features=1008, bias=True)
            (mask): Linear(in_features=896, out_features=504, bias=True)
            (input_proj): Linear(in_features=896, out_features=896, bias=True)
            (output_proj): Linear(in_features=896, out_features=896, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.469)
          (norm2): Sequential(
            (0): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=896, out_features=3584, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3584, out_features=896, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=896)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=896, out_features=1008, bias=True)
            (mask): Linear(in_features=896, out_features=504, bias=True)
            (input_proj): Linear(in_features=896, out_features=896, bias=True)
            (output_proj): Linear(in_features=896, out_features=896, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.484)
          (norm2): Sequential(
            (0): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=896, out_features=3584, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3584, out_features=896, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): InternImageLayer(
          (norm1): Sequential(
            (0): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          )
          (dcn): DCNv3(
            (dw_conv): Sequential(
              (0): Conv2d(896, 896, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=896)
              (1): Sequential(
                (0): to_channels_last()
                (1): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
              )
              (2): GELU(approximate='none')
            )
            (offset): Linear(in_features=896, out_features=1008, bias=True)
            (mask): Linear(in_features=896, out_features=504, bias=True)
            (input_proj): Linear(in_features=896, out_features=896, bias=True)
            (output_proj): Linear(in_features=896, out_features=896, bias=True)
          )
          (drop_path): DropPath(drop_prob=0.500)
          (norm2): Sequential(
            (0): LayerNorm((896,), eps=1e-06, elementwise_affine=True)
          )
          (mlp): MLPLayer(
            (fc1): Linear(in_features=896, out_features=3584, bias=True)
            (act): GELU(approximate='none')
            (fc2): Linear(in_features=3584, out_features=896, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (conv_head): Sequential(
    (0): Conv2d(896, 1344, kernel_size=(1, 1), stride=(1, 1), bias=False)
    (1): Sequential(
      (0): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): GELU(approximate='none')
  )
  (head): Linear(in_features=1344, out_features=14, bias=True)
  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
)
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
layer-wise lr decay is used !
no decay params: {no_decay_name}
lr_ratio_params:
patch_embed.conv1.weight (0.0001, 0.013939837037683136, 0.05, True)
patch_embed.conv1.bias (0.0001, 0.013939837037683136, 0.0, True)
patch_embed.norm1.1.weight (0.0001, 0.013939837037683136, 0.0, True)
patch_embed.norm1.1.bias (0.0001, 0.013939837037683136, 0.0, True)
patch_embed.conv2.weight (0.0001, 0.013939837037683136, 0.05, True)
patch_embed.conv2.bias (0.0001, 0.013939837037683136, 0.0, True)
patch_embed.norm2.1.weight (0.0001, 0.013939837037683136, 0.0, True)
patch_embed.norm2.1.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.gamma1 (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.gamma2 (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.norm1.0.weight (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.norm1.0.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.dcn.dw_conv.0.weight (0.0001, 0.013939837037683136, 0.05, True)
levels.0.blocks.0.dcn.dw_conv.0.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.dcn.dw_conv.1.1.weight (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.dcn.dw_conv.1.1.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.dcn.offset.weight (0.0001, 0.013939837037683136, 0.05, True)
levels.0.blocks.0.dcn.offset.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.dcn.mask.weight (0.0001, 0.013939837037683136, 0.05, True)
levels.0.blocks.0.dcn.mask.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.dcn.input_proj.weight (0.0001, 0.013939837037683136, 0.05, True)
levels.0.blocks.0.dcn.input_proj.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.dcn.output_proj.weight (0.0001, 0.013939837037683136, 0.05, True)
levels.0.blocks.0.dcn.output_proj.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.norm2.0.weight (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.norm2.0.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.mlp.fc1.weight (0.0001, 0.013939837037683136, 0.05, True)
levels.0.blocks.0.mlp.fc1.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.0.mlp.fc2.weight (0.0001, 0.013939837037683136, 0.05, True)
levels.0.blocks.0.mlp.fc2.bias (0.0001, 0.013939837037683136, 0.0, True)
levels.0.blocks.1.gamma1 (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.gamma2 (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.norm1.0.weight (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.norm1.0.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.dcn.dw_conv.0.weight (0.0001, 0.015931242328780727, 0.05, True)
levels.0.blocks.1.dcn.dw_conv.0.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.dcn.dw_conv.1.1.weight (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.dcn.dw_conv.1.1.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.dcn.offset.weight (0.0001, 0.015931242328780727, 0.05, True)
levels.0.blocks.1.dcn.offset.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.dcn.mask.weight (0.0001, 0.015931242328780727, 0.05, True)
levels.0.blocks.1.dcn.mask.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.dcn.input_proj.weight (0.0001, 0.015931242328780727, 0.05, True)
levels.0.blocks.1.dcn.input_proj.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.dcn.output_proj.weight (0.0001, 0.015931242328780727, 0.05, True)
levels.0.blocks.1.dcn.output_proj.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.norm2.0.weight (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.norm2.0.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.mlp.fc1.weight (0.0001, 0.015931242328780727, 0.05, True)
levels.0.blocks.1.mlp.fc1.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.1.mlp.fc2.weight (0.0001, 0.015931242328780727, 0.05, True)
levels.0.blocks.1.mlp.fc2.bias (0.0001, 0.015931242328780727, 0.0, True)
levels.0.blocks.2.gamma1 (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.gamma2 (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.norm1.0.weight (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.norm1.0.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.dcn.dw_conv.0.weight (0.0001, 0.018207134090035115, 0.05, True)
levels.0.blocks.2.dcn.dw_conv.0.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.dcn.dw_conv.1.1.weight (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.dcn.dw_conv.1.1.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.dcn.offset.weight (0.0001, 0.018207134090035115, 0.05, True)
levels.0.blocks.2.dcn.offset.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.dcn.mask.weight (0.0001, 0.018207134090035115, 0.05, True)
levels.0.blocks.2.dcn.mask.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.dcn.input_proj.weight (0.0001, 0.018207134090035115, 0.05, True)
levels.0.blocks.2.dcn.input_proj.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.dcn.output_proj.weight (0.0001, 0.018207134090035115, 0.05, True)
levels.0.blocks.2.dcn.output_proj.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.norm2.0.weight (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.norm2.0.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.mlp.fc1.weight (0.0001, 0.018207134090035115, 0.05, True)
levels.0.blocks.2.mlp.fc1.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.2.mlp.fc2.weight (0.0001, 0.018207134090035115, 0.05, True)
levels.0.blocks.2.mlp.fc2.bias (0.0001, 0.018207134090035115, 0.0, True)
levels.0.blocks.3.gamma1 (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.gamma2 (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.norm1.0.weight (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.norm1.0.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.dcn.dw_conv.0.weight (0.0001, 0.02080815324575442, 0.05, True)
levels.0.blocks.3.dcn.dw_conv.0.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.dcn.dw_conv.1.1.weight (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.dcn.dw_conv.1.1.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.dcn.offset.weight (0.0001, 0.02080815324575442, 0.05, True)
levels.0.blocks.3.dcn.offset.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.dcn.mask.weight (0.0001, 0.02080815324575442, 0.05, True)
levels.0.blocks.3.dcn.mask.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.dcn.input_proj.weight (0.0001, 0.02080815324575442, 0.05, True)
levels.0.blocks.3.dcn.input_proj.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.dcn.output_proj.weight (0.0001, 0.02080815324575442, 0.05, True)
levels.0.blocks.3.dcn.output_proj.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.norm2.0.weight (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.norm2.0.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.mlp.fc1.weight (0.0001, 0.02080815324575442, 0.05, True)
levels.0.blocks.3.mlp.fc1.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.blocks.3.mlp.fc2.weight (0.0001, 0.02080815324575442, 0.05, True)
levels.0.blocks.3.mlp.fc2.bias (0.0001, 0.02080815324575442, 0.0, True)
levels.0.downsample.conv.weight (0.0001, 0.02378074656657648, 0.05, True)
levels.0.downsample.norm.1.weight (0.0001, 0.02378074656657648, 0.0, True)
levels.0.downsample.norm.1.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.gamma1 (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.gamma2 (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.norm1.0.weight (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.norm1.0.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.dcn.dw_conv.0.weight (0.0001, 0.02378074656657648, 0.05, True)
levels.1.blocks.0.dcn.dw_conv.0.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.dcn.dw_conv.1.1.weight (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.dcn.dw_conv.1.1.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.dcn.offset.weight (0.0001, 0.02378074656657648, 0.05, True)
levels.1.blocks.0.dcn.offset.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.dcn.mask.weight (0.0001, 0.02378074656657648, 0.05, True)
levels.1.blocks.0.dcn.mask.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.dcn.input_proj.weight (0.0001, 0.02378074656657648, 0.05, True)
levels.1.blocks.0.dcn.input_proj.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.dcn.output_proj.weight (0.0001, 0.02378074656657648, 0.05, True)
levels.1.blocks.0.dcn.output_proj.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.norm2.0.weight (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.norm2.0.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.mlp.fc1.weight (0.0001, 0.02378074656657648, 0.05, True)
levels.1.blocks.0.mlp.fc1.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.0.mlp.fc2.weight (0.0001, 0.02378074656657648, 0.05, True)
levels.1.blocks.0.mlp.fc2.bias (0.0001, 0.02378074656657648, 0.0, True)
levels.1.blocks.1.gamma1 (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.gamma2 (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.norm1.0.weight (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.norm1.0.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.dcn.dw_conv.0.weight (0.0001, 0.027177996076087403, 0.05, True)
levels.1.blocks.1.dcn.dw_conv.0.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.dcn.dw_conv.1.1.weight (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.dcn.dw_conv.1.1.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.dcn.offset.weight (0.0001, 0.027177996076087403, 0.05, True)
levels.1.blocks.1.dcn.offset.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.dcn.mask.weight (0.0001, 0.027177996076087403, 0.05, True)
levels.1.blocks.1.dcn.mask.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.dcn.input_proj.weight (0.0001, 0.027177996076087403, 0.05, True)
levels.1.blocks.1.dcn.input_proj.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.dcn.output_proj.weight (0.0001, 0.027177996076087403, 0.05, True)
levels.1.blocks.1.dcn.output_proj.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.norm2.0.weight (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.norm2.0.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.mlp.fc1.weight (0.0001, 0.027177996076087403, 0.05, True)
levels.1.blocks.1.mlp.fc1.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.1.mlp.fc2.weight (0.0001, 0.027177996076087403, 0.05, True)
levels.1.blocks.1.mlp.fc2.bias (0.0001, 0.027177996076087403, 0.0, True)
levels.1.blocks.2.gamma1 (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.gamma2 (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.norm1.0.weight (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.norm1.0.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.dcn.dw_conv.0.weight (0.0001, 0.03106056694409989, 0.05, True)
levels.1.blocks.2.dcn.dw_conv.0.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.dcn.dw_conv.1.1.weight (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.dcn.dw_conv.1.1.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.dcn.offset.weight (0.0001, 0.03106056694409989, 0.05, True)
levels.1.blocks.2.dcn.offset.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.dcn.mask.weight (0.0001, 0.03106056694409989, 0.05, True)
levels.1.blocks.2.dcn.mask.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.dcn.input_proj.weight (0.0001, 0.03106056694409989, 0.05, True)
levels.1.blocks.2.dcn.input_proj.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.dcn.output_proj.weight (0.0001, 0.03106056694409989, 0.05, True)
levels.1.blocks.2.dcn.output_proj.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.norm2.0.weight (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.norm2.0.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.mlp.fc1.weight (0.0001, 0.03106056694409989, 0.05, True)
levels.1.blocks.2.mlp.fc1.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.2.mlp.fc2.weight (0.0001, 0.03106056694409989, 0.05, True)
levels.1.blocks.2.mlp.fc2.bias (0.0001, 0.03106056694409989, 0.0, True)
levels.1.blocks.3.gamma1 (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.gamma2 (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.norm1.0.weight (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.norm1.0.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.dcn.dw_conv.0.weight (0.0001, 0.03549779079325702, 0.05, True)
levels.1.blocks.3.dcn.dw_conv.0.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.dcn.dw_conv.1.1.weight (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.dcn.dw_conv.1.1.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.dcn.offset.weight (0.0001, 0.03549779079325702, 0.05, True)
levels.1.blocks.3.dcn.offset.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.dcn.mask.weight (0.0001, 0.03549779079325702, 0.05, True)
levels.1.blocks.3.dcn.mask.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.dcn.input_proj.weight (0.0001, 0.03549779079325702, 0.05, True)
levels.1.blocks.3.dcn.input_proj.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.dcn.output_proj.weight (0.0001, 0.03549779079325702, 0.05, True)
levels.1.blocks.3.dcn.output_proj.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.norm2.0.weight (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.norm2.0.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.mlp.fc1.weight (0.0001, 0.03549779079325702, 0.05, True)
levels.1.blocks.3.mlp.fc1.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.blocks.3.mlp.fc2.weight (0.0001, 0.03549779079325702, 0.05, True)
levels.1.blocks.3.mlp.fc2.bias (0.0001, 0.03549779079325702, 0.0, True)
levels.1.downsample.conv.weight (0.0001, 0.040568903763722304, 0.05, True)
levels.1.downsample.norm.1.weight (0.0001, 0.040568903763722304, 0.0, True)
levels.1.downsample.norm.1.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.gamma1 (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.gamma2 (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.norm1.0.weight (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.norm1.0.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.dcn.dw_conv.0.weight (0.0001, 0.040568903763722304, 0.05, True)
levels.2.blocks.0.dcn.dw_conv.0.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.dcn.dw_conv.1.1.weight (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.dcn.dw_conv.1.1.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.dcn.offset.weight (0.0001, 0.040568903763722304, 0.05, True)
levels.2.blocks.0.dcn.offset.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.dcn.mask.weight (0.0001, 0.040568903763722304, 0.05, True)
levels.2.blocks.0.dcn.mask.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.dcn.input_proj.weight (0.0001, 0.040568903763722304, 0.05, True)
levels.2.blocks.0.dcn.input_proj.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.dcn.output_proj.weight (0.0001, 0.040568903763722304, 0.05, True)
levels.2.blocks.0.dcn.output_proj.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.norm2.0.weight (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.norm2.0.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.mlp.fc1.weight (0.0001, 0.040568903763722304, 0.05, True)
levels.2.blocks.0.mlp.fc1.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.0.mlp.fc2.weight (0.0001, 0.040568903763722304, 0.05, True)
levels.2.blocks.0.mlp.fc2.bias (0.0001, 0.040568903763722304, 0.0, True)
levels.2.blocks.1.gamma1 (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.gamma2 (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.norm1.0.weight (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.norm1.0.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.dcn.dw_conv.0.weight (0.0001, 0.04636446144425406, 0.05, True)
levels.2.blocks.1.dcn.dw_conv.0.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.dcn.dw_conv.1.1.weight (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.dcn.dw_conv.1.1.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.dcn.offset.weight (0.0001, 0.04636446144425406, 0.05, True)
levels.2.blocks.1.dcn.offset.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.dcn.mask.weight (0.0001, 0.04636446144425406, 0.05, True)
levels.2.blocks.1.dcn.mask.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.dcn.input_proj.weight (0.0001, 0.04636446144425406, 0.05, True)
levels.2.blocks.1.dcn.input_proj.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.dcn.output_proj.weight (0.0001, 0.04636446144425406, 0.05, True)
levels.2.blocks.1.dcn.output_proj.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.norm2.0.weight (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.norm2.0.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.mlp.fc1.weight (0.0001, 0.04636446144425406, 0.05, True)
levels.2.blocks.1.mlp.fc1.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.1.mlp.fc2.weight (0.0001, 0.04636446144425406, 0.05, True)
levels.2.blocks.1.mlp.fc2.bias (0.0001, 0.04636446144425406, 0.0, True)
levels.2.blocks.2.gamma1 (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.gamma2 (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.norm1.0.weight (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.norm1.0.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.dcn.dw_conv.0.weight (0.0001, 0.05298795593629036, 0.05, True)
levels.2.blocks.2.dcn.dw_conv.0.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.dcn.dw_conv.1.1.weight (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.dcn.dw_conv.1.1.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.dcn.offset.weight (0.0001, 0.05298795593629036, 0.05, True)
levels.2.blocks.2.dcn.offset.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.dcn.mask.weight (0.0001, 0.05298795593629036, 0.05, True)
levels.2.blocks.2.dcn.mask.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.dcn.input_proj.weight (0.0001, 0.05298795593629036, 0.05, True)
levels.2.blocks.2.dcn.input_proj.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.dcn.output_proj.weight (0.0001, 0.05298795593629036, 0.05, True)
levels.2.blocks.2.dcn.output_proj.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.norm2.0.weight (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.norm2.0.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.mlp.fc1.weight (0.0001, 0.05298795593629036, 0.05, True)
levels.2.blocks.2.mlp.fc1.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.2.mlp.fc2.weight (0.0001, 0.05298795593629036, 0.05, True)
levels.2.blocks.2.mlp.fc2.bias (0.0001, 0.05298795593629036, 0.0, True)
levels.2.blocks.3.gamma1 (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.gamma2 (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.norm1.0.weight (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.norm1.0.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.dcn.dw_conv.0.weight (0.0001, 0.06055766392718898, 0.05, True)
levels.2.blocks.3.dcn.dw_conv.0.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.dcn.dw_conv.1.1.weight (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.dcn.dw_conv.1.1.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.dcn.offset.weight (0.0001, 0.06055766392718898, 0.05, True)
levels.2.blocks.3.dcn.offset.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.dcn.mask.weight (0.0001, 0.06055766392718898, 0.05, True)
levels.2.blocks.3.dcn.mask.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.dcn.input_proj.weight (0.0001, 0.06055766392718898, 0.05, True)
levels.2.blocks.3.dcn.input_proj.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.dcn.output_proj.weight (0.0001, 0.06055766392718898, 0.05, True)
levels.2.blocks.3.dcn.output_proj.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.norm2.0.weight (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.norm2.0.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.mlp.fc1.weight (0.0001, 0.06055766392718898, 0.05, True)
levels.2.blocks.3.mlp.fc1.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.3.mlp.fc2.weight (0.0001, 0.06055766392718898, 0.05, True)
levels.2.blocks.3.mlp.fc2.bias (0.0001, 0.06055766392718898, 0.0, True)
levels.2.blocks.4.gamma1 (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.gamma2 (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.norm1.0.weight (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.norm1.0.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.dcn.dw_conv.0.weight (0.0001, 0.06920875877393026, 0.05, True)
levels.2.blocks.4.dcn.dw_conv.0.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.dcn.dw_conv.1.1.weight (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.dcn.dw_conv.1.1.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.dcn.offset.weight (0.0001, 0.06920875877393026, 0.05, True)
levels.2.blocks.4.dcn.offset.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.dcn.mask.weight (0.0001, 0.06920875877393026, 0.05, True)
levels.2.blocks.4.dcn.mask.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.dcn.input_proj.weight (0.0001, 0.06920875877393026, 0.05, True)
levels.2.blocks.4.dcn.input_proj.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.dcn.output_proj.weight (0.0001, 0.06920875877393026, 0.05, True)
levels.2.blocks.4.dcn.output_proj.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.norm2.0.weight (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.norm2.0.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.mlp.fc1.weight (0.0001, 0.06920875877393026, 0.05, True)
levels.2.blocks.4.mlp.fc1.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.4.mlp.fc2.weight (0.0001, 0.06920875877393026, 0.05, True)
levels.2.blocks.4.mlp.fc2.bias (0.0001, 0.06920875877393026, 0.0, True)
levels.2.blocks.5.gamma1 (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.gamma2 (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.norm1.0.weight (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.norm1.0.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.dcn.dw_conv.0.weight (0.0001, 0.07909572431306316, 0.05, True)
levels.2.blocks.5.dcn.dw_conv.0.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.dcn.dw_conv.1.1.weight (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.dcn.dw_conv.1.1.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.dcn.offset.weight (0.0001, 0.07909572431306316, 0.05, True)
levels.2.blocks.5.dcn.offset.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.dcn.mask.weight (0.0001, 0.07909572431306316, 0.05, True)
levels.2.blocks.5.dcn.mask.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.dcn.input_proj.weight (0.0001, 0.07909572431306316, 0.05, True)
levels.2.blocks.5.dcn.input_proj.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.dcn.output_proj.weight (0.0001, 0.07909572431306316, 0.05, True)
levels.2.blocks.5.dcn.output_proj.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.norm2.0.weight (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.norm2.0.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.mlp.fc1.weight (0.0001, 0.07909572431306316, 0.05, True)
levels.2.blocks.5.mlp.fc1.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.5.mlp.fc2.weight (0.0001, 0.07909572431306316, 0.05, True)
levels.2.blocks.5.mlp.fc2.bias (0.0001, 0.07909572431306316, 0.0, True)
levels.2.blocks.6.gamma1 (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.gamma2 (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.norm1.0.weight (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.norm1.0.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.dcn.dw_conv.0.weight (0.0001, 0.09039511350064361, 0.05, True)
levels.2.blocks.6.dcn.dw_conv.0.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.dcn.dw_conv.1.1.weight (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.dcn.dw_conv.1.1.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.dcn.offset.weight (0.0001, 0.09039511350064361, 0.05, True)
levels.2.blocks.6.dcn.offset.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.dcn.mask.weight (0.0001, 0.09039511350064361, 0.05, True)
levels.2.blocks.6.dcn.mask.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.dcn.input_proj.weight (0.0001, 0.09039511350064361, 0.05, True)
levels.2.blocks.6.dcn.input_proj.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.dcn.output_proj.weight (0.0001, 0.09039511350064361, 0.05, True)
levels.2.blocks.6.dcn.output_proj.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.norm2.0.weight (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.norm2.0.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.mlp.fc1.weight (0.0001, 0.09039511350064361, 0.05, True)
levels.2.blocks.6.mlp.fc1.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.6.mlp.fc2.weight (0.0001, 0.09039511350064361, 0.05, True)
levels.2.blocks.6.mlp.fc2.bias (0.0001, 0.09039511350064361, 0.0, True)
levels.2.blocks.7.gamma1 (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.gamma2 (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.norm1.0.weight (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.norm1.0.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.dcn.dw_conv.0.weight (0.0001, 0.1033087011435927, 0.05, True)
levels.2.blocks.7.dcn.dw_conv.0.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.dcn.dw_conv.1.1.weight (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.dcn.dw_conv.1.1.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.dcn.offset.weight (0.0001, 0.1033087011435927, 0.05, True)
levels.2.blocks.7.dcn.offset.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.dcn.mask.weight (0.0001, 0.1033087011435927, 0.05, True)
levels.2.blocks.7.dcn.mask.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.dcn.input_proj.weight (0.0001, 0.1033087011435927, 0.05, True)
levels.2.blocks.7.dcn.input_proj.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.dcn.output_proj.weight (0.0001, 0.1033087011435927, 0.05, True)
levels.2.blocks.7.dcn.output_proj.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.norm2.0.weight (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.norm2.0.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.mlp.fc1.weight (0.0001, 0.1033087011435927, 0.05, True)
levels.2.blocks.7.mlp.fc1.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.7.mlp.fc2.weight (0.0001, 0.1033087011435927, 0.05, True)
levels.2.blocks.7.mlp.fc2.bias (0.0001, 0.1033087011435927, 0.0, True)
levels.2.blocks.8.gamma1 (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.gamma2 (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.norm1.0.weight (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.norm1.0.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.dcn.dw_conv.0.weight (0.0001, 0.1180670870212488, 0.05, True)
levels.2.blocks.8.dcn.dw_conv.0.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.dcn.dw_conv.1.1.weight (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.dcn.dw_conv.1.1.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.dcn.offset.weight (0.0001, 0.1180670870212488, 0.05, True)
levels.2.blocks.8.dcn.offset.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.dcn.mask.weight (0.0001, 0.1180670870212488, 0.05, True)
levels.2.blocks.8.dcn.mask.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.dcn.input_proj.weight (0.0001, 0.1180670870212488, 0.05, True)
levels.2.blocks.8.dcn.input_proj.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.dcn.output_proj.weight (0.0001, 0.1180670870212488, 0.05, True)
levels.2.blocks.8.dcn.output_proj.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.norm2.0.weight (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.norm2.0.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.mlp.fc1.weight (0.0001, 0.1180670870212488, 0.05, True)
levels.2.blocks.8.mlp.fc1.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.8.mlp.fc2.weight (0.0001, 0.1180670870212488, 0.05, True)
levels.2.blocks.8.mlp.fc2.bias (0.0001, 0.1180670870212488, 0.0, True)
levels.2.blocks.9.gamma1 (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.gamma2 (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.norm1.0.weight (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.norm1.0.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.dcn.dw_conv.0.weight (0.0001, 0.13493381373857005, 0.05, True)
levels.2.blocks.9.dcn.dw_conv.0.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.dcn.dw_conv.1.1.weight (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.dcn.dw_conv.1.1.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.dcn.offset.weight (0.0001, 0.13493381373857005, 0.05, True)
levels.2.blocks.9.dcn.offset.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.dcn.mask.weight (0.0001, 0.13493381373857005, 0.05, True)
levels.2.blocks.9.dcn.mask.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.dcn.input_proj.weight (0.0001, 0.13493381373857005, 0.05, True)
levels.2.blocks.9.dcn.input_proj.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.dcn.output_proj.weight (0.0001, 0.13493381373857005, 0.05, True)
levels.2.blocks.9.dcn.output_proj.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.norm2.0.weight (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.norm2.0.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.mlp.fc1.weight (0.0001, 0.13493381373857005, 0.05, True)
levels.2.blocks.9.mlp.fc1.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.9.mlp.fc2.weight (0.0001, 0.13493381373857005, 0.05, True)
levels.2.blocks.9.mlp.fc2.bias (0.0001, 0.13493381373857005, 0.0, True)
levels.2.blocks.10.gamma1 (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.gamma2 (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.norm1.0.weight (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.norm1.0.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.dcn.dw_conv.0.weight (0.0001, 0.15421007284408006, 0.05, True)
levels.2.blocks.10.dcn.dw_conv.0.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.dcn.dw_conv.1.1.weight (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.dcn.dw_conv.1.1.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.dcn.offset.weight (0.0001, 0.15421007284408006, 0.05, True)
levels.2.blocks.10.dcn.offset.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.dcn.mask.weight (0.0001, 0.15421007284408006, 0.05, True)
levels.2.blocks.10.dcn.mask.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.dcn.input_proj.weight (0.0001, 0.15421007284408006, 0.05, True)
levels.2.blocks.10.dcn.input_proj.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.dcn.output_proj.weight (0.0001, 0.15421007284408006, 0.05, True)
levels.2.blocks.10.dcn.output_proj.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.norm2.0.weight (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.norm2.0.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.mlp.fc1.weight (0.0001, 0.15421007284408006, 0.05, True)
levels.2.blocks.10.mlp.fc1.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.10.mlp.fc2.weight (0.0001, 0.15421007284408006, 0.05, True)
levels.2.blocks.10.mlp.fc2.bias (0.0001, 0.15421007284408006, 0.0, True)
levels.2.blocks.11.gamma1 (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.gamma2 (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.norm1.0.weight (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.norm1.0.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.dcn.dw_conv.0.weight (0.0001, 0.1762400832503772, 0.05, True)
levels.2.blocks.11.dcn.dw_conv.0.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.dcn.dw_conv.1.1.weight (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.dcn.dw_conv.1.1.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.dcn.offset.weight (0.0001, 0.1762400832503772, 0.05, True)
levels.2.blocks.11.dcn.offset.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.dcn.mask.weight (0.0001, 0.1762400832503772, 0.05, True)
levels.2.blocks.11.dcn.mask.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.dcn.input_proj.weight (0.0001, 0.1762400832503772, 0.05, True)
levels.2.blocks.11.dcn.input_proj.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.dcn.output_proj.weight (0.0001, 0.1762400832503772, 0.05, True)
levels.2.blocks.11.dcn.output_proj.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.norm2.0.weight (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.norm2.0.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.mlp.fc1.weight (0.0001, 0.1762400832503772, 0.05, True)
levels.2.blocks.11.mlp.fc1.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.11.mlp.fc2.weight (0.0001, 0.1762400832503772, 0.05, True)
levels.2.blocks.11.mlp.fc2.bias (0.0001, 0.1762400832503772, 0.0, True)
levels.2.blocks.12.gamma1 (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.gamma2 (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.norm1.0.weight (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.norm1.0.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.dcn.dw_conv.0.weight (0.0001, 0.2014172380004311, 0.05, True)
levels.2.blocks.12.dcn.dw_conv.0.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.dcn.dw_conv.1.1.weight (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.dcn.dw_conv.1.1.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.dcn.offset.weight (0.0001, 0.2014172380004311, 0.05, True)
levels.2.blocks.12.dcn.offset.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.dcn.mask.weight (0.0001, 0.2014172380004311, 0.05, True)
levels.2.blocks.12.dcn.mask.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.dcn.input_proj.weight (0.0001, 0.2014172380004311, 0.05, True)
levels.2.blocks.12.dcn.input_proj.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.dcn.output_proj.weight (0.0001, 0.2014172380004311, 0.05, True)
levels.2.blocks.12.dcn.output_proj.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.norm2.0.weight (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.norm2.0.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.mlp.fc1.weight (0.0001, 0.2014172380004311, 0.05, True)
levels.2.blocks.12.mlp.fc1.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.12.mlp.fc2.weight (0.0001, 0.2014172380004311, 0.05, True)
levels.2.blocks.12.mlp.fc2.bias (0.0001, 0.2014172380004311, 0.0, True)
levels.2.blocks.13.gamma1 (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.gamma2 (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.norm1.0.weight (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.norm1.0.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.dcn.dw_conv.0.weight (0.0001, 0.23019112914334983, 0.05, True)
levels.2.blocks.13.dcn.dw_conv.0.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.dcn.dw_conv.1.1.weight (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.dcn.dw_conv.1.1.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.dcn.offset.weight (0.0001, 0.23019112914334983, 0.05, True)
levels.2.blocks.13.dcn.offset.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.dcn.mask.weight (0.0001, 0.23019112914334983, 0.05, True)
levels.2.blocks.13.dcn.mask.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.dcn.input_proj.weight (0.0001, 0.23019112914334983, 0.05, True)
levels.2.blocks.13.dcn.input_proj.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.dcn.output_proj.weight (0.0001, 0.23019112914334983, 0.05, True)
levels.2.blocks.13.dcn.output_proj.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.norm2.0.weight (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.norm2.0.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.mlp.fc1.weight (0.0001, 0.23019112914334983, 0.05, True)
levels.2.blocks.13.mlp.fc1.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.13.mlp.fc2.weight (0.0001, 0.23019112914334983, 0.05, True)
levels.2.blocks.13.mlp.fc2.bias (0.0001, 0.23019112914334983, 0.0, True)
levels.2.blocks.14.gamma1 (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.gamma2 (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.norm1.0.weight (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.norm1.0.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.dcn.dw_conv.0.weight (0.0001, 0.2630755761638284, 0.05, True)
levels.2.blocks.14.dcn.dw_conv.0.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.dcn.dw_conv.1.1.weight (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.dcn.dw_conv.1.1.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.dcn.offset.weight (0.0001, 0.2630755761638284, 0.05, True)
levels.2.blocks.14.dcn.offset.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.dcn.mask.weight (0.0001, 0.2630755761638284, 0.05, True)
levels.2.blocks.14.dcn.mask.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.dcn.input_proj.weight (0.0001, 0.2630755761638284, 0.05, True)
levels.2.blocks.14.dcn.input_proj.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.dcn.output_proj.weight (0.0001, 0.2630755761638284, 0.05, True)
levels.2.blocks.14.dcn.output_proj.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.norm2.0.weight (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.norm2.0.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.mlp.fc1.weight (0.0001, 0.2630755761638284, 0.05, True)
levels.2.blocks.14.mlp.fc1.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.14.mlp.fc2.weight (0.0001, 0.2630755761638284, 0.05, True)
levels.2.blocks.14.mlp.fc2.bias (0.0001, 0.2630755761638284, 0.0, True)
levels.2.blocks.15.gamma1 (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.gamma2 (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.norm1.0.weight (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.norm1.0.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.dcn.dw_conv.0.weight (0.0001, 0.30065780133008957, 0.05, True)
levels.2.blocks.15.dcn.dw_conv.0.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.dcn.dw_conv.1.1.weight (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.dcn.dw_conv.1.1.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.dcn.offset.weight (0.0001, 0.30065780133008957, 0.05, True)
levels.2.blocks.15.dcn.offset.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.dcn.mask.weight (0.0001, 0.30065780133008957, 0.05, True)
levels.2.blocks.15.dcn.mask.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.dcn.input_proj.weight (0.0001, 0.30065780133008957, 0.05, True)
levels.2.blocks.15.dcn.input_proj.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.dcn.output_proj.weight (0.0001, 0.30065780133008957, 0.05, True)
levels.2.blocks.15.dcn.output_proj.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.norm2.0.weight (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.norm2.0.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.mlp.fc1.weight (0.0001, 0.30065780133008957, 0.05, True)
levels.2.blocks.15.mlp.fc1.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.15.mlp.fc2.weight (0.0001, 0.30065780133008957, 0.05, True)
levels.2.blocks.15.mlp.fc2.bias (0.0001, 0.30065780133008957, 0.0, True)
levels.2.blocks.16.gamma1 (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.gamma2 (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.norm1.0.weight (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.norm1.0.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.dcn.dw_conv.0.weight (0.0001, 0.34360891580581665, 0.05, True)
levels.2.blocks.16.dcn.dw_conv.0.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.dcn.dw_conv.1.1.weight (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.dcn.dw_conv.1.1.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.dcn.offset.weight (0.0001, 0.34360891580581665, 0.05, True)
levels.2.blocks.16.dcn.offset.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.dcn.mask.weight (0.0001, 0.34360891580581665, 0.05, True)
levels.2.blocks.16.dcn.mask.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.dcn.input_proj.weight (0.0001, 0.34360891580581665, 0.05, True)
levels.2.blocks.16.dcn.input_proj.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.dcn.output_proj.weight (0.0001, 0.34360891580581665, 0.05, True)
levels.2.blocks.16.dcn.output_proj.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.norm2.0.weight (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.norm2.0.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.mlp.fc1.weight (0.0001, 0.34360891580581665, 0.05, True)
levels.2.blocks.16.mlp.fc1.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.16.mlp.fc2.weight (0.0001, 0.34360891580581665, 0.05, True)
levels.2.blocks.16.mlp.fc2.bias (0.0001, 0.34360891580581665, 0.0, True)
levels.2.blocks.17.gamma1 (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.gamma2 (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.norm1.0.weight (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.norm1.0.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.dcn.dw_conv.0.weight (0.0001, 0.39269590377807617, 0.05, True)
levels.2.blocks.17.dcn.dw_conv.0.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.dcn.dw_conv.1.1.weight (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.dcn.dw_conv.1.1.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.dcn.offset.weight (0.0001, 0.39269590377807617, 0.05, True)
levels.2.blocks.17.dcn.offset.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.dcn.mask.weight (0.0001, 0.39269590377807617, 0.05, True)
levels.2.blocks.17.dcn.mask.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.dcn.input_proj.weight (0.0001, 0.39269590377807617, 0.05, True)
levels.2.blocks.17.dcn.input_proj.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.dcn.output_proj.weight (0.0001, 0.39269590377807617, 0.05, True)
levels.2.blocks.17.dcn.output_proj.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.norm2.0.weight (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.norm2.0.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.mlp.fc1.weight (0.0001, 0.39269590377807617, 0.05, True)
levels.2.blocks.17.mlp.fc1.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.17.mlp.fc2.weight (0.0001, 0.39269590377807617, 0.05, True)
levels.2.blocks.17.mlp.fc2.bias (0.0001, 0.39269590377807617, 0.0, True)
levels.2.blocks.18.gamma1 (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.gamma2 (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.norm1.0.weight (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.norm1.0.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.dcn.dw_conv.0.weight (0.0001, 0.4487953186035156, 0.05, True)
levels.2.blocks.18.dcn.dw_conv.0.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.dcn.dw_conv.1.1.weight (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.dcn.dw_conv.1.1.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.dcn.offset.weight (0.0001, 0.4487953186035156, 0.05, True)
levels.2.blocks.18.dcn.offset.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.dcn.mask.weight (0.0001, 0.4487953186035156, 0.05, True)
levels.2.blocks.18.dcn.mask.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.dcn.input_proj.weight (0.0001, 0.4487953186035156, 0.05, True)
levels.2.blocks.18.dcn.input_proj.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.dcn.output_proj.weight (0.0001, 0.4487953186035156, 0.05, True)
levels.2.blocks.18.dcn.output_proj.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.norm2.0.weight (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.norm2.0.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.mlp.fc1.weight (0.0001, 0.4487953186035156, 0.05, True)
levels.2.blocks.18.mlp.fc1.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.18.mlp.fc2.weight (0.0001, 0.4487953186035156, 0.05, True)
levels.2.blocks.18.mlp.fc2.bias (0.0001, 0.4487953186035156, 0.0, True)
levels.2.blocks.19.gamma1 (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.gamma2 (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.norm1.0.weight (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.norm1.0.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.dcn.dw_conv.0.weight (0.0001, 0.512908935546875, 0.05, True)
levels.2.blocks.19.dcn.dw_conv.0.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.dcn.dw_conv.1.1.weight (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.dcn.dw_conv.1.1.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.dcn.offset.weight (0.0001, 0.512908935546875, 0.05, True)
levels.2.blocks.19.dcn.offset.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.dcn.mask.weight (0.0001, 0.512908935546875, 0.05, True)
levels.2.blocks.19.dcn.mask.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.dcn.input_proj.weight (0.0001, 0.512908935546875, 0.05, True)
levels.2.blocks.19.dcn.input_proj.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.dcn.output_proj.weight (0.0001, 0.512908935546875, 0.05, True)
levels.2.blocks.19.dcn.output_proj.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.norm2.0.weight (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.norm2.0.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.mlp.fc1.weight (0.0001, 0.512908935546875, 0.05, True)
levels.2.blocks.19.mlp.fc1.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.19.mlp.fc2.weight (0.0001, 0.512908935546875, 0.05, True)
levels.2.blocks.19.mlp.fc2.bias (0.0001, 0.512908935546875, 0.0, True)
levels.2.blocks.20.gamma1 (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.gamma2 (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.norm1.0.weight (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.norm1.0.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.dcn.dw_conv.0.weight (0.0001, 0.586181640625, 0.05, True)
levels.2.blocks.20.dcn.dw_conv.0.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.dcn.dw_conv.1.1.weight (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.dcn.dw_conv.1.1.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.dcn.offset.weight (0.0001, 0.586181640625, 0.05, True)
levels.2.blocks.20.dcn.offset.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.dcn.mask.weight (0.0001, 0.586181640625, 0.05, True)
levels.2.blocks.20.dcn.mask.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.dcn.input_proj.weight (0.0001, 0.586181640625, 0.05, True)
levels.2.blocks.20.dcn.input_proj.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.dcn.output_proj.weight (0.0001, 0.586181640625, 0.05, True)
levels.2.blocks.20.dcn.output_proj.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.norm2.0.weight (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.norm2.0.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.mlp.fc1.weight (0.0001, 0.586181640625, 0.05, True)
levels.2.blocks.20.mlp.fc1.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.blocks.20.mlp.fc2.weight (0.0001, 0.586181640625, 0.05, True)
levels.2.blocks.20.mlp.fc2.bias (0.0001, 0.586181640625, 0.0, True)
levels.2.downsample.conv.weight (0.0001, 0.669921875, 0.05, True)
levels.2.downsample.norm.1.weight (0.0001, 0.669921875, 0.0, True)
levels.2.downsample.norm.1.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.gamma1 (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.gamma2 (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.norm1.0.weight (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.norm1.0.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.dcn.dw_conv.0.weight (0.0001, 0.669921875, 0.05, True)
levels.3.blocks.0.dcn.dw_conv.0.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.dcn.dw_conv.1.1.weight (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.dcn.dw_conv.1.1.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.dcn.offset.weight (0.0001, 0.669921875, 0.05, True)
levels.3.blocks.0.dcn.offset.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.dcn.mask.weight (0.0001, 0.669921875, 0.05, True)
levels.3.blocks.0.dcn.mask.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.dcn.input_proj.weight (0.0001, 0.669921875, 0.05, True)
levels.3.blocks.0.dcn.input_proj.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.dcn.output_proj.weight (0.0001, 0.669921875, 0.05, True)
levels.3.blocks.0.dcn.output_proj.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.norm2.0.weight (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.norm2.0.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.mlp.fc1.weight (0.0001, 0.669921875, 0.05, True)
levels.3.blocks.0.mlp.fc1.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.0.mlp.fc2.weight (0.0001, 0.669921875, 0.05, True)
levels.3.blocks.0.mlp.fc2.bias (0.0001, 0.669921875, 0.0, True)
levels.3.blocks.1.gamma1 (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.gamma2 (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.norm1.0.weight (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.norm1.0.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.dcn.dw_conv.0.weight (0.0001, 0.765625, 0.05, True)
levels.3.blocks.1.dcn.dw_conv.0.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.dcn.dw_conv.1.1.weight (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.dcn.dw_conv.1.1.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.dcn.offset.weight (0.0001, 0.765625, 0.05, True)
levels.3.blocks.1.dcn.offset.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.dcn.mask.weight (0.0001, 0.765625, 0.05, True)
levels.3.blocks.1.dcn.mask.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.dcn.input_proj.weight (0.0001, 0.765625, 0.05, True)
levels.3.blocks.1.dcn.input_proj.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.dcn.output_proj.weight (0.0001, 0.765625, 0.05, True)
levels.3.blocks.1.dcn.output_proj.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.norm2.0.weight (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.norm2.0.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.mlp.fc1.weight (0.0001, 0.765625, 0.05, True)
levels.3.blocks.1.mlp.fc1.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.1.mlp.fc2.weight (0.0001, 0.765625, 0.05, True)
levels.3.blocks.1.mlp.fc2.bias (0.0001, 0.765625, 0.0, True)
levels.3.blocks.2.gamma1 (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.gamma2 (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.norm1.0.weight (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.norm1.0.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.dcn.dw_conv.0.weight (0.0001, 0.875, 0.05, True)
levels.3.blocks.2.dcn.dw_conv.0.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.dcn.dw_conv.1.1.weight (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.dcn.dw_conv.1.1.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.dcn.offset.weight (0.0001, 0.875, 0.05, True)
levels.3.blocks.2.dcn.offset.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.dcn.mask.weight (0.0001, 0.875, 0.05, True)
levels.3.blocks.2.dcn.mask.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.dcn.input_proj.weight (0.0001, 0.875, 0.05, True)
levels.3.blocks.2.dcn.input_proj.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.dcn.output_proj.weight (0.0001, 0.875, 0.05, True)
levels.3.blocks.2.dcn.output_proj.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.norm2.0.weight (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.norm2.0.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.mlp.fc1.weight (0.0001, 0.875, 0.05, True)
levels.3.blocks.2.mlp.fc1.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.2.mlp.fc2.weight (0.0001, 0.875, 0.05, True)
levels.3.blocks.2.mlp.fc2.bias (0.0001, 0.875, 0.0, True)
levels.3.blocks.3.gamma1 (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.gamma2 (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.norm1.0.weight (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.norm1.0.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.dcn.dw_conv.0.weight (0.0001, 1.0, 0.05, True)
levels.3.blocks.3.dcn.dw_conv.0.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.dcn.dw_conv.1.1.weight (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.dcn.dw_conv.1.1.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.dcn.offset.weight (0.0001, 1.0, 0.05, True)
levels.3.blocks.3.dcn.offset.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.dcn.mask.weight (0.0001, 1.0, 0.05, True)
levels.3.blocks.3.dcn.mask.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.dcn.input_proj.weight (0.0001, 1.0, 0.05, True)
levels.3.blocks.3.dcn.input_proj.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.dcn.output_proj.weight (0.0001, 1.0, 0.05, True)
levels.3.blocks.3.dcn.output_proj.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.norm2.0.weight (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.norm2.0.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.mlp.fc1.weight (0.0001, 1.0, 0.05, True)
levels.3.blocks.3.mlp.fc1.bias (0.0001, 1.0, 0.0, True)
levels.3.blocks.3.mlp.fc2.weight (0.0001, 1.0, 0.05, True)
levels.3.blocks.3.mlp.fc2.bias (0.0001, 1.0, 0.0, True)
conv_head.0.weight (0.0001, None, 0.05, True)
conv_head.1.0.weight (0.0001, None, 0.0, True)
conv_head.1.0.bias (0.0001, None, 0.0, True)
head.weight (0.0001, None, 0.05, True)
head.bias (0.0001, None, 0.0, True)
/scratch/smehta90/InternImage_MIMIC/classification/utils.py:430: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  self._scaler = torch.cuda.amp.GradScaler()
[2025-11-20 03:13:45 internimage_b_mimic_cxr_224](main.py 214): INFO Using native Torch AMP. Training in mixed precision.
[2025-11-20 03:13:45 internimage_b_mimic_cxr_224](main.py 226): INFO using fp16_compress_hook!
[2025-11-20 03:13:45 internimage_b_mimic_cxr_224](main.py 234): INFO number of params: 96135662
[2025-11-20 03:13:45 internimage_b_mimic_cxr_224](main.py 247): INFO Using BCEWithLogitsLoss for multi-label classification
All checkpoints founded in output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224: ['ckpt_epoch_11.pth', 'ckpt_epoch_best.pth', 'ckpt_epoch_ema_best.pth']
The latest checkpoint founded: output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_11.pth
[2025-11-20 03:13:45 internimage_b_mimic_cxr_224](main.py 270): INFO auto resuming from output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_11.pth
[2025-11-20 03:13:45 internimage_b_mimic_cxr_224](utils.py 60): INFO ==============> Resuming form output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_11.pth....................
resuming model
[2025-11-20 03:13:48 internimage_b_mimic_cxr_224](utils.py 92): INFO <All keys matched successfully>
resuming optimizer
resuming lr_scheduler
[2025-11-20 03:13:48 internimage_b_mimic_cxr_224](utils.py 110): INFO => loaded successfully output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_11.pth (epoch 11)
[2025-11-20 03:14:26 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 38.284 (38.284)	Loss 0.2719 (0.2719)	Mem 4564MB
[2025-11-20 03:14:59 internimage_b_mimic_cxr_224](utils_multilabel.py 141): INFO  * Mean AUC-ROC 0.8084 Loss 0.2300
[2025-11-20 03:14:59 internimage_b_mimic_cxr_224](utils_multilabel.py 142): INFO  * Per-class AUC-ROC: ['0.8399', '0.8283', '0.8170', '0.9034', '0.7359', '0.5874', '0.7255', '0.7460', '0.8643', '0.9246', '0.7709', '0.7583', '0.8911', '0.9255']
[2025-11-20 03:14:59 internimage_b_mimic_cxr_224](main.py 283): INFO AUC-ROC of the network on the 2991 val images: 0.8084
Using EMA with decay = 0.99990000
[2025-11-20 03:14:59 internimage_b_mimic_cxr_224](utils.py 24): INFO ==============> Resuming form output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_11.pth....................
[2025-11-20 03:15:01 internimage_b_mimic_cxr_224](utils.py 44): INFO <All keys matched successfully>
[2025-11-20 03:15:01 internimage_b_mimic_cxr_224](utils.py 45): INFO Loaded state_dict_ema
[2025-11-20 03:15:10 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 9.285 (9.285)	Loss 0.2741 (0.2741)	Mem 4564MB
[2025-11-20 03:15:22 internimage_b_mimic_cxr_224](utils_multilabel.py 141): INFO  * Mean AUC-ROC 0.8028 Loss 0.2347
[2025-11-20 03:15:22 internimage_b_mimic_cxr_224](utils_multilabel.py 142): INFO  * Per-class AUC-ROC: ['0.8358', '0.8261', '0.8153', '0.9029', '0.7342', '0.5947', '0.7147', '0.7412', '0.8607', '0.9218', '0.7481', '0.7535', '0.8694', '0.9204']
[2025-11-20 03:15:22 internimage_b_mimic_cxr_224](main.py 315): INFO AUC-ROC of the ema network on the 2991 val images: 0.8028
[2025-11-20 03:15:22 internimage_b_mimic_cxr_224](main.py 331): INFO Start training
/scratch/smehta90/InternImage_MIMIC/classification/main.py:483: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
/scratch/smehta90/InternImage_MIMIC/classification/main.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
/home/smehta90/.conda/envs/INTERNIMAGE_ENV/lib/python3.12/site-packages/torch/autograd/graph.py:841: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [1344, 896, 1, 1], strides() = [896, 1, 896, 896]
bucket_view.sizes() = [1344, 896, 1, 1], strides() = [896, 1, 1, 1] (Triggered internally at /pytorch/torch/csrc/distributed/c10d/reducer.cpp:334.)
  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[2025-11-20 03:16:10 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][0/2882]	eta 1 day, 14:35:27 lr 0.000001	time 48.2051 (48.2051)	model_time 17.6434 (17.6434)	loss 0.2524 (0.2524)	grad_norm 0.9190 (0.9190/0.0000)	mem 34249MB
[2025-11-20 03:17:38 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][50/2882]	eta 2:06:07 lr 0.000001	time 0.6228 (2.6720)	model_time 0.6227 (0.9546)	loss 0.2334 (0.2341)	grad_norm 0.7169 (0.9296/0.2209)	mem 34629MB
[2025-11-20 03:19:21 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][100/2882]	eta 1:49:52 lr 0.000001	time 0.6221 (2.3696)	model_time 0.6219 (0.7891)	loss 0.2297 (0.2338)	grad_norm 0.6383 (0.9646/0.2569)	mem 34629MB
[2025-11-20 03:21:06 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][150/2882]	eta 1:43:33 lr 0.000001	time 0.6246 (2.2744)	model_time 0.6244 (0.7337)	loss 0.2290 (0.2330)	grad_norm 1.1682 (0.9360/0.2547)	mem 34629MB
[2025-11-20 03:22:39 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][200/2882]	eta 1:37:10 lr 0.000001	time 0.6225 (2.1740)	model_time 0.6223 (0.7065)	loss 0.2393 (0.2335)	grad_norm 0.8256 (0.9301/0.2514)	mem 34629MB
[2025-11-20 03:24:14 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][250/2882]	eta 1:32:59 lr 0.000001	time 0.6204 (2.1200)	model_time 0.6202 (0.6891)	loss 0.2576 (0.2334)	grad_norm 0.7995 (0.9329/0.2541)	mem 34629MB
[2025-11-20 03:26:09 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][300/2882]	eta 1:32:26 lr 0.000001	time 12.4413 (2.1481)	model_time 0.6220 (0.6777)	loss 0.2871 (0.2338)	grad_norm 1.0811 (0.9282/0.2528)	mem 34629MB
[2025-11-20 03:27:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][350/2882]	eta 1:29:52 lr 0.000001	time 0.6211 (2.1298)	model_time 0.6210 (0.6699)	loss 0.2533 (0.2334)	grad_norm 0.9278 (0.9349/0.2553)	mem 34629MB
[2025-11-20 03:29:25 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][400/2882]	eta 1:26:58 lr 0.000001	time 0.6201 (2.1026)	model_time 0.6199 (0.6638)	loss 0.2346 (0.2334)	grad_norm 0.7233 (0.9157/0.2446)	mem 34629MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg'
[2025-11-20 03:31:13 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][450/2882]	eta 1:25:27 lr 0.000001	time 0.6216 (2.1084)	model_time 0.6214 (0.6593)	loss 0.2399 (0.2333)	grad_norm 0.9747 (0.9306/0.2479)	mem 34629MB
[2025-11-20 03:33:21 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][500/2882]	eta 1:25:29 lr 0.000001	time 0.6211 (2.1536)	model_time 0.6209 (0.6553)	loss 0.2200 (0.2334)	grad_norm 0.9825 (0.9344/0.2473)	mem 34629MB
[2025-11-20 03:35:30 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][550/2882]	eta 1:25:10 lr 0.000001	time 0.6177 (2.1915)	model_time 0.6175 (0.6522)	loss 0.2547 (0.2337)	grad_norm 1.1245 (0.9430/0.2476)	mem 34629MB
[2025-11-20 03:37:41 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][600/2882]	eta 1:24:44 lr 0.000001	time 10.1251 (2.2280)	model_time 0.6196 (0.6495)	loss 0.2328 (0.2335)	grad_norm 1.3801 (0.9442/0.2477)	mem 34629MB
[2025-11-20 03:39:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][650/2882]	eta 1:23:56 lr 0.000001	time 0.6199 (2.2567)	model_time 0.6197 (0.6472)	loss 0.2502 (0.2335)	grad_norm 1.0064 (0.9383/0.2550)	mem 34629MB
[2025-11-20 03:41:55 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][700/2882]	eta 1:22:37 lr 0.000001	time 0.6233 (2.2719)	model_time 0.6231 (0.6453)	loss 0.2291 (0.2332)	grad_norm 0.7688 (0.9314/0.2540)	mem 34629MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg'
[2025-11-20 03:43:48 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][750/2882]	eta 1:20:43 lr 0.000001	time 0.6208 (2.2720)	model_time 0.6207 (0.6437)	loss 0.2274 (0.2333)	grad_norm 1.3690 (0.9304/0.2576)	mem 34629MB
[2025-11-20 03:45:49 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][800/2882]	eta 1:19:08 lr 0.000001	time 0.6184 (2.2809)	model_time 0.6182 (0.6422)	loss 0.2472 (0.2334)	grad_norm 1.4160 (0.9319/0.2582)	mem 34629MB
[2025-11-20 03:47:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][850/2882]	eta 1:17:30 lr 0.000001	time 0.6200 (2.2885)	model_time 0.6198 (0.6409)	loss 0.2354 (0.2334)	grad_norm 0.6260 (0.9223/0.2549)	mem 34629MB
[2025-11-20 03:49:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][900/2882]	eta 1:15:51 lr 0.000001	time 5.2950 (2.2965)	model_time 0.6204 (0.6398)	loss 0.2363 (0.2332)	grad_norm 0.7812 (0.9210/0.2552)	mem 34629MB
[2025-11-20 03:51:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][950/2882]	eta 1:14:06 lr 0.000001	time 0.6208 (2.3016)	model_time 0.6207 (0.6388)	loss 0.2494 (0.2330)	grad_norm 0.7560 (0.9177/0.2476)	mem 34629MB
[2025-11-20 03:53:59 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1000/2882]	eta 1:12:35 lr 0.000001	time 0.8100 (2.3145)	model_time 0.6211 (0.6380)	loss 0.2343 (0.2330)	grad_norm 0.9036 (0.9269/0.2444)	mem 34629MB
[2025-11-20 03:55:55 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1050/2882]	eta 1:10:40 lr 0.000001	time 0.6183 (2.3148)	model_time 0.6181 (0.6371)	loss 0.2427 (0.2330)	grad_norm 0.9208 (0.9299/0.2378)	mem 34629MB
[2025-11-20 03:58:00 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1100/2882]	eta 1:08:59 lr 0.000001	time 0.6179 (2.3230)	model_time 0.6177 (0.6364)	loss 0.2075 (0.2330)	grad_norm 1.1533 (0.9232/0.2401)	mem 34629MB
[2025-11-20 03:59:57 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1150/2882]	eta 1:07:04 lr 0.000001	time 0.6220 (2.3238)	model_time 0.6219 (0.6357)	loss 0.2425 (0.2330)	grad_norm 1.0651 (0.9202/0.2352)	mem 34629MB
[2025-11-20 04:01:53 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1200/2882]	eta 1:05:08 lr 0.000001	time 0.6231 (2.3239)	model_time 0.6230 (0.6351)	loss 0.2429 (0.2330)	grad_norm 0.9323 (0.9281/0.2353)	mem 34629MB
[2025-11-20 04:03:53 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1250/2882]	eta 1:03:17 lr 0.000001	time 0.6272 (2.3268)	model_time 0.6270 (0.6346)	loss 0.2429 (0.2329)	grad_norm 0.9973 (0.9383/0.2332)	mem 34629MB
[2025-11-20 04:06:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1300/2882]	eta 1:01:40 lr 0.000001	time 10.2603 (2.3389)	model_time 0.6247 (0.6343)	loss 0.2226 (0.2330)	grad_norm 1.0361 (0.9385/0.2376)	mem 34629MB
[2025-11-20 04:08:00 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1350/2882]	eta 0:59:41 lr 0.000001	time 0.6206 (2.3376)	model_time 0.6205 (0.6339)	loss 0.2287 (0.2329)	grad_norm 0.7242 (0.9276/0.2398)	mem 34629MB
[2025-11-20 04:10:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1400/2882]	eta 0:57:51 lr 0.000001	time 0.6220 (2.3426)	model_time 0.6218 (0.6334)	loss 0.2403 (0.2328)	grad_norm 0.7692 (0.9283/0.2369)	mem 34629MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg'
[2025-11-20 04:12:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1450/2882]	eta 0:55:57 lr 0.000001	time 0.6218 (2.3449)	model_time 0.6217 (0.6331)	loss 0.2380 (0.2328)	grad_norm 0.7690 (0.9303/0.2383)	mem 34629MB
[2025-11-20 04:14:03 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1500/2882]	eta 0:54:02 lr 0.000001	time 0.6181 (2.3460)	model_time 0.6180 (0.6326)	loss 0.2371 (0.2328)	grad_norm 1.0479 (0.9171/0.2407)	mem 34629MB
[2025-11-20 04:16:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1550/2882]	eta 0:52:08 lr 0.000001	time 0.6217 (2.3487)	model_time 0.6216 (0.6323)	loss 0.2041 (0.2326)	grad_norm 0.9847 (0.9111/0.2376)	mem 34629MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg'
[2025-11-20 04:18:17 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1600/2882]	eta 0:50:22 lr 0.000001	time 15.4950 (2.3578)	model_time 0.6206 (0.6319)	loss 0.2188 (0.2327)	grad_norm 0.5494 (0.9094/0.2390)	mem 34629MB
[2025-11-20 04:20:19 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1650/2882]	eta 0:48:27 lr 0.000001	time 0.6207 (2.3604)	model_time 0.6205 (0.6316)	loss 0.2511 (0.2326)	grad_norm 1.0820 (0.9110/0.2424)	mem 34629MB
[2025-11-20 04:22:12 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1700/2882]	eta 0:46:26 lr 0.000001	time 0.6199 (2.3573)	model_time 0.6197 (0.6313)	loss 0.2087 (0.2325)	grad_norm 0.9165 (0.9052/0.2410)	mem 34629MB
[2025-11-20 04:24:13 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1750/2882]	eta 0:44:30 lr 0.000001	time 0.6221 (2.3593)	model_time 0.6219 (0.6310)	loss 0.2094 (0.2324)	grad_norm 0.6641 (0.9035/0.2443)	mem 34629MB
[2025-11-20 04:26:11 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1800/2882]	eta 0:42:32 lr 0.000001	time 0.6199 (2.3590)	model_time 0.6197 (0.6307)	loss 0.2469 (0.2323)	grad_norm 1.1396 (0.9213/0.2472)	mem 34629MB
[2025-11-20 04:28:18 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1850/2882]	eta 0:40:39 lr 0.000001	time 0.6184 (2.3641)	model_time 0.6182 (0.6304)	loss 0.2330 (0.2323)	grad_norm 1.1953 (0.9215/0.2606)	mem 34629MB
[2025-11-20 04:30:21 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1900/2882]	eta 0:38:43 lr 0.000001	time 11.2122 (2.3664)	model_time 0.6200 (0.6301)	loss 0.2149 (0.2323)	grad_norm 1.0492 (0.9268/0.2604)	mem 34629MB
[2025-11-20 04:32:11 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][1950/2882]	eta 0:36:41 lr 0.000001	time 0.6184 (2.3622)	model_time 0.6183 (0.6300)	loss 0.2513 (0.2323)	grad_norm 0.9416 (0.9222/0.2513)	mem 34629MB
[2025-11-20 04:34:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2000/2882]	eta 0:34:41 lr 0.000001	time 0.6225 (2.3595)	model_time 0.6224 (0.6297)	loss 0.2364 (0.2322)	grad_norm 1.1721 (0.9399/0.2639)	mem 34629MB
[2025-11-20 04:36:03 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2050/2882]	eta 0:32:43 lr 0.000001	time 0.6186 (2.3602)	model_time 0.6185 (0.6295)	loss 0.2427 (0.2322)	grad_norm 0.6526 (0.9373/0.2651)	mem 34629MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg'
[2025-11-20 04:38:03 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2100/2882]	eta 0:30:46 lr 0.000001	time 2.3362 (2.3614)	model_time 0.6250 (0.6293)	loss 0.2673 (0.2322)	grad_norm 1.2872 (0.9274/0.2581)	mem 34629MB
[2025-11-20 04:40:07 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2150/2882]	eta 0:28:50 lr 0.000001	time 6.8847 (2.3639)	model_time 0.6258 (0.6293)	loss 0.2691 (0.2322)	grad_norm 1.1826 (0.9203/0.2535)	mem 34629MB
[2025-11-20 04:42:19 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2200/2882]	eta 0:26:56 lr 0.000001	time 15.5622 (2.3701)	model_time 0.6199 (0.6291)	loss 0.2454 (0.2322)	grad_norm 0.8471 (0.9263/0.2510)	mem 34629MB
[2025-11-20 04:44:34 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2250/2882]	eta 0:25:02 lr 0.000001	time 0.6212 (2.3777)	model_time 0.6210 (0.6289)	loss 0.2389 (0.2321)	grad_norm 1.2843 (0.9290/0.2570)	mem 34629MB
[2025-11-20 04:46:30 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2300/2882]	eta 0:23:03 lr 0.000001	time 0.6190 (2.3765)	model_time 0.6189 (0.6288)	loss 0.2310 (0.2320)	grad_norm 0.6724 (0.9206/0.2464)	mem 34629MB
[2025-11-20 04:48:28 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2350/2882]	eta 0:21:03 lr 0.000001	time 0.6230 (2.3759)	model_time 0.6228 (0.6287)	loss 0.2206 (0.2321)	grad_norm 0.8706 (0.9322/0.2491)	mem 34629MB
[2025-11-20 04:50:24 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2400/2882]	eta 0:19:04 lr 0.000001	time 0.6217 (2.3750)	model_time 0.6215 (0.6285)	loss 0.2140 (0.2320)	grad_norm 0.5406 (0.9268/0.2515)	mem 34629MB
[2025-11-20 04:52:15 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2450/2882]	eta 0:17:04 lr 0.000001	time 0.6215 (2.3718)	model_time 0.6214 (0.6284)	loss 0.2295 (0.2320)	grad_norm 0.8724 (0.9379/0.2497)	mem 34629MB
[2025-11-20 04:54:30 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2500/2882]	eta 0:15:08 lr 0.000001	time 14.9718 (2.3782)	model_time 0.6247 (0.6283)	loss 0.2393 (0.2320)	grad_norm 0.6671 (0.9339/0.2588)	mem 34629MB
[2025-11-20 04:56:39 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2550/2882]	eta 0:13:10 lr 0.000001	time 0.6221 (2.3820)	model_time 0.6219 (0.6282)	loss 0.2496 (0.2321)	grad_norm 0.8827 (0.9364/0.2555)	mem 34629MB
[2025-11-20 04:58:45 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2600/2882]	eta 0:11:12 lr 0.000001	time 0.6183 (2.3849)	model_time 0.6181 (0.6280)	loss 0.2668 (0.2320)	grad_norm 0.8927 (0.9330/0.2617)	mem 34629MB
[2025-11-20 05:00:41 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2650/2882]	eta 0:09:12 lr 0.000001	time 0.6233 (2.3835)	model_time 0.6231 (0.6279)	loss 0.2118 (0.2321)	grad_norm 0.7869 (0.9268/0.2664)	mem 34629MB
[2025-11-20 05:02:40 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2700/2882]	eta 0:07:13 lr 0.000001	time 0.6215 (2.3836)	model_time 0.6213 (0.6278)	loss 0.2216 (0.2320)	grad_norm 0.6777 (0.9352/0.2705)	mem 34629MB
[2025-11-20 05:04:35 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2750/2882]	eta 0:05:14 lr 0.000001	time 0.6210 (2.3820)	model_time 0.6208 (0.6277)	loss 0.2474 (0.2320)	grad_norm 0.9529 (0.9310/0.2744)	mem 34629MB
[2025-11-20 05:06:56 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2800/2882]	eta 0:03:15 lr 0.000001	time 19.7423 (2.3900)	model_time 0.6214 (0.6275)	loss 0.2292 (0.2320)	grad_norm 1.4810 (0.9299/0.2711)	mem 34629MB
[2025-11-20 05:09:01 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [12/20][2850/2882]	eta 0:01:16 lr 0.000001	time 0.6256 (2.3917)	model_time 0.6254 (0.6274)	loss 0.2387 (0.2320)	grad_norm 0.6889 (0.9316/0.2735)	mem 34629MB
[2025-11-20 05:10:08 internimage_b_mimic_cxr_224](main.py 583): INFO EPOCH 12 training takes 1:54:45
[2025-11-20 05:10:08 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_12.pth saving......
[2025-11-20 05:10:10 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_12.pth saved !!!
[2025-11-20 05:10:39 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 29.881 (29.881)	Loss 0.2701 (0.2701)	Mem 34629MB
[2025-11-20 05:11:18 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:12] * Mean AUC-ROC 0.8099 Loss 0.2302
[2025-11-20 05:11:18 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:12] * Per-class AUC-ROC: ['0.8402', '0.8301', '0.8182', '0.9043', '0.7356', '0.5996', '0.7352', '0.7471', '0.8654', '0.9235', '0.7692', '0.7572', '0.8864', '0.9262']
[2025-11-20 05:11:18 internimage_b_mimic_cxr_224](main.py 365): INFO AUC-ROC of the network on the 2991 val images: 0.8099
[2025-11-20 05:11:18 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_best.pth saving......
[2025-11-20 05:11:20 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_best.pth saved !!!
[2025-11-20 05:11:20 internimage_b_mimic_cxr_224](main.py 380): INFO Max AUC-ROC: 0.8099
[2025-11-20 05:11:28 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 8.757 (8.757)	Loss 0.2738 (0.2738)	Mem 34629MB
[2025-11-20 05:11:41 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:12] * Mean AUC-ROC 0.8047 Loss 0.2334
[2025-11-20 05:11:41 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:12] * Per-class AUC-ROC: ['0.8372', '0.8273', '0.8164', '0.9035', '0.7354', '0.5951', '0.7178', '0.7426', '0.8619', '0.9226', '0.7526', '0.7556', '0.8754', '0.9219']
[2025-11-20 05:11:41 internimage_b_mimic_cxr_224](main.py 385): INFO AUC-ROC of the ema network on the 2991 val images: 0.8047
[2025-11-20 05:11:41 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saving......
[2025-11-20 05:11:42 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saved !!!
[2025-11-20 05:11:42 internimage_b_mimic_cxr_224](main.py 400): INFO Max ema AUC-ROC: 0.8047
/scratch/smehta90/InternImage_MIMIC/classification/main.py:483: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
/scratch/smehta90/InternImage_MIMIC/classification/main.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
[2025-11-20 05:12:10 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][0/2882]	eta 21:51:37 lr 0.000001	time 27.3065 (27.3065)	model_time 0.6264 (0.6264)	loss 0.2409 (0.2409)	grad_norm 1.4608 (1.4608/0.0000)	mem 34629MB
[2025-11-20 05:13:54 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][50/2882]	eta 2:02:10 lr 0.000001	time 4.0077 (2.5884)	model_time 0.6205 (0.6210)	loss 0.2481 (0.2291)	grad_norm 1.3000 (0.8988/0.2489)	mem 34629MB
[2025-11-20 05:15:35 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][100/2882]	eta 1:46:45 lr 0.000001	time 0.6210 (2.3025)	model_time 0.6208 (0.6230)	loss 0.2270 (0.2313)	grad_norm 0.9910 (0.9265/0.2503)	mem 34630MB
[2025-11-20 05:17:19 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][150/2882]	eta 1:41:38 lr 0.000001	time 0.6245 (2.2321)	model_time 0.6242 (0.6227)	loss 0.2229 (0.2291)	grad_norm 1.0748 (0.9122/0.2265)	mem 34630MB
[2025-11-20 05:19:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][200/2882]	eta 1:38:18 lr 0.000001	time 0.6269 (2.1993)	model_time 0.6267 (0.6227)	loss 0.2228 (0.2292)	grad_norm 0.8745 (0.9208/0.2370)	mem 34630MB
[2025-11-20 05:20:54 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][250/2882]	eta 1:36:23 lr 0.000001	time 0.6247 (2.1974)	model_time 0.6245 (0.6230)	loss 0.2367 (0.2297)	grad_norm 0.8893 (0.9244/0.2471)	mem 34630MB
[2025-11-20 05:22:47 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][300/2882]	eta 1:35:02 lr 0.000001	time 4.9370 (2.2086)	model_time 0.6190 (0.6228)	loss 0.2226 (0.2301)	grad_norm 1.2190 (0.9245/0.2458)	mem 34630MB
[2025-11-20 05:24:49 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][350/2882]	eta 1:34:36 lr 0.000001	time 10.4086 (2.2418)	model_time 0.6203 (0.6225)	loss 0.2426 (0.2302)	grad_norm 1.0204 (0.9307/0.2492)	mem 34630MB
[2025-11-20 05:26:41 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][400/2882]	eta 1:32:41 lr 0.000001	time 0.6206 (2.2409)	model_time 0.6204 (0.6224)	loss 0.2202 (0.2303)	grad_norm 0.8211 (0.9219/0.2420)	mem 34630MB
[2025-11-20 05:28:38 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][450/2882]	eta 1:31:16 lr 0.000001	time 0.6206 (2.2520)	model_time 0.6204 (0.6221)	loss 0.2322 (0.2301)	grad_norm 0.9777 (0.9317/0.2514)	mem 34630MB
[2025-11-20 05:30:27 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][500/2882]	eta 1:29:04 lr 0.000001	time 0.6269 (2.2438)	model_time 0.6266 (0.6221)	loss 0.2395 (0.2307)	grad_norm 0.8323 (0.9353/0.2529)	mem 34630MB
[2025-11-20 05:32:13 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][550/2882]	eta 1:26:49 lr 0.000001	time 0.6186 (2.2340)	model_time 0.6184 (0.6223)	loss 0.2371 (0.2307)	grad_norm 1.2779 (0.9335/0.2432)	mem 34630MB
[2025-11-20 05:34:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][600/2882]	eta 1:24:57 lr 0.000001	time 6.6586 (2.2339)	model_time 0.6212 (0.6221)	loss 0.2457 (0.2307)	grad_norm 0.8813 (0.9323/0.2476)	mem 34630MB
[2025-11-20 05:36:02 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][650/2882]	eta 1:23:23 lr 0.000001	time 6.5711 (2.2419)	model_time 0.6189 (0.6219)	loss 0.1981 (0.2311)	grad_norm 0.9315 (0.9445/0.2460)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg'
[2025-11-20 05:37:59 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][700/2882]	eta 1:21:48 lr 0.000001	time 0.6209 (2.2493)	model_time 0.6207 (0.6220)	loss 0.2077 (0.2310)	grad_norm 1.0057 (0.9482/0.2496)	mem 34630MB
[2025-11-20 05:39:57 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][750/2882]	eta 1:20:11 lr 0.000001	time 0.6186 (2.2569)	model_time 0.6185 (0.6219)	loss 0.2354 (0.2313)	grad_norm 0.8753 (0.9539/0.2656)	mem 34630MB
[2025-11-20 05:41:49 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][800/2882]	eta 1:18:15 lr 0.000001	time 0.6207 (2.2554)	model_time 0.6205 (0.6218)	loss 0.2412 (0.2313)	grad_norm 1.2353 (0.9461/0.2665)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg'
[2025-11-20 05:43:41 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][850/2882]	eta 1:16:22 lr 0.000001	time 0.6208 (2.2552)	model_time 0.6206 (0.6219)	loss 0.2366 (0.2313)	grad_norm 0.9350 (0.9456/0.2682)	mem 34630MB
[2025-11-20 05:45:47 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][900/2882]	eta 1:14:56 lr 0.000001	time 6.8369 (2.2688)	model_time 0.6211 (0.6218)	loss 0.2413 (0.2313)	grad_norm 0.7268 (0.9547/0.2692)	mem 34630MB
[2025-11-20 05:48:02 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][950/2882]	eta 1:13:47 lr 0.000001	time 12.5542 (2.2915)	model_time 0.6208 (0.6216)	loss 0.2181 (0.2313)	grad_norm 0.8743 (0.9498/0.2684)	mem 34630MB
[2025-11-20 05:49:57 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1000/2882]	eta 1:11:54 lr 0.000001	time 0.6194 (2.2928)	model_time 0.6192 (0.6217)	loss 0.2222 (0.2312)	grad_norm 1.3415 (0.9548/0.2687)	mem 34630MB
[2025-11-20 05:51:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1050/2882]	eta 1:09:56 lr 0.000001	time 0.6225 (2.2905)	model_time 0.6223 (0.6216)	loss 0.2539 (0.2313)	grad_norm 0.9686 (0.9397/0.2486)	mem 34630MB
[2025-11-20 05:53:48 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1100/2882]	eta 1:08:08 lr 0.000001	time 0.6181 (2.2943)	model_time 0.6180 (0.6216)	loss 0.2700 (0.2313)	grad_norm 1.4939 (0.9481/0.2394)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg'
[2025-11-20 05:55:42 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1150/2882]	eta 1:06:12 lr 0.000001	time 0.6218 (2.2934)	model_time 0.6216 (0.6215)	loss 0.2486 (0.2313)	grad_norm 0.8374 (0.9565/0.2416)	mem 34630MB
[2025-11-20 05:57:45 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1200/2882]	eta 1:04:28 lr 0.000001	time 0.6211 (2.3002)	model_time 0.6209 (0.6215)	loss 0.2517 (0.2313)	grad_norm 1.2417 (0.9563/0.2466)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg'
[2025-11-20 06:00:03 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1250/2882]	eta 1:03:04 lr 0.000001	time 2.0404 (2.3189)	model_time 0.6239 (0.6216)	loss 0.2437 (0.2313)	grad_norm 0.7681 (0.9598/0.2532)	mem 34630MB
[2025-11-20 06:02:03 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1300/2882]	eta 1:01:12 lr 0.000001	time 0.6242 (2.3216)	model_time 0.6239 (0.6216)	loss 0.2452 (0.2314)	grad_norm 0.9051 (0.9549/0.2559)	mem 34630MB
[2025-11-20 06:04:01 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1350/2882]	eta 0:59:18 lr 0.000001	time 0.6213 (2.3230)	model_time 0.6211 (0.6216)	loss 0.2370 (0.2313)	grad_norm 0.8572 (0.9645/0.2631)	mem 34630MB
[2025-11-20 06:05:56 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1400/2882]	eta 0:57:22 lr 0.000001	time 5.9530 (2.3226)	model_time 0.6198 (0.6216)	loss 0.2387 (0.2314)	grad_norm 0.5211 (0.9592/0.2719)	mem 34630MB
[2025-11-20 06:07:57 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1450/2882]	eta 0:55:30 lr 0.000001	time 0.6216 (2.3255)	model_time 0.6214 (0.6216)	loss 0.1996 (0.2312)	grad_norm 0.9201 (0.9516/0.2774)	mem 34630MB
[2025-11-20 06:09:54 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1500/2882]	eta 0:53:34 lr 0.000001	time 0.6189 (2.3262)	model_time 0.6188 (0.6216)	loss 0.2285 (0.2313)	grad_norm 0.9280 (0.9401/0.2619)	mem 34630MB
[2025-11-20 06:12:09 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1550/2882]	eta 0:51:54 lr 0.000001	time 0.6198 (2.3381)	model_time 0.6196 (0.6215)	loss 0.2287 (0.2313)	grad_norm 0.8165 (0.9242/0.2490)	mem 34630MB
[2025-11-20 06:14:02 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1600/2882]	eta 0:49:54 lr 0.000001	time 0.6203 (2.3358)	model_time 0.6201 (0.6215)	loss 0.2338 (0.2312)	grad_norm 0.8858 (0.9307/0.2494)	mem 34630MB
[2025-11-20 06:16:01 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1650/2882]	eta 0:47:59 lr 0.000001	time 0.6212 (2.3375)	model_time 0.6210 (0.6215)	loss 0.2125 (0.2312)	grad_norm 1.0883 (0.9282/0.2414)	mem 34630MB
[2025-11-20 06:18:11 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1700/2882]	eta 0:46:11 lr 0.000001	time 11.8143 (2.3448)	model_time 0.6223 (0.6214)	loss 0.2546 (0.2313)	grad_norm 1.2994 (0.9317/0.2404)	mem 34630MB
[2025-11-20 06:20:12 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1750/2882]	eta 0:44:16 lr 0.000001	time 0.6201 (2.3468)	model_time 0.6199 (0.6214)	loss 0.2151 (0.2313)	grad_norm 1.0896 (0.9353/0.2269)	mem 34630MB
[2025-11-20 06:22:13 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1800/2882]	eta 0:42:21 lr 0.000001	time 0.6217 (2.3489)	model_time 0.6215 (0.6214)	loss 0.2339 (0.2313)	grad_norm 0.6950 (0.9443/0.2377)	mem 34630MB
[2025-11-20 06:24:31 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1850/2882]	eta 0:40:35 lr 0.000001	time 1.3079 (2.3601)	model_time 0.6215 (0.6214)	loss 0.2217 (0.2314)	grad_norm 1.0594 (0.9500/0.2492)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg'
[2025-11-20 06:26:39 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1900/2882]	eta 0:38:42 lr 0.000001	time 0.6220 (2.3655)	model_time 0.6217 (0.6214)	loss 0.2373 (0.2313)	grad_norm 0.8021 (0.9414/0.2428)	mem 34630MB
[2025-11-20 06:28:49 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][1950/2882]	eta 0:36:50 lr 0.000001	time 0.6249 (2.3713)	model_time 0.6247 (0.6215)	loss 0.2143 (0.2313)	grad_norm 1.0189 (0.9422/0.2373)	mem 34630MB
[2025-11-20 06:30:57 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2000/2882]	eta 0:34:55 lr 0.000001	time 12.4462 (2.3759)	model_time 0.6250 (0.6216)	loss 0.2385 (0.2313)	grad_norm 1.0273 (0.9395/0.2299)	mem 34630MB
[2025-11-20 06:33:00 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2050/2882]	eta 0:32:58 lr 0.000001	time 0.6232 (2.3782)	model_time 0.6230 (0.6217)	loss 0.2428 (0.2313)	grad_norm 0.7450 (0.9402/0.2399)	mem 34630MB
[2025-11-20 06:35:02 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2100/2882]	eta 0:31:00 lr 0.000001	time 0.6229 (2.3796)	model_time 0.6227 (0.6218)	loss 0.2122 (0.2314)	grad_norm 1.0364 (0.9372/0.2414)	mem 34630MB
[2025-11-20 06:37:07 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2150/2882]	eta 0:29:03 lr 0.000001	time 3.1571 (2.3822)	model_time 0.6251 (0.6218)	loss 0.2335 (0.2314)	grad_norm 1.3122 (0.9455/0.2447)	mem 34630MB
[2025-11-20 06:39:00 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2200/2882]	eta 0:27:03 lr 0.000001	time 0.6210 (2.3798)	model_time 0.6208 (0.6219)	loss 0.2528 (0.2315)	grad_norm 0.7136 (0.9600/0.2661)	mem 34630MB
[2025-11-20 06:40:53 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2250/2882]	eta 0:25:02 lr 0.000001	time 0.6188 (2.3770)	model_time 0.6186 (0.6219)	loss 0.2059 (0.2315)	grad_norm 0.6523 (0.9500/0.2830)	mem 34630MB
[2025-11-20 06:43:03 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2300/2882]	eta 0:23:06 lr 0.000001	time 16.7166 (2.3820)	model_time 0.6220 (0.6218)	loss 0.2228 (0.2315)	grad_norm 0.6402 (0.9454/0.2853)	mem 34630MB
[2025-11-20 06:44:54 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2350/2882]	eta 0:21:05 lr 0.000001	time 0.6226 (2.3783)	model_time 0.6224 (0.6218)	loss 0.2244 (0.2315)	grad_norm 0.9462 (0.9474/0.2856)	mem 34630MB
[2025-11-20 06:46:54 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2400/2882]	eta 0:19:06 lr 0.000001	time 3.5174 (2.3789)	model_time 0.6246 (0.6218)	loss 0.2289 (0.2315)	grad_norm 0.7837 (0.9532/0.2821)	mem 34630MB
[2025-11-20 06:48:54 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2450/2882]	eta 0:17:07 lr 0.000001	time 0.6242 (2.3792)	model_time 0.6240 (0.6218)	loss 0.2456 (0.2315)	grad_norm 0.8351 (0.9477/0.2702)	mem 34630MB
[2025-11-20 06:50:54 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2500/2882]	eta 0:15:08 lr 0.000001	time 0.6199 (2.3795)	model_time 0.6198 (0.6218)	loss 0.2159 (0.2315)	grad_norm 0.7457 (0.9517/0.2604)	mem 34630MB
[2025-11-20 06:52:45 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2550/2882]	eta 0:13:09 lr 0.000001	time 0.6198 (2.3766)	model_time 0.6196 (0.6218)	loss 0.2483 (0.2315)	grad_norm 0.9624 (0.9573/0.2536)	mem 34630MB
[2025-11-20 06:54:56 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2600/2882]	eta 0:11:11 lr 0.000001	time 16.3015 (2.3814)	model_time 0.6198 (0.6218)	loss 0.2197 (0.2315)	grad_norm 0.6184 (0.9606/0.2534)	mem 34630MB
[2025-11-20 06:56:59 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2650/2882]	eta 0:09:12 lr 0.000001	time 0.6221 (2.3828)	model_time 0.6220 (0.6218)	loss 0.2343 (0.2314)	grad_norm 0.5338 (0.9604/0.2505)	mem 34630MB
[2025-11-20 06:59:03 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2700/2882]	eta 0:07:14 lr 0.000001	time 0.6214 (2.3847)	model_time 0.6212 (0.6218)	loss 0.2423 (0.2314)	grad_norm 0.6238 (0.9410/0.2511)	mem 34630MB
[2025-11-20 07:01:08 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2750/2882]	eta 0:05:15 lr 0.000001	time 0.6197 (2.3867)	model_time 0.6195 (0.6218)	loss 0.2069 (0.2314)	grad_norm 1.2029 (0.9419/0.2484)	mem 34630MB
[2025-11-20 07:03:13 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2800/2882]	eta 0:03:15 lr 0.000001	time 0.6224 (2.3886)	model_time 0.6222 (0.6218)	loss 0.2283 (0.2314)	grad_norm 0.8468 (0.9262/0.2468)	mem 34630MB
[2025-11-20 07:05:26 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [13/20][2850/2882]	eta 0:01:16 lr 0.000001	time 7.0258 (2.3935)	model_time 0.6254 (0.6219)	loss 0.2276 (0.2315)	grad_norm 1.2201 (0.9421/0.2553)	mem 34630MB
[2025-11-20 07:07:00 internimage_b_mimic_cxr_224](main.py 583): INFO EPOCH 13 training takes 1:55:17
[2025-11-20 07:07:00 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_13.pth saving......
[2025-11-20 07:07:02 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_13.pth saved !!!
[2025-11-20 07:07:36 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 33.841 (33.841)	Loss 0.2725 (0.2725)	Mem 34630MB
[2025-11-20 07:08:16 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:13] * Mean AUC-ROC 0.8085 Loss 0.2295
[2025-11-20 07:08:16 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:13] * Per-class AUC-ROC: ['0.8403', '0.8308', '0.8209', '0.9029', '0.7370', '0.5948', '0.7198', '0.7460', '0.8652', '0.9244', '0.7616', '0.7604', '0.8887', '0.9264']
[2025-11-20 07:08:16 internimage_b_mimic_cxr_224](main.py 365): INFO AUC-ROC of the network on the 2991 val images: 0.8085
[2025-11-20 07:08:16 internimage_b_mimic_cxr_224](main.py 380): INFO Max AUC-ROC: 0.8099
[2025-11-20 07:08:25 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 9.011 (9.011)	Loss 0.2732 (0.2732)	Mem 34630MB
[2025-11-20 07:08:37 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:13] * Mean AUC-ROC 0.8061 Loss 0.2324
[2025-11-20 07:08:37 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:13] * Per-class AUC-ROC: ['0.8383', '0.8281', '0.8170', '0.9036', '0.7355', '0.5962', '0.7204', '0.7438', '0.8629', '0.9231', '0.7559', '0.7572', '0.8799', '0.9232']
[2025-11-20 07:08:37 internimage_b_mimic_cxr_224](main.py 385): INFO AUC-ROC of the ema network on the 2991 val images: 0.8061
[2025-11-20 07:08:37 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saving......
[2025-11-20 07:08:40 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saved !!!
[2025-11-20 07:08:40 internimage_b_mimic_cxr_224](main.py 400): INFO Max ema AUC-ROC: 0.8061
/scratch/smehta90/InternImage_MIMIC/classification/main.py:483: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
/scratch/smehta90/InternImage_MIMIC/classification/main.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
[2025-11-20 07:09:09 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][0/2882]	eta 23:39:56 lr 0.000001	time 29.5616 (29.5616)	model_time 0.6223 (0.6223)	loss 0.2335 (0.2335)	grad_norm 0.6711 (0.6711/0.0000)	mem 34630MB
[2025-11-20 07:11:12 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][50/2882]	eta 2:21:11 lr 0.000001	time 0.6182 (2.9913)	model_time 0.6181 (0.6227)	loss 0.2105 (0.2290)	grad_norm 0.9904 (0.9216/0.2599)	mem 34630MB
[2025-11-20 07:13:06 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][100/2882]	eta 2:02:25 lr 0.000001	time 0.6205 (2.6404)	model_time 0.6203 (0.6207)	loss 0.2302 (0.2303)	grad_norm 0.9377 (0.9657/0.2606)	mem 34630MB
[2025-11-20 07:15:09 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][150/2882]	eta 1:57:30 lr 0.000001	time 0.6180 (2.5808)	model_time 0.6178 (0.6202)	loss 0.2327 (0.2306)	grad_norm 0.8278 (0.9616/0.2615)	mem 34630MB
[2025-11-20 07:17:06 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][200/2882]	eta 1:52:35 lr 0.000001	time 1.0803 (2.5189)	model_time 0.6211 (0.6209)	loss 0.2514 (0.2308)	grad_norm 1.4159 (0.9710/0.2691)	mem 34630MB
[2025-11-20 07:19:02 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][250/2882]	eta 1:48:50 lr 0.000001	time 0.6203 (2.4814)	model_time 0.6202 (0.6211)	loss 0.2541 (0.2304)	grad_norm 1.3158 (0.9665/0.2588)	mem 34630MB
[2025-11-20 07:21:09 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][300/2882]	eta 1:47:06 lr 0.000001	time 10.5422 (2.4889)	model_time 0.6243 (0.6214)	loss 0.2360 (0.2310)	grad_norm 1.0624 (0.9545/0.2500)	mem 34630MB
[2025-11-20 07:23:13 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][350/2882]	eta 1:45:00 lr 0.000001	time 1.4743 (2.4884)	model_time 0.6187 (0.6215)	loss 0.2449 (0.2309)	grad_norm 0.7579 (0.9623/0.2441)	mem 34630MB
[2025-11-20 07:25:13 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][400/2882]	eta 1:42:25 lr 0.000001	time 1.4207 (2.4760)	model_time 0.6208 (0.6214)	loss 0.2317 (0.2310)	grad_norm 1.0441 (0.9619/0.2492)	mem 34630MB
[2025-11-20 07:27:07 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][450/2882]	eta 1:39:32 lr 0.000001	time 0.6215 (2.4560)	model_time 0.6213 (0.6213)	loss 0.2252 (0.2311)	grad_norm 0.8563 (0.9544/0.2577)	mem 34630MB
[2025-11-20 07:29:00 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][500/2882]	eta 1:36:40 lr 0.000001	time 3.3790 (2.4352)	model_time 0.6197 (0.6212)	loss 0.2114 (0.2310)	grad_norm 0.6487 (0.9436/0.2489)	mem 34630MB
[2025-11-20 07:30:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][550/2882]	eta 1:33:54 lr 0.000001	time 0.6195 (2.4162)	model_time 0.6193 (0.6212)	loss 0.2153 (0.2310)	grad_norm 0.7306 (0.9424/0.2452)	mem 34630MB
[2025-11-20 07:32:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][600/2882]	eta 1:31:49 lr 0.000001	time 8.4733 (2.4145)	model_time 0.6218 (0.6212)	loss 0.2288 (0.2311)	grad_norm 0.8545 (0.9555/0.2484)	mem 34630MB
[2025-11-20 07:34:49 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][650/2882]	eta 1:29:41 lr 0.000001	time 2.4456 (2.4112)	model_time 0.6203 (0.6212)	loss 0.2101 (0.2312)	grad_norm 1.2105 (0.9440/0.2525)	mem 34630MB
[2025-11-20 07:36:56 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][700/2882]	eta 1:27:59 lr 0.000001	time 0.6214 (2.4195)	model_time 0.6212 (0.6213)	loss 0.2239 (0.2315)	grad_norm 1.3050 (0.9330/0.2401)	mem 34630MB
[2025-11-20 07:38:59 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][750/2882]	eta 1:26:04 lr 0.000001	time 0.6193 (2.4223)	model_time 0.6192 (0.6213)	loss 0.2053 (0.2312)	grad_norm 0.8049 (0.9413/0.2427)	mem 34630MB
[2025-11-20 07:40:55 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][800/2882]	eta 1:23:49 lr 0.000001	time 0.6188 (2.4158)	model_time 0.6187 (0.6213)	loss 0.2505 (0.2312)	grad_norm 1.5941 (0.9385/0.2405)	mem 34630MB
[2025-11-20 07:42:47 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][850/2882]	eta 1:21:29 lr 0.000001	time 0.6219 (2.4060)	model_time 0.6217 (0.6213)	loss 0.2338 (0.2310)	grad_norm 1.3743 (0.9335/0.2514)	mem 34630MB
[2025-11-20 07:44:53 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][900/2882]	eta 1:19:39 lr 0.000001	time 9.4263 (2.4116)	model_time 0.6196 (0.6214)	loss 0.2137 (0.2308)	grad_norm 0.6052 (0.9376/0.2603)	mem 34630MB
[2025-11-20 07:46:57 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][950/2882]	eta 1:17:46 lr 0.000001	time 7.7207 (2.4156)	model_time 0.6180 (0.6213)	loss 0.2288 (0.2306)	grad_norm 1.1479 (0.9463/0.2655)	mem 34630MB
[2025-11-20 07:48:52 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1000/2882]	eta 1:15:36 lr 0.000001	time 0.6201 (2.4103)	model_time 0.6200 (0.6212)	loss 0.2164 (0.2307)	grad_norm 0.9483 (0.9449/0.2702)	mem 34630MB
[2025-11-20 07:50:53 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1050/2882]	eta 1:13:36 lr 0.000001	time 0.6215 (2.4106)	model_time 0.6213 (0.6213)	loss 0.2071 (0.2309)	grad_norm 1.0228 (0.9378/0.2569)	mem 34630MB
[2025-11-20 07:52:53 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1100/2882]	eta 1:11:34 lr 0.000001	time 0.6192 (2.4099)	model_time 0.6191 (0.6212)	loss 0.2274 (0.2308)	grad_norm 1.1120 (0.9338/0.2546)	mem 34630MB
[2025-11-20 07:54:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1150/2882]	eta 1:09:29 lr 0.000001	time 0.6183 (2.4073)	model_time 0.6181 (0.6212)	loss 0.2540 (0.2308)	grad_norm 1.1520 (0.9404/0.2510)	mem 34630MB
[2025-11-20 07:56:43 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1200/2882]	eta 1:07:18 lr 0.000001	time 1.5070 (2.4011)	model_time 0.6206 (0.6212)	loss 0.2183 (0.2307)	grad_norm 0.7705 (0.9307/0.2449)	mem 34630MB
[2025-11-20 07:58:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1250/2882]	eta 1:05:28 lr 0.000001	time 15.0327 (2.4073)	model_time 0.6223 (0.6212)	loss 0.2036 (0.2307)	grad_norm 0.6915 (0.9280/0.2364)	mem 34630MB
[2025-11-20 08:00:52 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1300/2882]	eta 1:03:29 lr 0.000001	time 0.6190 (2.4078)	model_time 0.6189 (0.6212)	loss 0.2162 (0.2307)	grad_norm 0.9578 (0.9227/0.2405)	mem 34630MB
[2025-11-20 08:02:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1350/2882]	eta 1:01:25 lr 0.000001	time 0.6204 (2.4059)	model_time 0.6203 (0.6211)	loss 0.2229 (0.2308)	grad_norm 1.0775 (0.9318/0.2443)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg'
[2025-11-20 08:04:45 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1400/2882]	eta 0:59:19 lr 0.000001	time 0.6188 (2.4017)	model_time 0.6186 (0.6211)	loss 0.2412 (0.2308)	grad_norm 0.8296 (0.9345/0.2421)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg'
[2025-11-20 08:06:45 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1450/2882]	eta 0:57:20 lr 0.000001	time 0.6207 (2.4023)	model_time 0.6206 (0.6210)	loss 0.2147 (0.2309)	grad_norm 1.4831 (0.9399/0.2534)	mem 34630MB
[2025-11-20 08:08:48 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1500/2882]	eta 0:55:22 lr 0.000001	time 10.1980 (2.4040)	model_time 0.6190 (0.6210)	loss 0.2007 (0.2309)	grad_norm 0.8241 (0.9371/0.2499)	mem 34630MB
[2025-11-20 08:11:00 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1550/2882]	eta 0:53:31 lr 0.000001	time 12.1278 (2.4113)	model_time 0.6213 (0.6211)	loss 0.2253 (0.2309)	grad_norm 0.6837 (0.9374/0.2535)	mem 34630MB
[2025-11-20 08:13:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1600/2882]	eta 0:51:35 lr 0.000001	time 0.6226 (2.4145)	model_time 0.6224 (0.6211)	loss 0.2077 (0.2309)	grad_norm 1.1365 (0.9562/0.2484)	mem 34630MB
[2025-11-20 08:15:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1650/2882]	eta 0:49:33 lr 0.000001	time 0.6200 (2.4139)	model_time 0.6199 (0.6211)	loss 0.2586 (0.2309)	grad_norm 1.1191 (0.9626/0.2551)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg'
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg'
[2025-11-20 08:17:01 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1700/2882]	eta 0:47:30 lr 0.000001	time 0.6188 (2.4113)	model_time 0.6186 (0.6211)	loss 0.2309 (0.2309)	grad_norm 1.2596 (0.9736/0.2612)	mem 34630MB
[2025-11-20 08:19:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1750/2882]	eta 0:45:31 lr 0.000001	time 0.6222 (2.4134)	model_time 0.6220 (0.6211)	loss 0.2446 (0.2310)	grad_norm 0.7382 (0.9677/0.2529)	mem 34630MB
[2025-11-20 08:20:57 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1800/2882]	eta 0:43:25 lr 0.000001	time 0.6210 (2.4081)	model_time 0.6208 (0.6212)	loss 0.2021 (0.2308)	grad_norm 0.7488 (0.9710/0.2611)	mem 34630MB
[2025-11-20 08:23:10 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1850/2882]	eta 0:41:32 lr 0.000001	time 18.4307 (2.4151)	model_time 0.6225 (0.6212)	loss 0.2155 (0.2309)	grad_norm 0.9378 (0.9610/0.2626)	mem 34630MB
[2025-11-20 08:25:09 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1900/2882]	eta 0:39:30 lr 0.000001	time 0.6217 (2.4142)	model_time 0.6215 (0.6212)	loss 0.2222 (0.2310)	grad_norm 0.6168 (0.9477/0.2608)	mem 34630MB
[2025-11-20 08:27:07 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][1950/2882]	eta 0:37:28 lr 0.000001	time 0.6212 (2.4129)	model_time 0.6210 (0.6212)	loss 0.2187 (0.2309)	grad_norm 0.8627 (0.9395/0.2556)	mem 34630MB
[2025-11-20 08:28:59 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2000/2882]	eta 0:35:24 lr 0.000001	time 0.6218 (2.4085)	model_time 0.6216 (0.6212)	loss 0.2402 (0.2310)	grad_norm 1.0534 (0.9338/0.2517)	mem 34630MB
[2025-11-20 08:30:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2050/2882]	eta 0:33:20 lr 0.000001	time 0.6212 (2.4044)	model_time 0.6210 (0.6212)	loss 0.2206 (0.2310)	grad_norm 1.1584 (0.9374/0.2578)	mem 34630MB
[2025-11-20 08:32:46 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2100/2882]	eta 0:31:18 lr 0.000001	time 3.5274 (2.4017)	model_time 0.6216 (0.6212)	loss 0.1977 (0.2310)	grad_norm 0.7243 (0.9418/0.2565)	mem 34630MB
[2025-11-20 08:34:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2150/2882]	eta 0:29:19 lr 0.000001	time 15.7349 (2.4043)	model_time 0.6209 (0.6213)	loss 0.2187 (0.2311)	grad_norm 1.1054 (0.9440/0.2530)	mem 34630MB
[2025-11-20 08:36:43 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2200/2882]	eta 0:27:17 lr 0.000001	time 0.6224 (2.4004)	model_time 0.6222 (0.6213)	loss 0.2275 (0.2311)	grad_norm 0.6762 (0.9411/0.2588)	mem 34630MB
[2025-11-20 08:38:42 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2250/2882]	eta 0:25:16 lr 0.000001	time 0.6214 (2.4001)	model_time 0.6212 (0.6213)	loss 0.2492 (0.2311)	grad_norm 0.8505 (0.9406/0.2593)	mem 34630MB
[2025-11-20 08:40:37 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2300/2882]	eta 0:23:15 lr 0.000001	time 0.6195 (2.3977)	model_time 0.6193 (0.6213)	loss 0.2227 (0.2311)	grad_norm 0.7859 (0.9355/0.2650)	mem 34630MB
[2025-11-20 08:42:37 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2350/2882]	eta 0:21:15 lr 0.000001	time 0.6202 (2.3979)	model_time 0.6200 (0.6213)	loss 0.2460 (0.2312)	grad_norm 1.1031 (0.9352/0.2586)	mem 34630MB
[2025-11-20 08:44:30 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2400/2882]	eta 0:19:14 lr 0.000001	time 0.6186 (2.3948)	model_time 0.6184 (0.6212)	loss 0.2180 (0.2312)	grad_norm 0.7760 (0.9240/0.2495)	mem 34630MB
[2025-11-20 08:46:26 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2450/2882]	eta 0:17:13 lr 0.000001	time 4.9561 (2.3934)	model_time 0.6244 (0.6212)	loss 0.2248 (0.2312)	grad_norm 1.3403 (0.9240/0.2478)	mem 34630MB
[2025-11-20 08:48:23 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2500/2882]	eta 0:15:13 lr 0.000001	time 0.6208 (2.3925)	model_time 0.6207 (0.6212)	loss 0.2414 (0.2312)	grad_norm 0.7369 (0.9171/0.2392)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg'
[2025-11-20 08:50:45 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2550/2882]	eta 0:13:17 lr 0.000001	time 0.6221 (2.4012)	model_time 0.6219 (0.6212)	loss 0.2268 (0.2312)	grad_norm 0.7175 (0.9081/0.2356)	mem 34630MB
[2025-11-20 08:52:56 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2600/2882]	eta 0:11:18 lr 0.000001	time 0.6211 (2.4053)	model_time 0.6210 (0.6212)	loss 0.2536 (0.2311)	grad_norm 1.2941 (0.9104/0.2318)	mem 34630MB
[2025-11-20 08:54:58 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2650/2882]	eta 0:09:18 lr 0.000001	time 0.6185 (2.4059)	model_time 0.6183 (0.6212)	loss 0.2535 (0.2311)	grad_norm 1.0591 (0.8938/0.2321)	mem 34630MB
[2025-11-20 08:57:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2700/2882]	eta 0:07:18 lr 0.000001	time 0.6198 (2.4081)	model_time 0.6196 (0.6212)	loss 0.2302 (0.2311)	grad_norm 0.6842 (0.8978/0.2378)	mem 34630MB
[2025-11-20 08:59:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2750/2882]	eta 0:05:17 lr 0.000001	time 4.9105 (2.4080)	model_time 0.6209 (0.6212)	loss 0.2247 (0.2311)	grad_norm 0.6248 (0.9021/0.2353)	mem 34630MB
[2025-11-20 09:01:18 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2800/2882]	eta 0:03:17 lr 0.000001	time 16.7996 (2.4127)	model_time 0.6205 (0.6212)	loss 0.2432 (0.2312)	grad_norm 0.6659 (0.9025/0.2439)	mem 34630MB
[2025-11-20 09:03:13 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [14/20][2850/2882]	eta 0:01:17 lr 0.000001	time 0.7434 (2.4109)	model_time 0.7431 (0.6212)	loss 0.2221 (0.2312)	grad_norm 0.6988 (0.9013/0.2349)	mem 34630MB
[2025-11-20 09:04:16 internimage_b_mimic_cxr_224](main.py 583): INFO EPOCH 14 training takes 1:55:36
[2025-11-20 09:04:16 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_14.pth saving......
[2025-11-20 09:04:18 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_14.pth saved !!!
[2025-11-20 09:04:48 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 30.009 (30.009)	Loss 0.2708 (0.2708)	Mem 34630MB
[2025-11-20 09:05:25 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:14] * Mean AUC-ROC 0.8101 Loss 0.2292
[2025-11-20 09:05:25 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:14] * Per-class AUC-ROC: ['0.8399', '0.8303', '0.8195', '0.9036', '0.7346', '0.5937', '0.7289', '0.7483', '0.8664', '0.9245', '0.7730', '0.7612', '0.8905', '0.9272']
[2025-11-20 09:05:25 internimage_b_mimic_cxr_224](main.py 365): INFO AUC-ROC of the network on the 2991 val images: 0.8101
[2025-11-20 09:05:25 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_best.pth saving......
[2025-11-20 09:05:28 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_best.pth saved !!!
[2025-11-20 09:05:28 internimage_b_mimic_cxr_224](main.py 380): INFO Max AUC-ROC: 0.8101
[2025-11-20 09:05:37 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 8.845 (8.845)	Loss 0.2729 (0.2729)	Mem 34630MB
[2025-11-20 09:05:49 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:14] * Mean AUC-ROC 0.8072 Loss 0.2316
[2025-11-20 09:05:49 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:14] * Per-class AUC-ROC: ['0.8390', '0.8288', '0.8185', '0.9038', '0.7357', '0.5969', '0.7222', '0.7448', '0.8638', '0.9236', '0.7582', '0.7581', '0.8833', '0.9243']
[2025-11-20 09:05:49 internimage_b_mimic_cxr_224](main.py 385): INFO AUC-ROC of the ema network on the 2991 val images: 0.8072
[2025-11-20 09:05:49 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saving......
[2025-11-20 09:05:51 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saved !!!
[2025-11-20 09:05:51 internimage_b_mimic_cxr_224](main.py 400): INFO Max ema AUC-ROC: 0.8072
/scratch/smehta90/InternImage_MIMIC/classification/main.py:483: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
/scratch/smehta90/InternImage_MIMIC/classification/main.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
[2025-11-20 09:06:18 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][0/2882]	eta 21:47:52 lr 0.000001	time 27.2285 (27.2285)	model_time 0.6279 (0.6279)	loss 0.2369 (0.2369)	grad_norm 1.2391 (1.2391/0.0000)	mem 34630MB
[2025-11-20 09:08:15 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][50/2882]	eta 2:13:10 lr 0.000001	time 0.6246 (2.8216)	model_time 0.6244 (0.6247)	loss 0.2252 (0.2318)	grad_norm 0.6643 (0.9917/0.2128)	mem 34630MB
[2025-11-20 09:09:59 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][100/2882]	eta 1:53:39 lr 0.000001	time 0.6256 (2.4515)	model_time 0.6254 (0.6262)	loss 0.2503 (0.2304)	grad_norm 0.9190 (0.9905/0.2415)	mem 34630MB
[2025-11-20 09:11:46 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][150/2882]	eta 1:46:53 lr 0.000001	time 0.6243 (2.3475)	model_time 0.6241 (0.6257)	loss 0.2321 (0.2304)	grad_norm 0.6566 (0.9732/0.2442)	mem 34630MB
[2025-11-20 09:13:32 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][200/2882]	eta 1:42:23 lr 0.000001	time 0.6207 (2.2908)	model_time 0.6205 (0.6249)	loss 0.2085 (0.2298)	grad_norm 1.1918 (0.9661/0.2492)	mem 34630MB
[2025-11-20 09:15:21 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][250/2882]	eta 1:39:37 lr 0.000001	time 0.6205 (2.2713)	model_time 0.6204 (0.6249)	loss 0.1953 (0.2300)	grad_norm 1.3726 (0.9614/0.2461)	mem 34630MB
[2025-11-20 09:17:24 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][300/2882]	eta 1:38:59 lr 0.000001	time 19.9238 (2.3003)	model_time 0.6242 (0.6244)	loss 0.2286 (0.2297)	grad_norm 1.0579 (0.9543/0.2425)	mem 34630MB
[2025-11-20 09:19:11 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][350/2882]	eta 1:36:06 lr 0.000001	time 1.3973 (2.2773)	model_time 0.6242 (0.6241)	loss 0.2219 (0.2303)	grad_norm 0.8173 (0.9554/0.2523)	mem 34630MB
[2025-11-20 09:20:58 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][400/2882]	eta 1:33:33 lr 0.000001	time 0.6233 (2.2618)	model_time 0.6231 (0.6242)	loss 0.2207 (0.2301)	grad_norm 0.9967 (0.9395/0.2457)	mem 34630MB
[2025-11-20 09:22:44 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][450/2882]	eta 1:31:02 lr 0.000001	time 0.6206 (2.2461)	model_time 0.6205 (0.6241)	loss 0.2325 (0.2304)	grad_norm 0.6231 (0.9447/0.2441)	mem 34630MB
[2025-11-20 09:24:39 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][500/2882]	eta 1:29:21 lr 0.000001	time 0.6214 (2.2510)	model_time 0.6213 (0.6238)	loss 0.2305 (0.2305)	grad_norm 0.7196 (0.9348/0.2377)	mem 34630MB
[2025-11-20 09:26:30 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][550/2882]	eta 1:27:22 lr 0.000001	time 0.6232 (2.2479)	model_time 0.6230 (0.6239)	loss 0.2342 (0.2306)	grad_norm 0.6525 (0.9177/0.2424)	mem 34630MB
[2025-11-20 09:28:46 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][600/2882]	eta 1:27:00 lr 0.000001	time 14.5114 (2.2878)	model_time 0.6207 (0.6237)	loss 0.2118 (0.2307)	grad_norm 0.9188 (0.9176/0.2669)	mem 34630MB
[2025-11-20 09:31:10 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][650/2882]	eta 1:26:46 lr 0.000001	time 16.3749 (2.3329)	model_time 0.6214 (0.6236)	loss 0.2270 (0.2309)	grad_norm 1.0605 (0.9181/0.2559)	mem 34630MB
[2025-11-20 09:33:18 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][700/2882]	eta 1:25:26 lr 0.000001	time 0.6195 (2.3496)	model_time 0.6194 (0.6234)	loss 0.2272 (0.2308)	grad_norm 1.5113 (0.9322/0.2613)	mem 34630MB
[2025-11-20 09:35:20 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][750/2882]	eta 1:23:42 lr 0.000001	time 0.6220 (2.3558)	model_time 0.6217 (0.6232)	loss 0.2174 (0.2311)	grad_norm 0.8740 (0.9343/0.2674)	mem 34630MB
[2025-11-20 09:37:19 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][800/2882]	eta 1:21:45 lr 0.000001	time 0.6232 (2.3563)	model_time 0.6230 (0.6231)	loss 0.2726 (0.2311)	grad_norm 1.1244 (0.9436/0.2844)	mem 34630MB
[2025-11-20 09:39:20 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][850/2882]	eta 1:19:56 lr 0.000001	time 0.6210 (2.3606)	model_time 0.6208 (0.6230)	loss 0.2354 (0.2312)	grad_norm 1.1479 (0.9576/0.2922)	mem 34630MB
[2025-11-20 09:41:28 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][900/2882]	eta 1:18:21 lr 0.000001	time 4.0715 (2.3721)	model_time 0.6201 (0.6229)	loss 0.2782 (0.2312)	grad_norm 1.4988 (0.9670/0.2725)	mem 34630MB
[2025-11-20 09:43:52 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][950/2882]	eta 1:17:13 lr 0.000001	time 20.1460 (2.3983)	model_time 0.6227 (0.6228)	loss 0.2152 (0.2311)	grad_norm 0.8045 (0.9506/0.2836)	mem 34630MB
[2025-11-20 09:46:02 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1000/2882]	eta 1:15:32 lr 0.000001	time 0.6197 (2.4084)	model_time 0.6195 (0.6227)	loss 0.2466 (0.2311)	grad_norm 1.2516 (0.9315/0.2834)	mem 34630MB
[2025-11-20 09:47:53 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1050/2882]	eta 1:13:16 lr 0.000001	time 0.6200 (2.3998)	model_time 0.6199 (0.6227)	loss 0.2208 (0.2310)	grad_norm 1.5745 (0.9235/0.2785)	mem 34630MB
[2025-11-20 09:49:43 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1100/2882]	eta 1:10:59 lr 0.000001	time 0.6194 (2.3903)	model_time 0.6192 (0.6226)	loss 0.2392 (0.2309)	grad_norm 0.5335 (0.9226/0.2612)	mem 34630MB
[2025-11-20 09:51:43 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1150/2882]	eta 1:09:00 lr 0.000001	time 0.6196 (2.3904)	model_time 0.6194 (0.6224)	loss 0.2243 (0.2311)	grad_norm 0.7319 (0.9210/0.2392)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg'
[2025-11-20 09:53:44 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1200/2882]	eta 1:07:03 lr 0.000001	time 6.6470 (2.3921)	model_time 0.6234 (0.6223)	loss 0.2433 (0.2311)	grad_norm 0.8952 (0.9166/0.2434)	mem 34630MB
[2025-11-20 09:55:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1250/2882]	eta 1:05:12 lr 0.000001	time 11.9441 (2.3976)	model_time 0.6229 (0.6224)	loss 0.2477 (0.2310)	grad_norm 0.7336 (0.9086/0.2335)	mem 34630MB
[2025-11-20 09:57:49 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1300/2882]	eta 1:03:10 lr 0.000001	time 0.6230 (2.3961)	model_time 0.6228 (0.6224)	loss 0.2007 (0.2310)	grad_norm 0.6727 (0.9150/0.2340)	mem 34630MB
[2025-11-20 09:59:49 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1350/2882]	eta 1:01:11 lr 0.000001	time 0.6217 (2.3966)	model_time 0.6216 (0.6223)	loss 0.2409 (0.2311)	grad_norm 0.6982 (0.9244/0.2334)	mem 34630MB
[2025-11-20 10:01:55 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1400/2882]	eta 0:59:18 lr 0.000001	time 0.6215 (2.4009)	model_time 0.6213 (0.6224)	loss 0.2523 (0.2310)	grad_norm 0.8895 (0.9212/0.2363)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg'
[2025-11-20 10:04:24 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1450/2882]	eta 0:57:46 lr 0.000001	time 0.6207 (2.4207)	model_time 0.6205 (0.6223)	loss 0.2426 (0.2312)	grad_norm 0.8234 (0.9346/0.2495)	mem 34630MB
[2025-11-20 10:06:44 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1500/2882]	eta 0:56:03 lr 0.000001	time 8.8708 (2.4338)	model_time 0.6203 (0.6222)	loss 0.2168 (0.2311)	grad_norm 1.0669 (0.9355/0.2455)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg'
[2025-11-20 10:09:08 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1550/2882]	eta 0:54:20 lr 0.000001	time 9.8401 (2.4482)	model_time 0.6198 (0.6222)	loss 0.2213 (0.2311)	grad_norm 1.1287 (0.9423/0.2507)	mem 34630MB
[2025-11-20 10:11:10 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1600/2882]	eta 0:52:18 lr 0.000001	time 0.6259 (2.4478)	model_time 0.6256 (0.6222)	loss 0.1973 (0.2309)	grad_norm 0.7486 (0.9397/0.2416)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg'
[2025-11-20 10:13:08 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1650/2882]	eta 0:50:12 lr 0.000001	time 0.6256 (2.4450)	model_time 0.6254 (0.6223)	loss 0.2297 (0.2309)	grad_norm 0.5925 (0.9313/0.2452)	mem 34630MB
[2025-11-20 10:15:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1700/2882]	eta 0:48:05 lr 0.000001	time 0.6223 (2.4412)	model_time 0.6221 (0.6223)	loss 0.2172 (0.2309)	grad_norm 0.8894 (0.9309/0.2423)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg'
[2025-11-20 10:17:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1750/2882]	eta 0:46:02 lr 0.000001	time 0.6228 (2.4407)	model_time 0.6226 (0.6223)	loss 0.2165 (0.2308)	grad_norm 0.9468 (0.9138/0.2342)	mem 34630MB
[2025-11-20 10:19:32 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1800/2882]	eta 0:44:16 lr 0.000001	time 10.6671 (2.4549)	model_time 0.6213 (0.6222)	loss 0.2167 (0.2308)	grad_norm 1.1354 (0.9079/0.2289)	mem 34630MB
[2025-11-20 10:22:09 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1850/2882]	eta 0:42:32 lr 0.000001	time 5.9270 (2.4729)	model_time 0.6224 (0.6222)	loss 0.2335 (0.2308)	grad_norm 1.0015 (0.9094/0.2248)	mem 34630MB
[2025-11-20 10:24:32 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1900/2882]	eta 0:40:38 lr 0.000001	time 0.6189 (2.4832)	model_time 0.6188 (0.6222)	loss 0.2291 (0.2308)	grad_norm 0.8057 (0.9114/0.2241)	mem 34630MB
[2025-11-20 10:26:43 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][1950/2882]	eta 0:38:37 lr 0.000001	time 0.6192 (2.4868)	model_time 0.6190 (0.6222)	loss 0.2129 (0.2308)	grad_norm 1.0778 (0.9114/0.2220)	mem 34630MB
[2025-11-20 10:28:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2000/2882]	eta 0:36:34 lr 0.000001	time 0.6215 (2.4880)	model_time 0.6213 (0.6221)	loss 0.2280 (0.2309)	grad_norm 1.1905 (0.9232/0.2317)	mem 34630MB
[2025-11-20 10:31:10 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2050/2882]	eta 0:34:36 lr 0.000001	time 0.6219 (2.4956)	model_time 0.6218 (0.6221)	loss 0.1969 (0.2308)	grad_norm 1.1078 (0.9322/0.2414)	mem 34630MB
[2025-11-20 10:33:48 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2100/2882]	eta 0:32:44 lr 0.000001	time 2.4299 (2.5115)	model_time 0.6193 (0.6221)	loss 0.2342 (0.2308)	grad_norm 0.8695 (0.9324/0.2402)	mem 34630MB
[2025-11-20 10:36:26 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2150/2882]	eta 0:30:49 lr 0.000001	time 7.1705 (2.5267)	model_time 0.6193 (0.6221)	loss 0.2288 (0.2308)	grad_norm 0.7224 (0.9288/0.2494)	mem 34630MB
[2025-11-20 10:38:36 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2200/2882]	eta 0:28:44 lr 0.000001	time 0.6209 (2.5284)	model_time 0.6207 (0.6220)	loss 0.2076 (0.2308)	grad_norm 0.8833 (0.9389/0.2580)	mem 34630MB
[2025-11-20 10:40:47 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2250/2882]	eta 0:26:39 lr 0.000001	time 0.6185 (2.5303)	model_time 0.6184 (0.6220)	loss 0.2173 (0.2307)	grad_norm 1.1659 (0.9280/0.2553)	mem 34630MB
[2025-11-20 10:42:54 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2300/2882]	eta 0:24:32 lr 0.000001	time 0.6213 (2.5305)	model_time 0.6211 (0.6220)	loss 0.2076 (0.2308)	grad_norm 0.6677 (0.9297/0.2541)	mem 34630MB
[2025-11-20 10:45:13 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2350/2882]	eta 0:22:29 lr 0.000001	time 0.6177 (2.5358)	model_time 0.6175 (0.6219)	loss 0.2623 (0.2308)	grad_norm 1.2921 (0.9228/0.2446)	mem 34630MB
[2025-11-20 10:47:33 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2400/2882]	eta 0:20:24 lr 0.000001	time 7.3241 (2.5414)	model_time 0.6214 (0.6219)	loss 0.2301 (0.2307)	grad_norm 0.8398 (0.9316/0.2490)	mem 34630MB
[2025-11-20 10:50:11 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2450/2882]	eta 0:18:23 lr 0.000001	time 15.2006 (2.5540)	model_time 0.6210 (0.6219)	loss 0.1935 (0.2307)	grad_norm 0.7038 (0.9414/0.2345)	mem 34630MB
[2025-11-20 10:52:24 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2500/2882]	eta 0:16:16 lr 0.000001	time 0.6198 (2.5559)	model_time 0.6196 (0.6219)	loss 0.2069 (0.2307)	grad_norm 1.0500 (0.9389/0.2404)	mem 34630MB
[2025-11-20 10:54:37 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2550/2882]	eta 0:14:09 lr 0.000001	time 0.6201 (2.5583)	model_time 0.6200 (0.6219)	loss 0.2435 (0.2307)	grad_norm 1.2898 (0.9401/0.2389)	mem 34630MB
[2025-11-20 10:56:52 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2600/2882]	eta 0:12:02 lr 0.000001	time 0.6216 (2.5607)	model_time 0.6214 (0.6220)	loss 0.2273 (0.2307)	grad_norm 0.6440 (0.9275/0.2348)	mem 34630MB
[2025-11-20 10:59:27 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2650/2882]	eta 0:09:56 lr 0.000001	time 0.6237 (2.5711)	model_time 0.6235 (0.6220)	loss 0.2285 (0.2308)	grad_norm 0.6724 (0.9392/0.2454)	mem 34630MB
[2025-11-20 11:01:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2700/2882]	eta 0:07:48 lr 0.000001	time 0.6193 (2.5767)	model_time 0.6191 (0.6220)	loss 0.2765 (0.2308)	grad_norm 1.1424 (0.9322/0.2394)	mem 34630MB
[2025-11-20 11:04:43 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2750/2882]	eta 0:05:42 lr 0.000001	time 23.9185 (2.5924)	model_time 0.6211 (0.6219)	loss 0.2250 (0.2309)	grad_norm 0.6071 (0.9322/0.2492)	mem 34630MB
[2025-11-20 11:06:57 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2800/2882]	eta 0:03:32 lr 0.000001	time 0.6208 (2.5938)	model_time 0.6206 (0.6219)	loss 0.2368 (0.2309)	grad_norm 1.0244 (0.9365/0.2388)	mem 34630MB
[2025-11-20 11:09:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [15/20][2850/2882]	eta 0:01:22 lr 0.000001	time 0.6204 (2.5929)	model_time 0.6202 (0.6219)	loss 0.2267 (0.2308)	grad_norm 0.9036 (0.9518/0.2510)	mem 34630MB
[2025-11-20 11:10:10 internimage_b_mimic_cxr_224](main.py 583): INFO EPOCH 15 training takes 2:04:18
[2025-11-20 11:10:10 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_15.pth saving......
[2025-11-20 11:10:11 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_15.pth saved !!!
[2025-11-20 11:10:44 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 32.304 (32.304)	Loss 0.2707 (0.2707)	Mem 34630MB
[2025-11-20 11:11:24 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:15] * Mean AUC-ROC 0.8102 Loss 0.2292
[2025-11-20 11:11:24 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:15] * Per-class AUC-ROC: ['0.8412', '0.8309', '0.8205', '0.9034', '0.7319', '0.5920', '0.7354', '0.7485', '0.8667', '0.9247', '0.7677', '0.7610', '0.8916', '0.9273']
[2025-11-20 11:11:24 internimage_b_mimic_cxr_224](main.py 365): INFO AUC-ROC of the network on the 2991 val images: 0.8102
[2025-11-20 11:11:24 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_best.pth saving......
[2025-11-20 11:11:25 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_best.pth saved !!!
[2025-11-20 11:11:25 internimage_b_mimic_cxr_224](main.py 380): INFO Max AUC-ROC: 0.8102
[2025-11-20 11:11:35 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 9.818 (9.818)	Loss 0.2728 (0.2728)	Mem 34630MB
[2025-11-20 11:11:48 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:15] * Mean AUC-ROC 0.8081 Loss 0.2310
[2025-11-20 11:11:48 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:15] * Per-class AUC-ROC: ['0.8395', '0.8293', '0.8189', '0.9039', '0.7357', '0.5978', '0.7239', '0.7458', '0.8646', '0.9239', '0.7606', '0.7592', '0.8859', '0.9252']
[2025-11-20 11:11:48 internimage_b_mimic_cxr_224](main.py 385): INFO AUC-ROC of the ema network on the 2991 val images: 0.8081
[2025-11-20 11:11:48 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saving......
[2025-11-20 11:11:49 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saved !!!
[2025-11-20 11:11:49 internimage_b_mimic_cxr_224](main.py 400): INFO Max ema AUC-ROC: 0.8081
/scratch/smehta90/InternImage_MIMIC/classification/main.py:483: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
/scratch/smehta90/InternImage_MIMIC/classification/main.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
[2025-11-20 11:12:22 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][0/2882]	eta 1 day, 2:20:32 lr 0.000001	time 32.9050 (32.9050)	model_time 0.6238 (0.6238)	loss 0.2360 (0.2360)	grad_norm 0.7899 (0.7899/0.0000)	mem 34630MB
[2025-11-20 11:14:29 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][50/2882]	eta 2:27:56 lr 0.000001	time 0.6223 (3.1342)	model_time 0.6222 (0.6206)	loss 0.2400 (0.2328)	grad_norm 1.1997 (0.9863/0.2607)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p15/p15570051/s55401413/9a8f6a80-430c8b00-49cfa607-abb0bb7b-e840b480.jpg'
[2025-11-20 11:16:36 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][100/2882]	eta 2:11:36 lr 0.000001	time 0.6212 (2.8384)	model_time 0.6211 (0.6222)	loss 0.2493 (0.2304)	grad_norm 1.4875 (1.0070/0.2953)	mem 34630MB
[2025-11-20 11:18:42 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][150/2882]	eta 2:04:28 lr 0.000001	time 0.6203 (2.7337)	model_time 0.6201 (0.6218)	loss 0.2573 (0.2297)	grad_norm 1.6989 (0.9821/0.2879)	mem 34630MB
[2025-11-20 11:20:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][200/2882]	eta 2:00:09 lr 0.000001	time 0.6217 (2.6882)	model_time 0.6216 (0.6217)	loss 0.2263 (0.2293)	grad_norm 0.8164 (0.9863/0.2857)	mem 34630MB
[2025-11-20 11:23:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][250/2882]	eta 1:57:52 lr 0.000001	time 0.6186 (2.6873)	model_time 0.6185 (0.6215)	loss 0.2325 (0.2292)	grad_norm 0.8675 (0.9722/0.2781)	mem 34630MB
[2025-11-20 11:25:33 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][300/2882]	eta 1:57:47 lr 0.000001	time 15.2311 (2.7370)	model_time 0.6240 (0.6216)	loss 0.2404 (0.2293)	grad_norm 1.0058 (0.9809/0.2810)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14755254/s57056175/73cd10ac-6033f8db-7b32e2a8-01506ab1-068192fe.jpg'
[2025-11-20 11:28:02 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][350/2882]	eta 1:57:00 lr 0.000001	time 0.6243 (2.7729)	model_time 0.6241 (0.6220)	loss 0.2181 (0.2294)	grad_norm 0.7845 (0.9716/0.2823)	mem 34630MB
[2025-11-20 11:30:10 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][400/2882]	eta 1:53:33 lr 0.000001	time 0.6271 (2.7450)	model_time 0.6268 (0.6224)	loss 0.2299 (0.2299)	grad_norm 0.7087 (0.9553/0.2652)	mem 34630MB
[2025-11-20 11:32:27 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][450/2882]	eta 1:51:17 lr 0.000001	time 0.6273 (2.7455)	model_time 0.6271 (0.6227)	loss 0.2107 (0.2298)	grad_norm 0.9563 (0.9512/0.2564)	mem 34630MB
[2025-11-20 11:34:39 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][500/2882]	eta 1:48:32 lr 0.000001	time 0.6235 (2.7342)	model_time 0.6233 (0.6231)	loss 0.2487 (0.2302)	grad_norm 0.8976 (0.9315/0.2524)	mem 34630MB
[2025-11-20 11:36:49 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][550/2882]	eta 1:45:46 lr 0.000001	time 0.6253 (2.7214)	model_time 0.6251 (0.6232)	loss 0.2367 (0.2302)	grad_norm 1.2223 (0.9520/0.2757)	mem 34630MB
[2025-11-20 11:39:11 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][600/2882]	eta 1:43:55 lr 0.000001	time 6.0508 (2.7325)	model_time 0.6262 (0.6233)	loss 0.2058 (0.2302)	grad_norm 1.6737 (0.9507/0.2832)	mem 34630MB
[2025-11-20 11:41:39 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][650/2882]	eta 1:42:15 lr 0.000001	time 0.6264 (2.7491)	model_time 0.6262 (0.6236)	loss 0.2113 (0.2304)	grad_norm 1.1369 (0.9605/0.2799)	mem 34630MB
[2025-11-20 11:44:11 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][700/2882]	eta 1:40:44 lr 0.000001	time 0.6235 (2.7702)	model_time 0.6233 (0.6237)	loss 0.1911 (0.2304)	grad_norm 0.7064 (0.9687/0.2873)	mem 34630MB
[2025-11-20 11:46:48 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][750/2882]	eta 1:39:19 lr 0.000001	time 0.6256 (2.7952)	model_time 0.6253 (0.6238)	loss 0.2476 (0.2304)	grad_norm 1.3546 (0.9717/0.2892)	mem 34630MB
[2025-11-20 11:49:20 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][800/2882]	eta 1:37:29 lr 0.000001	time 0.6248 (2.8097)	model_time 0.6245 (0.6239)	loss 0.2345 (0.2304)	grad_norm 0.8047 (0.9846/0.2885)	mem 34630MB
[2025-11-20 11:51:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][850/2882]	eta 1:35:34 lr 0.000001	time 0.6239 (2.8220)	model_time 0.6237 (0.6239)	loss 0.2069 (0.2304)	grad_norm 0.7903 (0.9707/0.2714)	mem 34630MB
[2025-11-20 11:54:26 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][900/2882]	eta 1:33:44 lr 0.000001	time 7.4085 (2.8378)	model_time 0.6243 (0.6239)	loss 0.2161 (0.2305)	grad_norm 1.0908 (0.9597/0.2652)	mem 34630MB
[2025-11-20 11:56:59 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][950/2882]	eta 1:31:44 lr 0.000001	time 0.6230 (2.8490)	model_time 0.6227 (0.6241)	loss 0.2224 (0.2303)	grad_norm 1.0141 (0.9513/0.2709)	mem 34630MB
[2025-11-20 11:59:15 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1000/2882]	eta 1:29:11 lr 0.000001	time 0.6245 (2.8434)	model_time 0.6243 (0.6241)	loss 0.2269 (0.2303)	grad_norm 0.8939 (0.9461/0.2600)	mem 34630MB
[2025-11-20 12:01:44 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1050/2882]	eta 1:27:00 lr 0.000001	time 0.6247 (2.8497)	model_time 0.6245 (0.6241)	loss 0.2140 (0.2303)	grad_norm 1.0890 (0.9501/0.2625)	mem 34630MB
[2025-11-20 12:04:01 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1100/2882]	eta 1:24:28 lr 0.000001	time 0.6182 (2.8441)	model_time 0.6181 (0.6241)	loss 0.2197 (0.2302)	grad_norm 1.2340 (0.9432/0.2589)	mem 34630MB
[2025-11-20 12:06:25 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1150/2882]	eta 1:22:09 lr 0.000001	time 3.7642 (2.8459)	model_time 0.6194 (0.6239)	loss 0.2347 (0.2300)	grad_norm 1.6567 (0.9516/0.2593)	mem 34630MB
[2025-11-20 12:09:01 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1200/2882]	eta 1:20:05 lr 0.000001	time 19.8912 (2.8571)	model_time 0.6198 (0.6238)	loss 0.2612 (0.2302)	grad_norm 1.0935 (0.9535/0.2495)	mem 34630MB
[2025-11-20 12:11:18 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1250/2882]	eta 1:17:35 lr 0.000001	time 0.6209 (2.8528)	model_time 0.6207 (0.6237)	loss 0.2106 (0.2300)	grad_norm 1.0594 (0.9489/0.2485)	mem 34630MB
[2025-11-20 12:13:34 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1300/2882]	eta 1:15:05 lr 0.000001	time 0.6207 (2.8480)	model_time 0.6206 (0.6236)	loss 0.2396 (0.2300)	grad_norm 0.9819 (0.9542/0.2556)	mem 34630MB
[2025-11-20 12:16:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1350/2882]	eta 1:12:51 lr 0.000001	time 0.6201 (2.8537)	model_time 0.6200 (0.6235)	loss 0.2068 (0.2300)	grad_norm 0.5171 (0.9531/0.2637)	mem 34630MB
[2025-11-20 12:18:35 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1400/2882]	eta 1:10:37 lr 0.000001	time 0.6215 (2.8595)	model_time 0.6213 (0.6234)	loss 0.2333 (0.2300)	grad_norm 1.2603 (0.9576/0.2678)	mem 34630MB
[2025-11-20 12:21:03 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1450/2882]	eta 1:08:19 lr 0.000001	time 0.6792 (2.8628)	model_time 0.6790 (0.6233)	loss 0.2070 (0.2300)	grad_norm 1.3087 (0.9581/0.2711)	mem 34630MB
[2025-11-20 12:23:35 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1500/2882]	eta 1:06:04 lr 0.000001	time 9.2822 (2.8689)	model_time 0.6219 (0.6232)	loss 0.2592 (0.2301)	grad_norm 1.1839 (0.9659/0.2788)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg'
[2025-11-20 12:25:52 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1550/2882]	eta 1:03:35 lr 0.000001	time 2.2220 (2.8643)	model_time 0.6207 (0.6231)	loss 0.1981 (0.2302)	grad_norm 1.3024 (0.9821/0.2889)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg'
[2025-11-20 12:28:20 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1600/2882]	eta 1:01:15 lr 0.000001	time 0.6189 (2.8672)	model_time 0.6188 (0.6231)	loss 0.2029 (0.2301)	grad_norm 0.7576 (0.9755/0.2873)	mem 34630MB
[2025-11-20 12:31:11 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1650/2882]	eta 0:59:13 lr 0.000001	time 0.6192 (2.8843)	model_time 0.6191 (0.6230)	loss 0.2462 (0.2301)	grad_norm 1.0191 (0.9728/0.2763)	mem 34630MB
[2025-11-20 12:34:05 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1700/2882]	eta 0:57:09 lr 0.000001	time 0.6208 (2.9018)	model_time 0.6206 (0.6230)	loss 0.2403 (0.2303)	grad_norm 0.6525 (0.9665/0.2821)	mem 34630MB
[2025-11-20 12:36:52 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1750/2882]	eta 0:54:58 lr 0.000001	time 0.6206 (2.9140)	model_time 0.6205 (0.6230)	loss 0.2150 (0.2302)	grad_norm 1.0807 (0.9583/0.2740)	mem 34630MB
[2025-11-20 12:39:57 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1800/2882]	eta 0:52:56 lr 0.000001	time 13.6463 (2.9359)	model_time 0.6211 (0.6230)	loss 0.2334 (0.2302)	grad_norm 0.7953 (0.9506/0.2690)	mem 34630MB
[2025-11-20 12:42:37 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1850/2882]	eta 0:50:37 lr 0.000001	time 1.0332 (2.9432)	model_time 0.6204 (0.6229)	loss 0.2250 (0.2302)	grad_norm 0.8137 (0.9388/0.2501)	mem 34630MB
[2025-11-20 12:45:03 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1900/2882]	eta 0:48:09 lr 0.000001	time 0.6218 (2.9426)	model_time 0.6217 (0.6228)	loss 0.2487 (0.2302)	grad_norm 0.6600 (0.9292/0.2452)	mem 34630MB
[2025-11-20 12:47:40 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][1950/2882]	eta 0:45:47 lr 0.000001	time 0.6201 (2.9477)	model_time 0.6200 (0.6228)	loss 0.2288 (0.2303)	grad_norm 0.5931 (0.9358/0.2551)	mem 34630MB
[2025-11-20 12:50:09 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2000/2882]	eta 0:43:20 lr 0.000001	time 0.6217 (2.9486)	model_time 0.6216 (0.6228)	loss 0.2527 (0.2302)	grad_norm 0.7733 (0.9402/0.2449)	mem 34630MB
[2025-11-20 12:52:33 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2050/2882]	eta 0:40:51 lr 0.000001	time 0.6229 (2.9469)	model_time 0.6227 (0.6228)	loss 0.1993 (0.2301)	grad_norm 0.8044 (0.9255/0.2479)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg'
[2025-11-20 12:54:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2100/2882]	eta 0:38:20 lr 0.000001	time 0.6192 (2.9417)	model_time 0.6191 (0.6228)	loss 0.2298 (0.2302)	grad_norm 0.6922 (0.9202/0.2407)	mem 34630MB
[2025-11-20 12:57:24 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2150/2882]	eta 0:35:55 lr 0.000001	time 9.7971 (2.9451)	model_time 0.6208 (0.6227)	loss 0.2574 (0.2303)	grad_norm 1.1963 (0.9259/0.2415)	mem 34630MB
[2025-11-20 12:59:46 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2200/2882]	eta 0:33:26 lr 0.000001	time 0.6223 (2.9425)	model_time 0.6221 (0.6226)	loss 0.2418 (0.2303)	grad_norm 1.0999 (0.9334/0.2460)	mem 34630MB
[2025-11-20 13:02:10 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2250/2882]	eta 0:30:58 lr 0.000001	time 0.6181 (2.9411)	model_time 0.6179 (0.6226)	loss 0.2153 (0.2303)	grad_norm 0.7735 (0.9368/0.2434)	mem 34630MB
[2025-11-20 13:04:37 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2300/2882]	eta 0:28:31 lr 0.000001	time 0.6218 (2.9412)	model_time 0.6216 (0.6226)	loss 0.1895 (0.2302)	grad_norm 1.2450 (0.9411/0.2382)	mem 34630MB
[2025-11-20 13:06:54 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2350/2882]	eta 0:26:02 lr 0.000001	time 0.6212 (2.9371)	model_time 0.6210 (0.6225)	loss 0.2351 (0.2303)	grad_norm 1.0016 (0.9575/0.2364)	mem 34630MB
[2025-11-20 13:09:16 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2400/2882]	eta 0:23:34 lr 0.000001	time 0.6217 (2.9349)	model_time 0.6216 (0.6225)	loss 0.2411 (0.2304)	grad_norm 1.7752 (0.9714/0.2432)	mem 34630MB
[2025-11-20 13:11:58 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2450/2882]	eta 0:21:10 lr 0.000001	time 13.8658 (2.9413)	model_time 0.6192 (0.6225)	loss 0.2538 (0.2304)	grad_norm 0.8674 (0.9668/0.2381)	mem 34630MB
[2025-11-20 13:14:24 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2500/2882]	eta 0:18:43 lr 0.000001	time 0.6209 (2.9408)	model_time 0.6207 (0.6224)	loss 0.2253 (0.2304)	grad_norm 0.8700 (0.9623/0.2372)	mem 34630MB
[2025-11-20 13:16:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2550/2882]	eta 0:16:16 lr 0.000001	time 0.6189 (2.9407)	model_time 0.6187 (0.6224)	loss 0.2329 (0.2304)	grad_norm 0.8824 (0.9582/0.2293)	mem 34630MB
[2025-11-20 13:19:31 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2600/2882]	eta 0:13:50 lr 0.000001	time 0.6217 (2.9457)	model_time 0.6216 (0.6224)	loss 0.2303 (0.2304)	grad_norm 0.9109 (0.9540/0.2263)	mem 34630MB
[2025-11-20 13:22:03 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2650/2882]	eta 0:11:23 lr 0.000001	time 0.6198 (2.9475)	model_time 0.6197 (0.6224)	loss 0.2432 (0.2304)	grad_norm 1.3179 (0.9522/0.2240)	mem 34630MB
[2025-11-20 13:24:34 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2700/2882]	eta 0:08:56 lr 0.000001	time 0.6215 (2.9487)	model_time 0.6214 (0.6223)	loss 0.2099 (0.2304)	grad_norm 1.0123 (0.9467/0.2264)	mem 34630MB
[2025-11-20 13:27:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2750/2882]	eta 0:06:29 lr 0.000001	time 1.9677 (2.9499)	model_time 0.6204 (0.6223)	loss 0.2400 (0.2304)	grad_norm 1.0437 (0.9488/0.2327)	mem 34630MB
[2025-11-20 13:29:31 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2800/2882]	eta 0:04:01 lr 0.000001	time 0.6215 (2.9497)	model_time 0.6214 (0.6223)	loss 0.2521 (0.2304)	grad_norm 0.9057 (0.9551/0.2343)	mem 34630MB
[2025-11-20 13:32:10 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [16/20][2850/2882]	eta 0:01:34 lr 0.000001	time 0.6187 (2.9538)	model_time 0.6186 (0.6223)	loss 0.2436 (0.2304)	grad_norm 1.1001 (0.9417/0.2385)	mem 34630MB
[2025-11-20 13:33:42 internimage_b_mimic_cxr_224](main.py 583): INFO EPOCH 16 training takes 2:21:52
[2025-11-20 13:33:42 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_16.pth saving......
[2025-11-20 13:33:44 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_16.pth saved !!!
[2025-11-20 13:34:23 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 39.369 (39.369)	Loss 0.2699 (0.2699)	Mem 34630MB
[2025-11-20 13:35:09 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:16] * Mean AUC-ROC 0.8107 Loss 0.2290
[2025-11-20 13:35:09 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:16] * Per-class AUC-ROC: ['0.8409', '0.8310', '0.8220', '0.9036', '0.7342', '0.5972', '0.7290', '0.7492', '0.8672', '0.9244', '0.7699', '0.7616', '0.8923', '0.9272']
[2025-11-20 13:35:09 internimage_b_mimic_cxr_224](main.py 365): INFO AUC-ROC of the network on the 2991 val images: 0.8107
[2025-11-20 13:35:09 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_best.pth saving......
[2025-11-20 13:35:11 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_best.pth saved !!!
[2025-11-20 13:35:11 internimage_b_mimic_cxr_224](main.py 380): INFO Max AUC-ROC: 0.8107
[2025-11-20 13:35:20 internimage_b_mimic_cxr_224](utils_multilabel.py 77): INFO Test: [0/24]	Time 8.922 (8.922)	Loss 0.2725 (0.2725)	Mem 34630MB
[2025-11-20 13:35:33 internimage_b_mimic_cxr_224](utils_multilabel.py 138): INFO [Epoch:16] * Mean AUC-ROC 0.8089 Loss 0.2305
[2025-11-20 13:35:33 internimage_b_mimic_cxr_224](utils_multilabel.py 139): INFO [Epoch:16] * Per-class AUC-ROC: ['0.8399', '0.8298', '0.8195', '0.9040', '0.7352', '0.5991', '0.7256', '0.7465', '0.8652', '0.9241', '0.7624', '0.7597', '0.8880', '0.9258']
[2025-11-20 13:35:33 internimage_b_mimic_cxr_224](main.py 385): INFO AUC-ROC of the ema network on the 2991 val images: 0.8089
[2025-11-20 13:35:33 internimage_b_mimic_cxr_224](utils.py 359): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saving......
[2025-11-20 13:35:35 internimage_b_mimic_cxr_224](utils.py 361): INFO output/mimic_cxr/internimage_b/internimage_b_mimic_cxr_224/ckpt_epoch_ema_best.pth saved !!!
[2025-11-20 13:35:35 internimage_b_mimic_cxr_224](main.py 400): INFO Max ema AUC-ROC: 0.8089
/scratch/smehta90/InternImage_MIMIC/classification/main.py:483: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
/scratch/smehta90/InternImage_MIMIC/classification/main.py:530: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with amp_autocast(dtype=amp_type):
[2025-11-20 13:36:17 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][0/2882]	eta 1 day, 9:26:40 lr 0.000001	time 41.7766 (41.7766)	model_time 0.6193 (0.6193)	loss 0.2120 (0.2120)	grad_norm 0.9524 (0.9524/0.0000)	mem 34630MB
[2025-11-20 13:38:45 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][50/2882]	eta 2:56:29 lr 0.000001	time 0.6180 (3.7394)	model_time 0.6179 (0.6201)	loss 0.2239 (0.2288)	grad_norm 0.7660 (0.8920/0.1815)	mem 34630MB
[2025-11-20 13:41:21 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][100/2882]	eta 2:38:55 lr 0.000001	time 0.6216 (3.4276)	model_time 0.6215 (0.6197)	loss 0.2357 (0.2294)	grad_norm 1.2925 (0.9408/0.2291)	mem 34630MB
[2025-11-20 13:43:49 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][150/2882]	eta 2:29:11 lr 0.000001	time 0.6186 (3.2765)	model_time 0.6184 (0.6197)	loss 0.2266 (0.2296)	grad_norm 0.7402 (0.9633/0.2603)	mem 34630MB
[2025-11-20 13:46:21 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][200/2882]	eta 2:23:47 lr 0.000001	time 0.6211 (3.2167)	model_time 0.6210 (0.6199)	loss 0.2131 (0.2299)	grad_norm 0.9530 (0.9579/0.2698)	mem 34630MB
[2025-11-20 13:48:50 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][250/2882]	eta 2:18:59 lr 0.000001	time 0.6219 (3.1685)	model_time 0.6217 (0.6202)	loss 0.2422 (0.2304)	grad_norm 1.5133 (0.9590/0.2791)	mem 34630MB
[2025-11-20 13:51:57 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][300/2882]	eta 2:20:28 lr 0.000001	time 31.1830 (3.2645)	model_time 0.6203 (0.6202)	loss 0.2176 (0.2309)	grad_norm 1.0929 (0.9593/0.2737)	mem 34630MB
[2025-11-20 13:54:31 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][350/2882]	eta 2:16:37 lr 0.000001	time 0.6255 (3.2376)	model_time 0.6253 (0.6209)	loss 0.2386 (0.2307)	grad_norm 1.1858 (0.9684/0.2884)	mem 34630MB
[2025-11-20 13:57:02 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][400/2882]	eta 2:12:47 lr 0.000001	time 0.6204 (3.2101)	model_time 0.6203 (0.6209)	loss 0.2386 (0.2309)	grad_norm 0.7562 (0.9639/0.2816)	mem 34630MB
[2025-11-20 13:59:40 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][450/2882]	eta 2:09:51 lr 0.000001	time 0.6234 (3.2039)	model_time 0.6232 (0.6209)	loss 0.2353 (0.2311)	grad_norm 0.7468 (0.9481/0.2640)	mem 34630MB
[2025-11-20 14:02:21 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][500/2882]	eta 2:07:17 lr 0.000001	time 0.6190 (3.2062)	model_time 0.6189 (0.6211)	loss 0.2818 (0.2312)	grad_norm 1.1923 (0.9532/0.2663)	mem 34630MB
[2025-11-20 14:04:58 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][550/2882]	eta 2:04:24 lr 0.000001	time 0.6230 (3.2008)	model_time 0.6227 (0.6211)	loss 0.2263 (0.2314)	grad_norm 0.9757 (0.9576/0.2587)	mem 34630MB
[2025-11-20 14:07:48 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][600/2882]	eta 2:02:18 lr 0.000001	time 24.0143 (3.2160)	model_time 0.6214 (0.6215)	loss 0.2510 (0.2313)	grad_norm 0.9229 (0.9517/0.2590)	mem 34630MB
[2025-11-20 14:10:26 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][650/2882]	eta 1:59:31 lr 0.000001	time 0.6228 (3.2130)	model_time 0.6226 (0.6214)	loss 0.2359 (0.2314)	grad_norm 1.0136 (0.9433/0.2479)	mem 34630MB
[2025-11-20 14:12:57 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][700/2882]	eta 1:56:20 lr 0.000001	time 0.6215 (3.1991)	model_time 0.6213 (0.6214)	loss 0.2429 (0.2314)	grad_norm 1.1746 (0.9529/0.2599)	mem 34630MB
[2025-11-20 14:15:28 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][750/2882]	eta 1:53:14 lr 0.000001	time 1.4115 (3.1870)	model_time 0.6211 (0.6213)	loss 0.2237 (0.2313)	grad_norm 0.9376 (0.9606/0.2676)	mem 34630MB
[2025-11-20 14:17:54 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][800/2882]	eta 1:50:00 lr 0.000001	time 0.6186 (3.1702)	model_time 0.6185 (0.6212)	loss 0.2351 (0.2311)	grad_norm 0.6432 (0.9500/0.2607)	mem 34630MB
[2025-11-20 14:20:18 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][850/2882]	eta 1:46:46 lr 0.000001	time 0.6189 (3.1530)	model_time 0.6187 (0.6212)	loss 0.2203 (0.2310)	grad_norm 1.0018 (0.9446/0.2531)	mem 34630MB
[2025-11-20 14:23:11 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][900/2882]	eta 1:44:42 lr 0.000001	time 22.8241 (3.1700)	model_time 0.6217 (0.6211)	loss 0.2457 (0.2313)	grad_norm 0.6534 (0.9524/0.2645)	mem 34630MB
[2025-11-20 14:25:40 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][950/2882]	eta 1:41:44 lr 0.000001	time 0.6240 (3.1598)	model_time 0.6238 (0.6210)	loss 0.2173 (0.2311)	grad_norm 0.8073 (0.9568/0.2671)	mem 34630MB
[2025-11-20 14:28:13 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1000/2882]	eta 1:38:58 lr 0.000001	time 0.6251 (3.1555)	model_time 0.6248 (0.6213)	loss 0.2837 (0.2312)	grad_norm 1.5060 (0.9462/0.2619)	mem 34630MB
[2025-11-20 14:30:59 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1050/2882]	eta 1:36:34 lr 0.000001	time 13.0891 (3.1631)	model_time 0.6239 (0.6214)	loss 0.2444 (0.2311)	grad_norm 0.9996 (0.9469/0.2711)	mem 34630MB
[2025-11-20 14:33:32 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1100/2882]	eta 1:33:47 lr 0.000001	time 0.6225 (3.1582)	model_time 0.6223 (0.6215)	loss 0.2547 (0.2311)	grad_norm 0.8375 (0.9420/0.2689)	mem 34630MB
[2025-11-20 14:35:59 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1150/2882]	eta 1:30:53 lr 0.000001	time 0.6231 (3.1484)	model_time 0.6228 (0.6217)	loss 0.2431 (0.2309)	grad_norm 0.6456 (0.9308/0.2705)	mem 34630MB
[2025-11-20 14:38:49 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1200/2882]	eta 1:28:33 lr 0.000001	time 14.8597 (3.1593)	model_time 0.6234 (0.6218)	loss 0.2048 (0.2309)	grad_norm 1.2523 (0.9326/0.2555)	mem 34630MB
[2025-11-20 14:41:46 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1250/2882]	eta 1:26:20 lr 0.000001	time 0.6259 (3.1741)	model_time 0.6257 (0.6219)	loss 0.2336 (0.2308)	grad_norm 0.8501 (0.9357/0.2605)	mem 34630MB
[2025-11-20 14:44:26 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1300/2882]	eta 1:23:43 lr 0.000001	time 0.6240 (3.1756)	model_time 0.6237 (0.6221)	loss 0.2424 (0.2308)	grad_norm 0.8848 (0.9333/0.2580)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14280310/s56920231/e9a7b16f-a2cbb1f8-5ba2fc13-a83319b0-6268fdff.jpg'
[2025-11-20 14:47:14 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1350/2882]	eta 1:21:15 lr 0.000001	time 11.9925 (3.1824)	model_time 0.6241 (0.6222)	loss 0.2256 (0.2309)	grad_norm 1.1263 (0.9284/0.2475)	mem 34630MB
[2025-11-20 14:49:42 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1400/2882]	eta 1:18:24 lr 0.000001	time 0.6250 (3.1745)	model_time 0.6249 (0.6223)	loss 0.2262 (0.2309)	grad_norm 0.9427 (0.9405/0.2433)	mem 34630MB
[2025-11-20 14:52:15 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1450/2882]	eta 1:15:40 lr 0.000001	time 0.6179 (3.1706)	model_time 0.6177 (0.6224)	loss 0.2305 (0.2308)	grad_norm 1.3401 (0.9403/0.2412)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p13/p13692794/s59700179/3e3c90ec-40f735f2-deec6e24-46e2cfe3-a981490b.jpg'
[2025-11-20 14:54:51 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1500/2882]	eta 1:12:59 lr 0.000001	time 0.6261 (3.1687)	model_time 0.6259 (0.6223)	loss 0.2486 (0.2307)	grad_norm 1.8222 (0.9334/0.2494)	mem 34630MB
[2025-11-20 14:57:45 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1550/2882]	eta 1:10:33 lr 0.000001	time 0.6254 (3.1785)	model_time 0.6252 (0.6224)	loss 0.2459 (0.2308)	grad_norm 1.0918 (0.9289/0.2423)	mem 34630MB
Error loading image /scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg: [Errno 13] Permission denied: '/scratch/aalla4/shared_folder/MIMIC/files/p14/p14064974/s53204465/166b0526-1791b559-e7e0b5bd-279d2c23-7c997ae1.jpg'
[2025-11-20 15:00:14 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1600/2882]	eta 1:07:47 lr 0.000001	time 0.6254 (3.1728)	model_time 0.6252 (0.6226)	loss 0.2436 (0.2307)	grad_norm 0.7990 (0.9291/0.2448)	mem 34630MB
[2025-11-20 15:02:59 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1650/2882]	eta 1:05:13 lr 0.000001	time 11.9965 (3.1765)	model_time 0.6235 (0.6226)	loss 0.2180 (0.2306)	grad_norm 0.7296 (0.9251/0.2411)	mem 34630MB
[2025-11-20 15:05:29 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1700/2882]	eta 1:02:28 lr 0.000001	time 0.6178 (3.1715)	model_time 0.6177 (0.6226)	loss 0.2026 (0.2305)	grad_norm 0.6088 (0.9297/0.2462)	mem 34630MB
[2025-11-20 15:08:04 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1750/2882]	eta 0:59:47 lr 0.000001	time 0.6178 (3.1689)	model_time 0.6176 (0.6225)	loss 0.1902 (0.2304)	grad_norm 0.7313 (0.9261/0.2519)	mem 34630MB
[2025-11-20 15:10:42 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1800/2882]	eta 0:57:09 lr 0.000001	time 6.3505 (3.1692)	model_time 0.6225 (0.6225)	loss 0.2304 (0.2304)	grad_norm 0.8107 (0.9250/0.2533)	mem 34630MB
[2025-11-20 15:13:28 internimage_b_mimic_cxr_224](main.py 574): INFO Train: [17/20][1850/2882]	eta 0:54:34 lr 0.000001	time 0.6206 (3.1729)	model_time 0.6205 (0.6224)	loss 0.2233 (0.2304)	grad_norm 0.9676 (0.9268/0.2559)	mem 34630MB
